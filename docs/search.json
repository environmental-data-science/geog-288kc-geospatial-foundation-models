[
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Description",
    "text": "Course Description\nThis project-driven seminar teaches students to build geospatial foundation models (GFMs) from scratch. Students implement every layer of the pipeline‚Äîfrom data pipelines and tokenization through attention mechanisms, full architectures, pretraining, evaluation, and deployment‚Äîculminating in a working end-to-end GFM tailored to a chosen geospatial application.\nBy the end of the course, students will be able to:\n\nDesign and implement geospatial data pipelines for multi-spectral, spatial, and temporal data\nBuild attention mechanisms and assemble transformer-based architectures for geospatial inputs\nPretrain using masked autoencoding and evaluate learned representations\nFine-tune models for specific Earth observation tasks\nDeploy models via APIs and interactive interfaces with honest performance analysis"
  },
  {
    "objectID": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "href": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "title": "Building Geospatial Foundation Models",
    "section": "Getting Started with the UCSB AI Sandbox",
    "text": "Getting Started with the UCSB AI Sandbox\nHere are detailed instructions for setting up your environment on the UCSB AI Sandbox, including foundation model installation and GPU optimization."
  },
  {
    "objectID": "index.html#course-structure-3-stages-10-weeks",
    "href": "index.html#course-structure-3-stages-10-weeks",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Structure: 3 Stages, 10 Weeks",
    "text": "Course Structure: 3 Stages, 10 Weeks\n\n\n\n\n\nflowchart TD\n    subgraph Stage1 [\"üèóÔ∏è Stage 1: Build GFM Architecture\"]\n        direction LR\n        W1[\"üìä&lt;br/&gt;Week 1&lt;br/&gt;Data Foundations&lt;br/&gt;Pipelines & Tokenization\"] --&gt; W2[\"üß†&lt;br/&gt;Week 2&lt;br/&gt;Attention Mechanisms&lt;br/&gt;Spatial-Temporal Focus\"]\n        W2 --&gt; W3[\"üèõÔ∏è&lt;br/&gt;Week 3&lt;br/&gt;Complete Architecture&lt;br/&gt;Vision Transformer\"]\n    end\n    \n    subgraph Stage2 [\"üöÄ Stage 2: Train Foundation Model\"]\n        direction LR\n        W4[\"üé≠&lt;br/&gt;Week 4&lt;br/&gt;Pretraining&lt;br/&gt;Masked Autoencoder\"] --&gt; W5[\"‚ö°&lt;br/&gt;Week 5&lt;br/&gt;Training Optimization&lt;br/&gt;Stability & Efficiency\"]\n        W5 --&gt; W6[\"üìà&lt;br/&gt;Week 6&lt;br/&gt;Evaluation & Analysis&lt;br/&gt;Embeddings & Probing\"]\n        W6 --&gt; W7[\"üîó&lt;br/&gt;Week 7&lt;br/&gt;Model Integration&lt;br/&gt;Prithvi, SatMAE\"]\n    end\n    \n    subgraph Stage3 [\"üéØ Stage 3: Apply & Deploy\"]\n        direction LR\n        W8[\"üéØ&lt;br/&gt;Week 8&lt;br/&gt;Fine-tuning&lt;br/&gt;Task-Specific Training\"] --&gt; W9[\"üöÄ&lt;br/&gt;Week 9&lt;br/&gt;Deployment&lt;br/&gt;APIs & Interfaces\"]\n        W9 --&gt; W10[\"üé§&lt;br/&gt;Week 10&lt;br/&gt;Presentations&lt;br/&gt;Project Synthesis\"]\n    end\n    \n    Stage1 --&gt; Stage2\n    Stage2 --&gt; Stage3\n    \n    style Stage1 fill:#e3f2fd\n    style Stage2 fill:#fff3e0  \n    style Stage3 fill:#e8f5e8\n    style W1 fill:#bbdefb\n    style W4 fill:#ffe0b2\n    style W8 fill:#c8e6c8\n\n\n\n\n\n\n\nüèóÔ∏è Stage 1: Build GFM Architecture (Weeks 1-3)\n\nWeek 1: Geospatial Data Foundations (data pipelines, tokenization, loaders)\nWeek 2: Spatial-Temporal Attention Mechanisms (from-scratch implementation)\nWeek 3: Complete GFM Architecture (Vision Transformer for geospatial)\n\n\n\nüöÄ Stage 2: Train a Foundation Model (Weeks 4-7)\n\nWeek 4: Pretraining Implementation (masked autoencoder)\nWeek 5: Training Loop Optimization (stability, efficiency, mixed precision)\nWeek 6: Model Evaluation & Analysis (embeddings, probing, reconstructions)\nWeek 7: Integration with Existing Models (Prithvi, SatMAE)\n\n\n\nüéØ Stage 3: Apply & Deploy (Weeks 8-10)\n\nWeek 8: Task-Specific Fine-tuning (efficient strategies, few-shot)\nWeek 9: Model Implementation & Deployment (APIs, UI, benchmarking)\nWeek 10: Project Presentations & Synthesis"
  },
  {
    "objectID": "index.html#course-sessions",
    "href": "index.html#course-sessions",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Sessions",
    "text": "Course Sessions\n\nWeekly sessions: see navbar ‚Üí üíª weekly sessions"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Building Geospatial Foundation Models",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\nKelly Caylor\nEmail: caylor@ucsb.edu\nLearn more: Bren profile\n\n\n\n\nTA\n\n\n\n\n\n\n\nAnna Boser\nEmail: anaboser@ucsb.edu\nLearn more: Bren profile"
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#geospatial-data-fundamentals",
    "href": "cheatsheets.html#geospatial-data-fundamentals",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#pytorch-for-geospatial-ai",
    "href": "cheatsheets.html#pytorch-for-geospatial-ai",
    "title": "Cheatsheets",
    "section": "üî• PyTorch for Geospatial AI",
    "text": "üî• PyTorch for Geospatial AI\n\nPyTorch Tensors & GPU Operations\nTorchGeo Datasets & Transforms\nData Loading for Satellite Imagery"
  },
  {
    "objectID": "cheatsheets.html#foundation-models-huggingface",
    "href": "cheatsheets.html#foundation-models-huggingface",
    "title": "Cheatsheets",
    "section": "ü§ó Foundation Models & HuggingFace",
    "text": "ü§ó Foundation Models & HuggingFace\n\nLoading Pre-trained Models\nModel Inference & Feature Extraction\nFine-tuning Strategies"
  },
  {
    "objectID": "cheatsheets.html#visualization-analysis",
    "href": "cheatsheets.html#visualization-analysis",
    "title": "Cheatsheets",
    "section": "üìä Visualization & Analysis",
    "text": "üìä Visualization & Analysis\n\nPlotting Satellite Imagery\nInteractive Maps with Folium\nGeospatial Plotting with Matplotlib"
  },
  {
    "objectID": "course-materials/c05-training-loop-optimization.html",
    "href": "course-materials/c05-training-loop-optimization.html",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n5\nStage 2: Train Foundation Model\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n\n\n\n\nBuild fit, train_step, eval_step with logging\nConfigure AdamW; optionally add LR scheduler/AMP\nSave and restore a basic checkpoint\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c05-training-loop-optimization.html#course-roadmap-mapping",
    "href": "course-materials/c05-training-loop-optimization.html#course-roadmap-mapping",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n5\nStage 2: Train Foundation Model\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n\n\n\n\nBuild fit, train_step, eval_step with logging\nConfigure AdamW; optionally add LR scheduler/AMP\nSave and restore a basic checkpoint\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c06-model-evaluation-analysis.html",
    "href": "course-materials/c06-model-evaluation-analysis.html",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n6\nStage 2: Train Foundation Model\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n\n\n\n\nImplement reconstruction visualization utilities\nAdd a simple metric (e.g., PSNR) and validation loop hooks\nInterpret embeddings or reconstructions qualitatively\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c06-model-evaluation-analysis.html#course-roadmap-mapping",
    "href": "course-materials/c06-model-evaluation-analysis.html#course-roadmap-mapping",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n6\nStage 2: Train Foundation Model\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n\n\n\n\nImplement reconstruction visualization utilities\nAdd a simple metric (e.g., PSNR) and validation loop hooks\nInterpret embeddings or reconstructions qualitatively\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c02-spatial-temporal-attention-mechanisms.html",
    "href": "course-materials/c02-spatial-temporal-attention-mechanisms.html",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n2\nStage 1: Build GFM Architecture\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n\n\n\n\nImplement patch/positional embeddings and MHA from scratch\nAssemble a PreNorm transformer block (MHA + MLP)\nValidate tensor shapes and simple layer tests\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c02-spatial-temporal-attention-mechanisms.html#course-roadmap-mapping",
    "href": "course-materials/c02-spatial-temporal-attention-mechanisms.html#course-roadmap-mapping",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n2\nStage 1: Build GFM Architecture\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n\n\n\n\nImplement patch/positional embeddings and MHA from scratch\nAssemble a PreNorm transformer block (MHA + MLP)\nValidate tensor shapes and simple layer tests\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c07-integration-with-existing-models.html",
    "href": "course-materials/c07-integration-with-existing-models.html",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n7\nStage 2: Train Foundation Model\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n\n\n\n\nCreate a light registry for model variants\nAdd basic HF Hub glue to load model configs/weights\nCompare MVP structure to Prithvi and define switch points\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c07-integration-with-existing-models.html#course-roadmap-mapping",
    "href": "course-materials/c07-integration-with-existing-models.html#course-roadmap-mapping",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n7\nStage 2: Train Foundation Model\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n\n\n\n\nCreate a light registry for model variants\nAdd basic HF Hub glue to load model configs/weights\nCompare MVP structure to Prithvi and define switch points\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c03-complete-gfm-architecture.html",
    "href": "course-materials/c03-complete-gfm-architecture.html",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n3\nStage 1: Build GFM Architecture\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n\n\n\n\nWire blocks into a GeoViT-style encoder\nAdd a simple reconstruction head\nRun end-to-end forward pass on dummy input\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c03-complete-gfm-architecture.html#course-roadmap-mapping",
    "href": "course-materials/c03-complete-gfm-architecture.html#course-roadmap-mapping",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n3\nStage 1: Build GFM Architecture\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n\n\n\n\nWire blocks into a GeoViT-style encoder\nAdd a simple reconstruction head\nRun end-to-end forward pass on dummy input\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c10-project-presentations-synthesis.html",
    "href": "course-materials/c10-project-presentations-synthesis.html",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n10\nStage 3: Apply & Deploy\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan\n\n\n\n\n\n\nSynthesize architecture, training, and evaluation learnings\nPresent MVP results and insights\nOutline next steps with Prithvi/scale-up\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c10-project-presentations-synthesis.html#course-roadmap-mapping",
    "href": "course-materials/c10-project-presentations-synthesis.html#course-roadmap-mapping",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n10\nStage 3: Apply & Deploy\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan\n\n\n\n\n\n\nSynthesize architecture, training, and evaluation learnings\nPresent MVP results and insights\nOutline next steps with Prithvi/scale-up\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c00-introduction-to-deeplearning-architecture.html",
    "href": "course-materials/c00-introduction-to-deeplearning-architecture.html",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/c00-introduction-to-deeplearning-architecture.html#week-0-overview",
    "href": "course-materials/c00-introduction-to-deeplearning-architecture.html#week-0-overview",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/c00-introduction-to-deeplearning-architecture.html#build-your-own-gfm-architecture-cheatsheet",
    "href": "course-materials/c00-introduction-to-deeplearning-architecture.html#build-your-own-gfm-architecture-cheatsheet",
    "title": "Week 0: Getting Started",
    "section": "Build-your-own GFM: Architecture Cheatsheet",
    "text": "Build-your-own GFM: Architecture Cheatsheet\nThis is a quick-reference for how our Geospatial Foundation Model (GFM) is organized, what each part does, and where to look during the course. We implement simple, readable components first, then show how to swap in optimized library counterparts as needed.\n\nRoadmap at a glance\n\ngraph TD\n  A[Week 1: Data Foundations] --&gt; B[Week 2: Attention & Blocks]\n  B --&gt; C[Week 3: Full Encoder]\n  C --&gt; D[Week 4: MAE Pretraining]\n  D --&gt; E[Week 5: Training Loop]\n  E --&gt; F[Week 6: Eval & Viz]\n  F --&gt; G[Week 7+: Interop & Scale]\n\n\nMinimal structure you‚Äôll use\ngeogfm/\n  core/\n    config.py                # Minimal typed configs for model, data, training\n  data/\n    loaders.py               # build_dataloader(...)\n    datasets/\n      stac_dataset.py        # Simple STAC-backed dataset\n    transforms/\n      normalization.py       # Per-channel normalization\n      patchify.py            # Extract fixed-size patches\n  modules/\n    attention/\n      multihead_attention.py     # Standard MHA (from scratch)\n    embeddings/\n      patch_embedding.py         # Conv patch embedding\n      positional_encoding.py     # Simple positional encoding\n    blocks/\n      transformer_block.py       # PreNorm block (MHA + MLP)\n    heads/\n      reconstruction_head.py     # Lightweight decoder/readout\n    losses/\n      mae_loss.py                # Masked reconstruction loss\n  models/\n    gfm_vit.py                   # GeoViT-style encoder\n    gfm_mae.py                   # MAE wrapper (masking + encoder + head)\n  training/\n    optimizer.py                 # AdamW builder\n    loop.py                      # fit/train_step/eval_step with basic checkpointing\n  evaluation/\n    visualization.py             # Visualize inputs vs reconstructions\n\n# Outside the package (repo root)\nconfigs/   # Small YAML/JSON run configs\ntests/     # Unit tests\ndata/      # Datasets, splits, stats, build scripts\n\n\nWhat each part does (one-liners)\n\ncore/config.py: Typed configs for model/data/training; keeps parameters organized.\ndata/datasets/stac_dataset.py: Reads imagery + metadata (e.g., STAC), returns tensors.\ndata/transforms/normalization.py: Normalizes channels using precomputed stats.\ndata/transforms/patchify.py: Turns large images into uniform patches for ViT.\ndata/loaders.py: Builds PyTorch DataLoaders for train/val.\nmodules/embeddings/patch_embedding.py: Projects image patches into token vectors.\nmodules/embeddings/positional_encoding.py: Adds position info to tokens.\nmodules/attention/multihead_attention.py: Lets tokens attend to each other.\nmodules/blocks/transformer_block.py: Core transformer layer (attention + MLP).\nmodules/heads/reconstruction_head.py: Reconstructs pixels from encoded tokens.\nmodules/losses/mae_loss.py: Computes masked reconstruction loss for MAE.\nmodels/gfm_vit.py: Assembles the encoder backbone from blocks.\nmodels/gfm_mae.py: Wraps encoder with masking + reconstruction for pretraining.\ntraining/optimizer.py: Creates AdamW with common defaults.\ntraining/loop.py: Runs epochs, backprop, validation, and simple checkpoints.\nevaluation/visualization.py: Plots sample inputs and reconstructions.\n\n\n\nFrom-scratch vs library-backed\n\nUse PyTorch for Dataset/DataLoader, AdamW, schedulers, AMP, and checkpointing.\nBuild core blocks from scratch first: PatchEmbedding, MHA, TransformerBlock, MAE loss/head.\nLater, swap in optimized options: torch.nn.MultiheadAttention, timm ViT blocks, FlashAttention, TorchGeo datasets, torchmetrics.\n\n\n\nQuick start (conceptual)\nfrom geogfm.core.config import ModelConfig, DataConfig, TrainConfig\nfrom geogfm.models.gfm_vit import GeoViTBackbone\nfrom geogfm.models.gfm_mae import MaskedAutoencoder\nfrom geogfm.data.loaders import build_dataloader\nfrom geogfm.training.loop import fit\n\nmodel_cfg = ModelConfig(architecture=\"gfm_vit\", embed_dim=768, depth=12, image_size=224)\ndata_cfg = DataConfig(dataset=\"stac\", patch_size=16, num_workers=8)\ntrain_cfg = TrainConfig(epochs=1, batch_size=8, optimizer={\"name\": \"adamw\", \"lr\": 2e-4})\n\nencoder = GeoViTBackbone(model_cfg)\nmodel = MaskedAutoencoder(model_cfg, encoder)\ntrain_dl, val_dl = build_dataloader(data_cfg)\nfit(model, (train_dl, val_dl), train_cfg)\nWhat to notice: - The encoder and MAE wrapper are separate, so we can reuse the encoder for other tasks later. - Data transforms (normalize/patchify) are decoupled from the model and driven by config.\n\n\nMVP vs later phases\n\nMVP (Weeks 1‚Äì6): files shown above; single-node training; basic logging and visualization.\nLater (Weeks 7‚Äì10): interop with existing models (e.g., Prithvi), task heads (classification/segmentation), inference tiling, Hub integration.\n\n\n\nComprehensive course roadmap (stages, files, libraries)\nThis table maps each week to the broader stages (see the course index), the key geogfm files you‚Äôll touch, and the primary deep learning tools you‚Äôll rely on.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n1\nStage 1: Build GFM Architecture\nData Foundations\ncore/config.py; data/datasets/stac_dataset.py; data/transforms/{normalization.py, patchify.py}; data/loaders.py\ntorch.utils.data.Dataset/DataLoader, rasterio, numpy\nConfig-driven dataloaders that yield normalized patches\n\n\n2\nStage 1\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n3\nStage 1\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking utilities, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n5\nStage 2\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n6\nStage 2\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n7\nStage 2\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n9\nStage 3\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n10\nStage 3\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan"
  },
  {
    "objectID": "course-materials/c04-pretraining-implementation.html",
    "href": "course-materials/c04-pretraining-implementation.html",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n\n\n\n\nImplement MAE wrapper with masking strategy\nBuild masked reconstruction loss; verify gradients\nOverfit a toy batch to confirm learning\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c04-pretraining-implementation.html#course-roadmap-mapping",
    "href": "course-materials/c04-pretraining-implementation.html#course-roadmap-mapping",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n\n\n\n\nImplement MAE wrapper with masking strategy\nBuild masked reconstruction loss; verify gradients\nOverfit a toy batch to confirm learning\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html",
    "href": "course-materials/c01-geospatial-data-foundations.html",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we‚Äôre building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text ‚Üí tokens ‚Üí embeddings. Our geospatial version: raw GeoTIFF ‚Üí patches ‚Üí embeddings."
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#introduction",
    "href": "course-materials/c01-geospatial-data-foundations.html#introduction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we‚Äôre building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text ‚Üí tokens ‚Üí embeddings. Our geospatial version: raw GeoTIFF ‚Üí patches ‚Üí embeddings."
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#course-roadmap-mapping",
    "href": "course-materials/c01-geospatial-data-foundations.html#course-roadmap-mapping",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Course Roadmap Mapping",
    "text": "Course Roadmap Mapping\nThis week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n1\nStage 1: Build GFM Architecture\nData Foundations\ncore/config.py; data/datasets/stac_dataset.py; data/transforms/{normalization.py, patchify.py}; data/loaders.py\ntorch.utils.data.Dataset/DataLoader, rasterio, numpy\nConfig-driven dataloaders that yield normalized patches\n\n\n\n\nWeekly goals\n\nImplement a minimal dataset, transforms, and dataloaders\nNormalize channels; extract patches deterministically\nVerify shapes/CRS/stats prints; run a tiny DataLoader"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#learning-objectives",
    "href": "course-materials/c01-geospatial-data-foundations.html#learning-objectives",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy building this pipeline, you will: - Implement GeoTIFF loading and preprocessing functions - Create patch extraction with spatial metadata - Build tensor normalization and encoding functions\n- Construct a PyTorch DataLoader for model training - Connect to a simple embedding layer to verify end-to-end functionality"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#session-roadmap",
    "href": "course-materials/c01-geospatial-data-foundations.html#session-roadmap",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\n\n\nflowchart TD\n    A[\"Setup & GeoTIFF Loading\"] --&gt; B[\"Geo Preprocessing Functions\"]\n    B --&gt; C[\"Patch Extraction with Metadata\"] \n    C --&gt; D[\"Tensor Operations & Normalization\"]\n    D --&gt; E[\"DataLoader Construction\"]\n    E --&gt; F[\"Embedding Layer Integration\"]\n    F --&gt; G[\"End-to-End Pipeline Test\"]"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#setting-up",
    "href": "course-materials/c01-geospatial-data-foundations.html#setting-up",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Setting Up",
    "text": "Setting Up\nLet‚Äôs establish our development environment and define the core constants we‚Äôll use throughout.\n\nImports and Configuration\n\n\nCode\nimport os\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport rasterio as rio\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Dict, Any\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Pipeline constants\nPATCH_SIZE = 64\nSTRIDE = 32  # 50% overlap\nBATCH_SIZE = 8\nEMBEDDING_DIM = 256\n\nprint(f\"‚úì Environment setup complete\")\nprint(f\"‚úì Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\nprint(f\"‚úì Stride: {STRIDE} (overlap: {(PATCH_SIZE-STRIDE)/PATCH_SIZE*100:.0f}%)\")\n\n\n‚úì Environment setup complete\n‚úì Patch size: 64x64\n‚úì Stride: 32 (overlap: 50%)\n\n\n\n\nData Preparation\n\n\nCode\n# Set up data paths - use book/data for course sample data\nif \"__file__\" in globals():\n    # From course-materials folder, go up 2 levels to book folder, then to data\n    DATA_DIR = Path(__file__).parent.parent / \"data\"\nelse:\n    # Fallback for interactive environments - look for book folder\n    current = Path.cwd()\n    while current.name not in [\"book\", \"geoAI\"] and current.parent != current:\n        current = current.parent\n    if current.name == \"book\":\n        DATA_DIR = current / \"data\"\n    elif current.name == \"geoAI\":\n        DATA_DIR = current / \"book\" / \"data\"\n    else:\n        DATA_DIR = Path(\"data\")\n\nDATA_DIR.mkdir(exist_ok=True)\nSAMPLE_PATH = DATA_DIR / \"landcover_sample.tif\"\n\n# Verify data file exists\nif not SAMPLE_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {SAMPLE_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(f\"‚úì Data ready: {SAMPLE_PATH.name}\")\nprint(f\"‚úì File size: {SAMPLE_PATH.stat().st_size / 1024:.1f} KB\")\nprint(f\"‚úì Full path: {SAMPLE_PATH}\")\n\n\n‚úì Data ready: landcover_sample.tif\n‚úì File size: 12.6 KB\n‚úì Full path: /Users/kellycaylor/dev/geoAI/book/data/landcover_sample.tif"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-1-geotiff-loading-and-inspection",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-1-geotiff-loading-and-inspection",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 1: GeoTIFF Loading and Inspection",
    "text": "Step 1: GeoTIFF Loading and Inspection\nGoal: Build a function that loads and extracts essential information from any GeoTIFF.\n\nüõ†Ô∏è Build It: GeoTIFF Loader Function\nYour task: Complete this function to load a GeoTIFF and return both the data and metadata.\n\n\nCode\ndef load_geotiff(file_path: Path) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n    \"\"\"\n    Load a GeoTIFF and extract data + metadata.\n    \n    Returns:\n        data: (bands, height, width) array\n        metadata: dict with CRS, transform, resolution, etc.\n    \"\"\"\n    with rio.open(file_path) as src:\n        # TODO: Load the data array\n        data = src.read()  # YOUR CODE: Load raster data\n        \n        # TODO: Extract metadata\n        metadata = {\n            'crs': src.crs,  # YOUR CODE: Get coordinate reference system\n            'transform': src.transform,  # YOUR CODE: Get geospatial transform\n            'shape': data.shape,  # YOUR CODE: Get array dimensions\n            'dtype': data.dtype,  # YOUR CODE: Get data type\n            'resolution': src.res,  # YOUR CODE: Get pixel resolution\n            'bounds': src.bounds,  # YOUR CODE: Get spatial bounds\n        }\n    \n    return data, metadata\n\n# Test your function\ndata, metadata = load_geotiff(SAMPLE_PATH)\nprint(f\"‚úì Loaded shape: {data.shape}\")\nprint(f\"‚úì Data type: {metadata['dtype']}\")\nprint(f\"‚úì Resolution: {metadata['resolution']}\")\nprint(f\"‚úì CRS: {metadata['crs']}\")\n\n\n‚úì Loaded shape: (3, 64, 64)\n‚úì Data type: uint8\n‚úì Resolution: (0.25, 0.25)\n‚úì CRS: PROJCS[\"Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",19],PARAMETER[\"scale_factor\",0.9993],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",-5300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n\n\n\n\nüîç Verify It: Inspect Your Data\n\n\nCode\n# Examine the data you loaded\nbands, height, width = data.shape\nprint(f\"Image dimensions: {height}√ó{width} pixels\")\nprint(f\"Number of bands: {bands}\")\nprint(f\"Value ranges per band:\")\nfor i, band in enumerate(data):\n    print(f\"  Band {i+1}: {band.min():.0f} to {band.max():.0f}\")\n\n# Quick visualization\nfig, axes = plt.subplots(1, bands, figsize=(12, 4))\nif bands == 1:\n    axes = [axes]\n\nfor i, band in enumerate(data):\n    axes[i].imshow(band, cmap='viridis')\n    axes[i].set_title(f'Band {i+1}')\n    axes[i].axis('off')\n\nplt.suptitle('Raw Satellite Bands')\nplt.tight_layout()\nplt.show()\n\n\nImage dimensions: 64√ó64 pixels\nNumber of bands: 3\nValue ranges per band:\n  Band 1: 0 to 254\n  Band 2: 0 to 254\n  Band 3: 0 to 254"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-2-geo-preprocessing-functions",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-2-geo-preprocessing-functions",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 2: Geo Preprocessing Functions",
    "text": "Step 2: Geo Preprocessing Functions\nGoal: Build preprocessing functions that operate on the full image before patch extraction.\n\nüõ†Ô∏è Build It: Normalization Functions\nWe‚Äôll create two normalization functions that can work with either local statistics (calculated from the input data) or global statistics (pre-computed from a training dataset). Global statistics ensure consistent normalization across different image tiles and are crucial for foundation model training.\nWhy use global statistics? When training on multiple images, each tile might have different value ranges. Using global statistics ensures that the same pixel value represents the same relative intensity across all training data.\n\nMin-Max Normalization Function\n\n\nCode\ndef minmax_normalize(data: np.ndarray, \n                    global_min: np.ndarray = None, \n                    global_max: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Min-max normalize spectral bands to [0,1] range.\n    \n    Args:\n        data: (bands, height, width) array\n        global_min: Optional (bands,) array of global minimums per band\n        global_max: Optional (bands,) array of global maximums per band\n    \n    Returns:\n        normalized: (bands, height, width) array with values in [0,1]\n        stats: Dictionary containing the min/max values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_min is None or global_max is None:\n        # Calculate per-band statistics from this data\n        mins = np.array([data[i].min() for i in range(bands)])\n        maxs = np.array([data[i].max() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        mins = global_min\n        maxs = global_max\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        band_range = maxs[i] - mins[i]\n        if band_range &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - mins[i]) / band_range\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'mins': mins,\n        'maxs': maxs,\n        'output_range': (normalized.min(), normalized.max())\n    }\n    \n    return normalized, stats\n\n\n\n\nZ-Score Normalization Function\n\n\nCode\ndef zscore_normalize(data: np.ndarray,\n                    global_mean: np.ndarray = None,\n                    global_std: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Z-score normalize spectral bands to mean=0, std=1.\n    \n    Args:\n        data: (bands, height, width) array\n        global_mean: Optional (bands,) array of global means per band\n        global_std: Optional (bands,) array of global standard deviations per band\n    \n    Returns:\n        normalized: (bands, height, width) standardized array\n        stats: Dictionary containing the mean/std values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_mean is None or global_std is None:\n        # Calculate per-band statistics from this data\n        means = np.array([data[i].mean() for i in range(bands)])\n        stds = np.array([data[i].std() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        means = global_mean\n        stds = global_std\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        if stds[i] &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - means[i]) / stds[i]\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'means': means,\n        'stds': stds,\n        'output_mean': normalized.mean(),\n        'output_std': normalized.std()\n    }\n    \n    return normalized, stats\n\nprint(\"‚úì Normalization functions created\")\nprint(\"  - minmax_normalize: scales to [0,1] range\")\nprint(\"  - zscore_normalize: standardizes to mean=0, std=1\")\n\n\n‚úì Normalization functions created\n  - minmax_normalize: scales to [0,1] range\n  - zscore_normalize: standardizes to mean=0, std=1\n\n\n\n\nTest Both Functions with Local Statistics\n\n\nCode\n# Test min-max normalization with local statistics\nminmax_data, minmax_stats = minmax_normalize(data)\nprint(\"üìä Min-Max Normalization (local stats):\")\nprint(f\"  Source: {minmax_stats['source']}\")\nprint(f\"  Original range: {data.min():.0f} to {data.max():.0f}\")\nprint(f\"  Normalized range: {minmax_stats['output_range'][0]:.3f} to {minmax_stats['output_range'][1]:.3f}\")\nprint(f\"  Per-band mins: {minmax_stats['mins']}\")\nprint(f\"  Per-band maxs: {minmax_stats['maxs']}\")\n\nprint()\n\n# Test z-score normalization with local statistics  \nzscore_data, zscore_stats = zscore_normalize(data)\nprint(\"üìä Z-Score Normalization (local stats):\")\nprint(f\"  Source: {zscore_stats['source']}\")\nprint(f\"  Output mean: {zscore_stats['output_mean']:.6f}\")\nprint(f\"  Output std: {zscore_stats['output_std']:.6f}\")\nprint(f\"  Per-band means: {zscore_stats['means']}\")\nprint(f\"  Per-band stds: {zscore_stats['stds']}\")\n\n\nüìä Min-Max Normalization (local stats):\n  Source: local (calculated from input)\n  Original range: 0 to 254\n  Normalized range: 0.000 to 1.000\n  Per-band mins: [0 0 0]\n  Per-band maxs: [254 254 254]\n\nüìä Z-Score Normalization (local stats):\n  Source: local (calculated from input)\n  Output mean: 0.000000\n  Output std: 1.000000\n  Per-band means: [126.14306641 126.14306641 126.14306641]\n  Per-band stds: [73.10237725 73.10237725 73.10237725]\n\n\n\n\nTest with Global Statistics\n\n\nCode\n# Simulate global statistics from a larger dataset\n# In practice, these would be pre-computed from your entire training corpus\nglobal_mins = np.array([100, 150, 200])  # Example global minimums per band\nglobal_maxs = np.array([1500, 2000, 2500])  # Example global maximums per band\nglobal_means = np.array([800, 1200, 1600])  # Example global means per band\nglobal_stds = np.array([300, 400, 500])  # Example global standard deviations per band\n\nprint(\"üåç Testing with Global Statistics:\")\nprint(f\"  Global mins: {global_mins}\")\nprint(f\"  Global maxs: {global_maxs}\")\nprint(f\"  Global means: {global_means}\")\nprint(f\"  Global stds: {global_stds}\")\n\nprint()\n\n# Test with global statistics\nminmax_global, minmax_global_stats = minmax_normalize(data, global_mins, global_maxs)\nzscore_global, zscore_global_stats = zscore_normalize(data, global_means, global_stds)\n\nprint(\"üìä Min-Max with Global Stats:\")\nprint(f\"  Source: {minmax_global_stats['source']}\")\nprint(f\"  Output range: {minmax_global_stats['output_range'][0]:.3f} to {minmax_global_stats['output_range'][1]:.3f}\")\n\nprint()\n\nprint(\"üìä Z-Score with Global Stats:\")\nprint(f\"  Source: {zscore_global_stats['source']}\")\nprint(f\"  Output mean: {zscore_global_stats['output_mean']:.3f}\")\nprint(f\"  Output std: {zscore_global_stats['output_std']:.3f}\")\n\n\nüåç Testing with Global Statistics:\n  Global mins: [100 150 200]\n  Global maxs: [1500 2000 2500]\n  Global means: [ 800 1200 1600]\n  Global stds: [300 400 500]\n\nüìä Min-Max with Global Stats:\n  Source: global (provided)\n  Output range: 0.000 to 0.182\n\nüìä Z-Score with Global Stats:\n  Source: global (provided)\n  Output mean: 168.496\n  Output std: 36.333\n\n\nWhat to notice: When using global statistics, the output ranges and distributions differ from local normalization. This is expected and ensures consistency across different image tiles in your dataset.\n\n\n\nüõ†Ô∏è Build It: Spatial Cropping Function\n\n\nCode\ndef crop_to_patches(data: np.ndarray, patch_size: int, stride: int) -&gt; np.ndarray:\n    \"\"\"\n    Crop image to dimensions that allow complete patch extraction.\n    \n    Args:\n        data: (bands, height, width) array\n        patch_size: size of patches to extract\n        stride: step size between patches\n        \n    Returns:\n        cropped: (bands, new_height, new_width) array\n    \"\"\"\n    bands, height, width = data.shape\n    \n    # TODO: Calculate how many complete patches fit\n    patches_h = (height - patch_size) // stride + 1\n    patches_w = (width - patch_size) // stride + 1\n    \n    # TODO: Calculate the required dimensions\n    new_height = (patches_h - 1) * stride + patch_size\n    new_width = (patches_w - 1) * stride + patch_size\n    \n    # TODO: Crop the data\n    cropped = data[:, :new_height, :new_width]\n    \n    print(f\"‚úì Cropped from {height}√ó{width} to {new_height}√ó{new_width}\")\n    print(f\"‚úì Will generate {patches_h}√ó{patches_w} = {patches_h*patches_w} patches\")\n    \n    return cropped\n\n# Test your cropping function\ncropped_data = crop_to_patches(data, 8, STRIDE)\n\n\n‚úì Cropped from 64√ó64 to 40√ó40\n‚úì Will generate 2√ó2 = 4 patches"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-3-patch-extraction-with-metadata",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-3-patch-extraction-with-metadata",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 3: Patch Extraction with Metadata",
    "text": "Step 3: Patch Extraction with Metadata\nGoal: Extract patches while preserving spatial context information.\n\nüõ†Ô∏è Build It: Patch Extraction Function\n\n\nCode\ndef extract_patches_with_metadata(\n    data: np.ndarray, \n    transform,\n    patch_size: int, \n    stride: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Extract patches with their spatial coordinates.\n    \n    Args:\n        data: (bands, height, width) normalized array\n        transform: rasterio transform object\n        patch_size: size of patches\n        stride: step between patches\n        \n    Returns:\n        patches: (n_patches, bands, patch_size, patch_size) array\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n    \"\"\"\n    bands, height, width = data.shape\n    patches = []\n    coordinates = []\n    \n    # TODO: Iterate through patch positions\n    for row in range(0, height - patch_size + 1, stride):\n        for col in range(0, width - patch_size + 1, stride):\n            # TODO: Extract patch from all bands\n            patch = data[:, row:row+patch_size, col:col+patch_size]\n            patches.append(patch)\n            \n            # TODO: Calculate real-world coordinates using transform\n            min_x, max_y = transform * (col, row)  # Top-left\n            max_x, min_y = transform * (col + patch_size, row + patch_size)  # Bottom-right\n            coordinates.append([min_x, min_y, max_x, max_y])\n    \n    patches = np.array(patches)\n    coordinates = np.array(coordinates)\n    \n    print(f\"‚úì Extracted {len(patches)} patches\")\n    print(f\"‚úì Patch shape: {patches.shape}\")\n    print(f\"‚úì Coordinate shape: {coordinates.shape}\")\n    \n    return patches, coordinates\n\n# Test your patch extraction\npatches, coords = extract_patches_with_metadata(\n    data, metadata['transform'], 8, 4\n)\n\n# Visualize a few patches\nfig, axes = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(8):\n    row, col = i // 4, i % 4\n    # Show first band of each patch\n    axes[row, col].imshow(patches[i, 0], cmap='viridis')\n    axes[row, col].set_title(f'Patch {i}')\n    axes[row, col].axis('off')\n\nplt.suptitle('Sample Extracted Patches (Band 1)')\nplt.tight_layout()\nplt.show()\n\n\n‚úì Extracted 225 patches\n‚úì Patch shape: (225, 3, 8, 8)\n‚úì Coordinate shape: (225, 4)"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-4-tensor-operations-metadata-encoding",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-4-tensor-operations-metadata-encoding",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 4: Tensor Operations & Metadata Encoding",
    "text": "Step 4: Tensor Operations & Metadata Encoding\nGoal: Convert numpy arrays to PyTorch tensors and encode metadata.\n\nüõ†Ô∏è Build It: Metadata Encoder\n\n\nCode\ndef encode_metadata(coordinates: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Encode spatial metadata as features.\n    \n    Args:\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n        \n    Returns:\n        encoded: (n_patches, n_features) array\n    \"\"\"\n    # TODO: Calculate spatial features\n    center_x = (coordinates[:, 0] + coordinates[:, 2]) / 2\n    center_y = (coordinates[:, 1] + coordinates[:, 3]) / 2\n    width = coordinates[:, 2] - coordinates[:, 0]\n    height = coordinates[:, 3] - coordinates[:, 1]\n    area = width * height\n    \n    # TODO: Normalize spatial features (handle zero std to avoid divide by zero)\n    def safe_normalize(values):\n        \"\"\"Normalize values, handling zero standard deviation.\"\"\"\n        mean_val = values.mean()\n        std_val = values.std()\n        if std_val &gt; 0:\n            return (values - mean_val) / std_val\n        else:\n            return np.zeros_like(values)  # All values are the same\n    \n    features = np.column_stack([\n        safe_normalize(center_x),     # Normalized center X\n        safe_normalize(center_y),     # Normalized center Y  \n        safe_normalize(area),         # Normalized area (handles constant area)\n        width / height,               # Aspect ratio\n    ])\n    \n    print(f\"‚úì Encoded metadata shape: {features.shape}\")\n    print(f\"‚úì Feature statistics:\")\n    feature_names = ['center_x', 'center_y', 'area', 'aspect_ratio']\n    for i, name in enumerate(feature_names):\n        print(f\"  {name}: mean={features[:, i].mean():.3f}, std={features[:, i].std():.3f}\")\n    \n    return features.astype(np.float32)\n\n# Test metadata encoding\nencoded_metadata = encode_metadata(coords)\n\n\n‚úì Encoded metadata shape: (225, 4)\n‚úì Feature statistics:\n  center_x: mean=0.000, std=1.000\n  center_y: mean=0.000, std=1.000\n  area: mean=0.000, std=0.000\n  aspect_ratio: mean=1.000, std=0.000\n\n\n\n\nüõ†Ô∏è Build It: Tensor Conversion\n\n\nCode\ndef create_tensors(patches: np.ndarray, metadata: np.ndarray) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Convert numpy arrays to PyTorch tensors.\n    \n    Args:\n        patches: (n_patches, bands, height, width) array\n        metadata: (n_patches, n_features) array\n        \n    Returns:\n        patch_tensors: (n_patches, bands, height, width) tensor\n        metadata_tensors: (n_patches, n_features) tensor\n    \"\"\"\n    # TODO: Convert to tensors with appropriate dtypes\n    patch_tensors = torch.from_numpy(patches).float()\n    metadata_tensors = torch.from_numpy(metadata).float()\n    \n    print(f\"‚úì Patch tensors: {patch_tensors.shape}, dtype: {patch_tensors.dtype}\")\n    print(f\"‚úì Metadata tensors: {metadata_tensors.shape}, dtype: {metadata_tensors.dtype}\")\n    \n    return patch_tensors, metadata_tensors\n\n# Create tensors\npatch_tensors, metadata_tensors = create_tensors(patches, encoded_metadata)\n\n\n‚úì Patch tensors: torch.Size([225, 3, 8, 8]), dtype: torch.float32\n‚úì Metadata tensors: torch.Size([225, 4]), dtype: torch.float32"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-5-dataloader-construction",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-5-dataloader-construction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 5: DataLoader Construction",
    "text": "Step 5: DataLoader Construction\nGoal: Build a PyTorch Dataset and DataLoader for training.\n\nüõ†Ô∏è Build It: Custom Dataset Class\n\n\nCode\nclass GeospatialDataset(Dataset):\n    \"\"\"Dataset for geospatial patches with metadata.\"\"\"\n    \n    def __init__(self, patch_tensors: torch.Tensor, metadata_tensors: torch.Tensor):\n        \"\"\"\n        Args:\n            patch_tensors: (n_patches, bands, height, width)\n            metadata_tensors: (n_patches, n_features)\n        \"\"\"\n        self.patches = patch_tensors\n        self.metadata = metadata_tensors\n        \n        # TODO: Create dummy labels for demonstration (in real use, load from file)\n        self.labels = torch.randint(0, 5, (len(patch_tensors),))  # 5 land cover classes\n        \n    def __len__(self) -&gt; int:\n        \"\"\"Return number of patches.\"\"\"\n        return len(self.patches)\n    \n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Get a single item.\n        \n        Returns:\n            patch: (bands, height, width) tensor\n            metadata: (n_features,) tensor  \n            label: scalar tensor\n        \"\"\"\n        return self.patches[idx], self.metadata[idx], self.labels[idx]\n\n# Test your dataset\ndataset = GeospatialDataset(patch_tensors, metadata_tensors)\nprint(f\"‚úì Dataset length: {len(dataset)}\")\n\n# Test getting an item\nsample_patch, sample_metadata, sample_label = dataset[0]\nprint(f\"‚úì Sample patch shape: {sample_patch.shape}\")\nprint(f\"‚úì Sample metadata shape: {sample_metadata.shape}\")\nprint(f\"‚úì Sample label: {sample_label.item()}\")\n\n\n‚úì Dataset length: 225\n‚úì Sample patch shape: torch.Size([3, 8, 8])\n‚úì Sample metadata shape: torch.Size([4])\n‚úì Sample label: 2\n\n\n\n\nüõ†Ô∏è Build It: DataLoader\n\n\nCode\n# TODO: Create DataLoader with appropriate batch size and shuffling\ndataloader = DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n    num_workers=0,  # Set to 0 for compatibility\n    pin_memory=True if torch.cuda.is_available() else False\n)\n\nprint(f\"‚úì DataLoader created with batch size {BATCH_SIZE}\")\nprint(f\"‚úì Number of batches: {len(dataloader)}\")\n\n# Test the DataLoader\nfor batch_idx, (patches, metadata, labels) in enumerate(dataloader):\n    print(f\"‚úì Batch {batch_idx}:\")\n    print(f\"  Patches: {patches.shape}\")\n    print(f\"  Metadata: {metadata.shape}\")\n    print(f\"  Labels: {labels.shape}\")\n    if batch_idx == 1:  # Show first two batches\n        break\n\n\n‚úì DataLoader created with batch size 8\n‚úì Number of batches: 29\n‚úì Batch 0:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])\n‚úì Batch 1:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-6-embedding-layer-integration",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-6-embedding-layer-integration",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 6: Embedding Layer Integration",
    "text": "Step 6: Embedding Layer Integration\nGoal: Connect to a simple embedding layer to verify end-to-end functionality.\n\nüõ†Ô∏è Build It: Simple GFM Embedding Layer\n\n\nCode\nclass SimpleGFMEmbedding(nn.Module):\n    \"\"\"Simple embedding layer for geospatial patches.\"\"\"\n    \n    def __init__(self, input_channels: int, metadata_features: int, embed_dim: int, patch_size: int = 64):\n        super().__init__()\n        \n        # TODO: Build adaptive patch encoder based on patch size\n        if patch_size &gt;= 32:\n            # Larger patches: multi-layer CNN\n            kernel1 = min(8, patch_size // 4)\n            kernel2 = min(4, patch_size // 8)\n            self.patch_encoder = nn.Sequential(\n                nn.Conv2d(input_channels, 32, kernel_size=kernel1, stride=kernel1//2),\n                nn.ReLU(),\n                nn.Conv2d(32, 64, kernel_size=kernel2, stride=kernel2//2), \n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d(1),\n                nn.Flatten(),\n            )\n        else:\n            # Smaller patches: simpler encoder\n            kernel = min(4, patch_size // 2)\n            self.patch_encoder = nn.Sequential(\n                nn.Conv2d(input_channels, 64, kernel_size=kernel, stride=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d(1),\n                nn.Flatten(),\n            )\n        \n        # TODO: Build metadata encoder\n        self.metadata_encoder = nn.Sequential(\n            nn.Linear(metadata_features, 32),\n            nn.ReLU(),\n            nn.Linear(32, 32),\n        )\n        \n        # TODO: Build fusion layer\n        # Calculate patch encoder output size\n        with torch.no_grad():\n            dummy_patch = torch.randn(1, input_channels, patch_size, patch_size)\n            patch_feat_size = self.patch_encoder(dummy_patch).shape[1]\n        \n        self.fusion = nn.Sequential(\n            nn.Linear(patch_feat_size + 32, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim),\n        )\n        \n    def forward(self, patches: torch.Tensor, metadata: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            patches: (batch, channels, height, width)\n            metadata: (batch, n_features)\n            \n        Returns:\n            embeddings: (batch, embed_dim)\n        \"\"\"\n        # TODO: Encode patches and metadata\n        patch_features = self.patch_encoder(patches)\n        metadata_features = self.metadata_encoder(metadata)\n        \n        # TODO: Fuse features\n        combined = torch.cat([patch_features, metadata_features], dim=1)\n        embeddings = self.fusion(combined)\n        \n        return embeddings\n\n# Create and test the model\nmodel = SimpleGFMEmbedding(\n    input_channels=bands, \n    metadata_features=encoded_metadata.shape[1], \n    embed_dim=EMBEDDING_DIM,\n    patch_size=PATCH_SIZE\n)\n\nprint(f\"‚úì Model created\")\nprint(f\"‚úì Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n\n‚úì Model created\n‚úì Model parameters: 130,848"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-7-end-to-end-pipeline-test",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-7-end-to-end-pipeline-test",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 7: End-to-End Pipeline Test",
    "text": "Step 7: End-to-End Pipeline Test\nGoal: Run the complete pipeline and verify everything works together.\n\nüõ†Ô∏è Build It: Complete Pipeline Function\n\n\nCode\ndef geotiff_to_embeddings_pipeline(\n    file_path: Path,\n    patch_size: int = 8,\n    stride: int = 4,\n    batch_size: int = 8,\n    embed_dim: int = 256\n) -&gt; torch.Tensor:\n    \"\"\"\n    Complete pipeline from GeoTIFF to embeddings.\n    \n    Args:\n        file_path: Path to GeoTIFF file\n        patch_size: Size of patches to extract\n        stride: Step between patches  \n        batch_size: Batch size for processing\n        embed_dim: Embedding dimension\n        \n    Returns:\n        all_embeddings: (n_patches, embed_dim) tensor\n    \"\"\"\n    print(\"üöÄ Starting GeoTIFF ‚Üí Embeddings Pipeline\")\n    \n    # Step 1: Load data\n    print(\"üìÅ Loading GeoTIFF...\")\n    data, metadata = load_geotiff(file_path)\n    \n    # Step 2: Preprocess\n    print(\"üîß Preprocessing...\")\n    norm_data, norm_stats = minmax_normalize(data)\n    cropped_data = crop_to_patches(norm_data, patch_size, stride)\n    \n    # Step 3: Extract patches\n    print(\"‚úÇÔ∏è Extracting patches...\")\n    patches, coords = extract_patches_with_metadata(cropped_data, metadata['transform'], patch_size, stride)\n    \n    # Step 4: Encode metadata\n    print(\"üìä Encoding metadata...\")\n    encoded_meta = encode_metadata(coords)\n    \n    # Step 5: Create tensors\n    print(\"üî¢ Creating tensors...\")\n    patch_tensors, meta_tensors = create_tensors(patches, encoded_meta)\n    \n    # Step 6: Create dataset and dataloader\n    print(\"üì¶ Creating DataLoader...\")\n    dataset = GeospatialDataset(patch_tensors, meta_tensors)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    # Step 7: Create model and generate embeddings\n    print(\"üß† Generating embeddings...\")\n    model = SimpleGFMEmbedding(\n        input_channels=data.shape[0],\n        metadata_features=encoded_meta.shape[1], \n        embed_dim=embed_dim,\n        patch_size=patch_size\n    )\n    model.eval()\n    \n    all_embeddings = []\n    with torch.no_grad():\n        for patches_batch, meta_batch, _ in dataloader:\n            embeddings = model(patches_batch, meta_batch)\n            all_embeddings.append(embeddings)\n    \n    all_embeddings = torch.cat(all_embeddings, dim=0)\n    print(f\"‚úÖ Pipeline complete! Generated {len(all_embeddings)} embeddings\")\n    \n    return all_embeddings\n\n# Run the complete pipeline\nembeddings = geotiff_to_embeddings_pipeline(SAMPLE_PATH)\nprint(f\"\\nüéâ Final Result:\")\nprint(f\"‚úì Embeddings shape: {embeddings.shape}\")\nprint(f\"‚úì Embedding statistics:\")\nprint(f\"  Mean: {embeddings.mean().item():.4f}\")\nprint(f\"  Std: {embeddings.std().item():.4f}\")\nprint(f\"  Min: {embeddings.min().item():.4f}\")\nprint(f\"  Max: {embeddings.max().item():.4f}\")\n\n\nüöÄ Starting GeoTIFF ‚Üí Embeddings Pipeline\nüìÅ Loading GeoTIFF...\nüîß Preprocessing...\n‚úì Cropped from 64√ó64 to 64√ó64\n‚úì Will generate 15√ó15 = 225 patches\n‚úÇÔ∏è Extracting patches...\n‚úì Extracted 225 patches\n‚úì Patch shape: (225, 3, 8, 8)\n‚úì Coordinate shape: (225, 4)\nüìä Encoding metadata...\n‚úì Encoded metadata shape: (225, 4)\n‚úì Feature statistics:\n  center_x: mean=0.000, std=1.000\n  center_y: mean=0.000, std=1.000\n  area: mean=0.000, std=0.000\n  aspect_ratio: mean=1.000, std=0.000\nüî¢ Creating tensors...\n‚úì Patch tensors: torch.Size([225, 3, 8, 8]), dtype: torch.float32\n‚úì Metadata tensors: torch.Size([225, 4]), dtype: torch.float32\nüì¶ Creating DataLoader...\nüß† Generating embeddings...\n‚úÖ Pipeline complete! Generated 225 embeddings\n\nüéâ Final Result:\n‚úì Embeddings shape: torch.Size([225, 256])\n‚úì Embedding statistics:\n  Mean: 0.0023\n  Std: 0.0716\n  Min: -0.2246\n  Max: 0.2414\n\n\n\n\nüîç Verify It: Pipeline Output Analysis\n\n\nCode\n# Visualize embedding similarities\nprint(\"üîç Analyzing embedding relationships...\")\n\n# Check if we have enough embeddings for analysis\nif len(embeddings) &lt; 10:\n    print(f\"‚ö†Ô∏è Only {len(embeddings)} embeddings available, using all of them\")\n    sample_size = len(embeddings)\nelse:\n    print(f\"‚úì Using first 10 of {len(embeddings)} embeddings for similarity analysis\")\n    sample_size = 10\n\nif sample_size &gt; 1:\n    # Compute pairwise cosine similarities\n    from torch.nn.functional import cosine_similarity\n    \n    sample_embeddings = embeddings[:sample_size]\n    similarity_matrix = torch.zeros(sample_size, sample_size)\n    \n    for i in range(sample_size):\n        for j in range(sample_size):\n            if i == j:\n                similarity_matrix[i, j] = 1.0  # Perfect self-similarity\n            else:\n                sim = cosine_similarity(sample_embeddings[i:i+1], sample_embeddings[j:j+1], dim=1)\n                similarity_matrix[i, j] = sim.item()\n    \n    # Plot similarity matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(similarity_matrix.numpy(), cmap='viridis', vmin=-1, vmax=1)\n    plt.colorbar(label='Cosine Similarity')\n    plt.title(f'Embedding Similarity Matrix (First {sample_size} Patches)')\n    plt.xlabel('Patch Index')\n    plt.ylabel('Patch Index')\n    plt.show()\n    \n    print(f\"‚úì Average similarity: {similarity_matrix.mean().item():.4f}\")\n    print(f\"‚úì Similarity range: {similarity_matrix.min().item():.4f} to {similarity_matrix.max().item():.4f}\")\nelse:\n    print(\"‚ö†Ô∏è Not enough embeddings for similarity analysis\")\n\n\nüîç Analyzing embedding relationships...\n‚úì Using first 10 of 225 embeddings for similarity analysis\n\n\n\n\n\n\n\n\n\n‚úì Average similarity: 0.9832\n‚úì Similarity range: 0.9332 to 1.0000"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#conclusion",
    "href": "course-materials/c01-geospatial-data-foundations.html#conclusion",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Conclusion",
    "text": "Conclusion\nüéâ Congratulations! You‚Äôve successfully built a complete pipeline that transforms raw satellite imagery into model-ready embeddings.\n\nWhat You Built:\n\nGeoTIFF Loader: Extracts both pixel data and spatial metadata\nPreprocessing Functions: Normalization and spatial cropping\n\nPatch Extractor: Creates patches while preserving spatial context\nMetadata Encoder: Transforms coordinates into learned features\nPyTorch Integration: Dataset, DataLoader, and model components\nEmbedding Generator: Simple CNN that produces vector representations\n\n\n\nKey Insights:\n\nSpatial Context Matters: Each patch carries location information\nPreprocessing is Critical: Normalization ensures stable training\n\nModular Design: Each step can be optimized independently\nEnd-to-End Testing: Verify the complete pipeline works\n\n\n\nWhat‚Äôs Next:\nIn the following sessions, you‚Äôll enhance each component: - Week 2: Advanced attention mechanisms for spatial relationships - Week 3: Complete GFM architecture with transformer blocks - Week 4: Pretraining strategies and masked autoencoding\nThe pipeline you built today forms the foundation for everything that follows! üöÄ"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#resources",
    "href": "course-materials/c01-geospatial-data-foundations.html#resources",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Resources",
    "text": "Resources\n\nPyTorch DataLoader Documentation\nRasterio User Guide\nGeospatial Foundation Model Examples"
  },
  {
    "objectID": "course-materials/c08-task-specific-finetuning.html",
    "href": "course-materials/c08-task-specific-finetuning.html",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n\n\n\n\nImplement a simple classifier/segmentation head\nFine-tune with frozen encoder; evaluate on a tiny dataset\nDiscuss efficient strategies (LoRA/prompting as concepts)\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c08-task-specific-finetuning.html#course-roadmap-mapping",
    "href": "course-materials/c08-task-specific-finetuning.html#course-roadmap-mapping",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n\n\n\n\nImplement a simple classifier/segmentation head\nFine-tune with frozen encoder; evaluate on a tiny dataset\nDiscuss efficient strategies (LoRA/prompting as concepts)\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c09-model-implementation-deployment.html",
    "href": "course-materials/c09-model-implementation-deployment.html",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n9\nStage 3: Apply & Deploy\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n\n\n\n\nImplement sliding-window/tiling inference utilities\nStitch predictions; sanity-check outputs\nOutline API/UI deployment considerations\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c09-model-implementation-deployment.html#course-roadmap-mapping",
    "href": "course-materials/c09-model-implementation-deployment.html#course-roadmap-mapping",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n9\nStage 3: Apply & Deploy\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n\n\n\n\nImplement sliding-window/tiling inference utilities\nStitch predictions; sanity-check outputs\nOutline API/UI deployment considerations\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/extras/projects/project-proposal-template.html",
    "href": "course-materials/extras/projects/project-proposal-template.html",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you‚Äôre addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] ‚Üí Mitigation: [How you‚Äôll address it] - Risk 2: [Description] ‚Üí Mitigation: [How you‚Äôll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn‚Äôt work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you‚Äôll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/extras/projects/project-proposal-template.html#project-proposal-due-week-3",
    "href": "course-materials/extras/projects/project-proposal-template.html#project-proposal-due-week-3",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you‚Äôre addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] ‚Üí Mitigation: [How you‚Äôll address it] - Risk 2: [Description] ‚Üí Mitigation: [How you‚Äôll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn‚Äôt work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you‚Äôll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html",
    "href": "course-materials/extras/projects/mvp-template.html",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you‚Äôre solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input ‚Üí Preprocessing ‚Üí Model ‚Üí Post-processing ‚Üí Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you‚Äôve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn‚Äôt work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You‚Äôd Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "href": "course-materials/extras/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you‚Äôre solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input ‚Üí Preprocessing ‚Üí Model ‚Üí Post-processing ‚Üí Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you‚Äôve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn‚Äôt work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You‚Äôd Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html#mvp-demonstration-checklist",
    "href": "course-materials/extras/projects/mvp-template.html#mvp-demonstration-checklist",
    "title": "Initial MVP Template",
    "section": "MVP Demonstration Checklist",
    "text": "MVP Demonstration Checklist\n\nBefore Your Presentation:\n\nTest all code and demos on presentation computer\nPrepare backup slides in case live demo fails\nTime your presentation (aim for 10-12 minutes + 3-5 for Q&A)\nHave clear talking points for each section\nSave sample outputs/screenshots as backup\n\n\n\nDuring Presentation:\n\nSpeak clearly and at appropriate pace\nShow your screen clearly for demos\nEngage with questions and feedback\nStay within time limits\nBe honest about current limitations\n\n\n\nAfter Presentation:\n\nTake notes on feedback received\nUpdate project plan based on suggestions\nSchedule follow-up consultations if needed"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html",
    "href": "course-materials/extras/resources/course_resources.html",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) ‚Äì Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) ‚Äì Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) ‚Äì Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 ‚Äì A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#essential-background-and-foundational-resources",
    "href": "course-materials/extras/resources/course_resources.html#essential-background-and-foundational-resources",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) ‚Äì Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) ‚Äì Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) ‚Äì Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 ‚Äì A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#week-by-week-resources",
    "href": "course-materials/extras/resources/course_resources.html#week-by-week-resources",
    "title": "GeoAI Course Resources",
    "section": "Week-by-Week Resources",
    "text": "Week-by-Week Resources\n\nWeek 1: Introduction to Geospatial Foundation Models\n\nüåê Development Seed Blog (2024): Using Foundation Models for Earth Observation\nüöÄ NASA/IBM Release: Prithvi HLS Foundation Model\n‚òÅÔ∏è AWS ML Blog (2025): Deploying GeoFMs on AWS\n\n\n\nWeek 2: Working with Geospatial Data\n\nüìö Book: Learning Geospatial Analysis with Python (4th ed., 2023)\nüß∞ TorchGeo Docs: https://pytorch.org/geo\nüåç OpenGeoAI Toolkit: https://github.com/opengeos/geoai\n\n\n\nWeek 3: Loading & Using Foundation Models\n\nü§ó Hugging Face Prithvi Models: https://huggingface.co/ibm-nasa-geospatial\nüìì Demo Notebooks: Available on Hugging Face model cards.\nüß™ AWS GeoFM Deployment Examples: Code and pipelines via GitHub.\nüß† IEEE GRSS Blog: Self-Supervised Geospatial Backbones\n\n\n\nWeek 4: Multi-modal & Generative Models\n\nüõ∞Ô∏è Sensor Fusion via TorchGeo: Stack Sentinel-1/2 & others.\nüåê Presto, Clay, DOFA Models: Explained in DevSeed & AWS blogs.\nüß™ DiffusionSat Example: Medium Tutorial on Generating EO Images\n\n\n\nWeek 5: Model Adaptation & Evaluation\n\nüîå Adapters for GeoFM: Explained via Development Seed blog.\nüìä Evaluation Metrics: IoU, mAP, F1 from Boutayeb et al.¬†(2024) + SpaceNet.\nüìÅ Radiant Earth MLHub: https://mlhub.earth ‚Äì Public geospatial ML datasets and benchmarks."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#interactive-sessions",
    "href": "course-materials/extras/resources/course_resources.html#interactive-sessions",
    "title": "GeoAI Course Resources",
    "section": "Interactive Sessions",
    "text": "Interactive Sessions\n\n1Ô∏è‚É£ PyTorch & TorchGeo Fundamentals\n\nTorchGeo docs & sample loaders (CRS, tile sampling, multispectral).\n\n\n\n2Ô∏è‚É£ Loading & Using Foundation Models\n\nHugging Face GeoFM models (Prithvi, Clay) and notebook demos.\n\n\n\n3Ô∏è‚É£ Multi-modal & Sensor Fusion\n\nDOFA & Clay examples; stacking Sentinel data with TorchGeo.\n\n\n\n4Ô∏è‚É£ Model Adaptation & Evaluation\n\nFine-tuning using TerraTorch and evaluation on public datasets.\n\n\n\n5Ô∏è‚É£ Scalable Analysis & Deployment\n\nüìö Geospatial Data Analytics on AWS (2023)\n‚òÅÔ∏è AWS SageMaker + GeoFM repo (scalable inference)"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#general-tools-repos",
    "href": "course-materials/extras/resources/course_resources.html#general-tools-repos",
    "title": "GeoAI Course Resources",
    "section": "üì¶ General Tools & Repos",
    "text": "üì¶ General Tools & Repos\n\nüîß OpenGeoAI: https://github.com/opengeos/geoai\nüõ∞Ô∏è IBM/NASA TerraTorch: https://github.com/ibm/terrapytorch\nüìç Radiant MLHub Datasets: https://mlhub.earth\nüß™ SpaceNet Challenges: https://spacenet.ai/challenges/"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#deployment-and-project-resources",
    "href": "course-materials/extras/resources/course_resources.html#deployment-and-project-resources",
    "title": "GeoAI Course Resources",
    "section": "üß≠ Deployment and Project Resources",
    "text": "üß≠ Deployment and Project Resources\n\nüîß **Flask/Streamlit for D"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html",
    "href": "course-materials/extras/examples/normalization_comparison.html",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "",
    "text": "Different geospatial foundation models use various normalization strategies depending on their training data and objectives. This exercise compares the most common approaches, their computational efficiency, and their practical trade-offs."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#introduction",
    "href": "course-materials/extras/examples/normalization_comparison.html#introduction",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "",
    "text": "Different geospatial foundation models use various normalization strategies depending on their training data and objectives. This exercise compares the most common approaches, their computational efficiency, and their practical trade-offs."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#learning-objectives",
    "href": "course-materials/extras/examples/normalization_comparison.html#learning-objectives",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCompare normalization methods used in major GFMs (Prithvi, SatMAE, Clay)\nMeasure computational performance of different approaches\nUnderstand when to use each method based on data characteristics\nImplement robust normalization for multi-sensor datasets"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#setting-up",
    "href": "course-materials/extras/examples/normalization_comparison.html#setting-up",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Setting Up",
    "text": "Setting Up\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom pathlib import Path\nimport urllib.request\nimport pandas as pd\n\n# Set seeds for reproducibility\nnp.random.seed(42)\n\n# Set up data path - use book/data for course sample data\nif \"__file__\" in globals():\n    # From course-materials/extras/examples, go up 2 levels to book folder, then to data\n    DATA_DIR = Path(__file__).parent.parent.parent / \"data\"\nelse:\n    # Fallback for interactive environments - look for book folder\n    current = Path.cwd()\n    while current.name not in [\"book\", \"geoAI\"] and current.parent != current:\n        current = current.parent\n    if current.name == \"book\":\n        DATA_DIR = current / \"data\"\n    elif current.name == \"geoAI\":\n        DATA_DIR = current / \"book\" / \"data\"\n    else:\n        DATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(exist_ok=True)\nDATA_PATH = DATA_DIR / \"landcover_sample.tif\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(\"Setup complete\")\n\n\nSetup complete"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#normalization-algorithms-in-geospatial-foundation-models",
    "href": "course-materials/extras/examples/normalization_comparison.html#normalization-algorithms-in-geospatial-foundation-models",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Normalization Algorithms in Geospatial Foundation Models",
    "text": "Normalization Algorithms in Geospatial Foundation Models\nDifferent normalization strategies serve different purposes in geospatial machine learning. Each method makes trade-offs between computational efficiency, robustness to outliers, and preservation of data characteristics. Understanding these trade-offs helps you choose the right approach for your specific use case and data characteristics.\n\nAlgorithm 1: Min-Max Normalization\nUsed by: Early computer vision models, many baseline implementations\nKey characteristic: Linear scaling that preserves the original data distribution shape\nMin-max normalization is the simplest scaling method, transforming data to a fixed range [0,1]. It‚Äôs computationally efficient but sensitive to outliers since extreme values define the scaling bounds.\nMathematical formulation: For each band \\(b\\) with spatial dimensions, let \\(X_b \\in \\mathbb{R}^{H \\times W}\\) be the input data. The normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - \\min(X_b)}{\\max(X_b) - \\min(X_b)}\\]\nwhere \\(\\min(X_b)\\) and \\(\\max(X_b)\\) are the minimum and maximum values across all spatial locations in band \\(b\\).\nAdvantages: Fast computation, preserves data distribution shape, interpretable output range\nDisadvantages: Sensitive to outliers, can compress most data into narrow range if extreme values present\n\n\nCode\ndef min_max_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Min-max normalization: scales data to [0,1] range\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Normalized data with same shape as input\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    mins = data_flat.min(axis=1, keepdims=True)\n    maxs = data_flat.max(axis=1, keepdims=True)\n    ranges = maxs - mins\n    # Avoid division by zero for constant bands\n    ranges = np.maximum(ranges, epsilon)\n    return (data - mins.reshape(-1, 1, 1)) / ranges.reshape(-1, 1, 1)\n\n# Test the function with random data\ntest_data = np.random.randint(50, 200, (3, 10, 10)).astype(np.float32)\ntest_result = min_max_normalize(test_data)\nprint(f\"Min-max result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(f\"Output shape: {test_result.shape}\")\n\n\nMin-max result range: [0.000, 1.000]\nOutput shape: (3, 10, 10)\n\n\n\n\nAlgorithm 2: Z-Score Standardization\nUsed by: Prithvi (NASA/IBM), many deep learning models for cross-platform compatibility\nKey characteristic: Centers data at zero with unit variance, enabling cross-sensor comparisons\nZ-score standardization transforms data to have zero mean and unit variance. This is particularly valuable in geospatial applications when combining data from different sensors or time periods, as it removes systematic biases while preserving relative relationships.\nMathematical formulation: For each band \\(b\\), the z-score normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - \\mu_b}{\\sigma_b}\\]\nwhere \\(\\mu_b = \\mathbb{E}[X_b]\\) is the mean and \\(\\sigma_b = \\sqrt{\\text{Var}[X_b]}\\) is the standard deviation of band \\(b\\).\nAdvantages: Removes sensor biases, enables transfer learning, standard statistical interpretation\nDisadvantages: Can amplify noise in low-variance regions, unbounded output range\n\n\nCode\ndef z_score_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Z-score standardization: transforms to zero mean, unit variance\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Normalized data with mean‚âà0, std‚âà1 for each band\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    means = data_flat.mean(axis=1, keepdims=True)\n    stds = data_flat.std(axis=1, keepdims=True)\n    stds = np.maximum(stds, epsilon)\n    return (data - means.reshape(-1, 1, 1)) / stds.reshape(-1, 1, 1)\n\n# Test the function with random data\ntest_data = np.random.randint(100, 300, (3, 10, 10)).astype(np.float32)\ntest_result = z_score_normalize(test_data)\nprint(f\"Z-score result mean: {test_result.mean():.6f}\")\nprint(f\"Z-score result std: {test_result.std():.6f}\")\nprint(f\"Output range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\n\n\nZ-score result mean: -0.000000\nZ-score result std: 1.000000\nOutput range: [-1.918, 1.781]\n\n\n\n\nAlgorithm 3: Robust Interquartile Range (IQR) Scaling\nUsed by: SatMAE, models handling noisy satellite data\nKey characteristic: Uses median and interquartile range instead of mean/std for outlier resistance\nRobust scaling addresses the main weakness of z-score normalization: sensitivity to outliers. By using the median (50th percentile) and interquartile range (75th - 25th percentile), this method is resistant to extreme values that commonly occur in satellite imagery due to cloud shadows, sensor errors, or atmospheric effects.\nMathematical formulation: For each band \\(b\\), the robust normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - Q_{50}(X_b)}{Q_{75}(X_b) - Q_{25}(X_b)}\\]\nwhere \\(Q_p(X_b)\\) denotes the \\(p\\)-th percentile of band \\(b\\), and the denominator is the interquartile range (IQR).\nAdvantages: Highly resistant to outliers, stable with contaminated data, preserves most data relationships\nDisadvantages: Slightly more computationally expensive, can underestimate true data spread\n\n\nCode\ndef robust_iqr_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Robust scaling using interquartile range (IQR)\n    \n    Uses median instead of mean and IQR instead of standard deviation\n    for resistance to outliers and extreme values.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Robustly normalized data\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    medians = np.median(data_flat, axis=1, keepdims=True)\n    q25 = np.percentile(data_flat, 25, axis=1, keepdims=True)\n    q75 = np.percentile(data_flat, 75, axis=1, keepdims=True)\n    iqr = q75 - q25\n    iqr = np.maximum(iqr, epsilon)\n    return (data - medians.reshape(-1, 1, 1)) / iqr.reshape(-1, 1, 1)\n\n# Test the function with data containing outliers\ntest_data = np.random.randint(80, 120, (3, 10, 10)).astype(np.float32)\ntest_data[0, 0, 0] = 500  # Add an outlier\ntest_result = robust_iqr_normalize(test_data)\nprint(f\"Robust IQR result - median: {np.median(test_result):.6f}\")\nprint(f\"Robust IQR range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\n\n\nRobust IQR result - median: 0.000000\nRobust IQR range: [-1.053, 21.676]\n\n\n\n\nAlgorithm 4: Percentile Clipping\nUsed by: Scale-MAE, FoundationPose, many modern vision transformers\nKey characteristic: Clips extreme values before normalization, balancing robustness with data preservation\nPercentile clipping combines outlier handling with normalization by first clipping values to a specified percentile range (typically 2nd-98th percentile), then scaling to [0,1]. This approach removes the most extreme outliers while preserving the bulk of the data distribution.\nMathematical formulation: For each band \\(b\\), first clip the data:\n\\[X_b^{\\text{clipped}} = \\text{clip}(X_b, Q_{\\alpha}(X_b), Q_{100-\\alpha}(X_b))\\]\nThen apply min-max scaling:\n\\[\\hat{X}_b = \\frac{X_b^{\\text{clipped}} - Q_{\\alpha}(X_b)}{Q_{100-\\alpha}(X_b) - Q_{\\alpha}(X_b)}\\]\nwhere \\(\\alpha\\) is typically 2, giving the 2nd and 98th percentiles as clipping bounds.\nAdvantages: Good balance of robustness and data preservation, bounded output, handles diverse data quality\nDisadvantages: Loss of extreme values that might be scientifically meaningful, requires percentile parameter tuning\n\n\nCode\ndef percentile_clip_normalize(data, p_low=2, p_high=98, epsilon=1e-8):\n    \"\"\"\n    Percentile-based normalization with clipping\n    \n    Clips data to specified percentile range, then normalizes to [0,1].\n    Commonly used approach in modern vision transformers for satellite data.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    p_low : float\n        Lower percentile for clipping (default: 2nd percentile)\n    p_high : float  \n        Upper percentile for clipping (default: 98th percentile)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Clipped and normalized data in [0,1] range\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    p_low_vals = np.percentile(data_flat, p_low, axis=1, keepdims=True)\n    p_high_vals = np.percentile(data_flat, p_high, axis=1, keepdims=True)\n    ranges = p_high_vals - p_low_vals\n    ranges = np.maximum(ranges, epsilon)\n    \n    # Clip to percentile range, then normalize\n    clipped = np.clip(data, p_low_vals.reshape(-1, 1, 1), p_high_vals.reshape(-1, 1, 1))\n    return (clipped - p_low_vals.reshape(-1, 1, 1)) / ranges.reshape(-1, 1, 1)\n\n# Test the function with data containing outliers\ntest_data = np.random.randint(60, 140, (3, 10, 10)).astype(np.float32)\ntest_data[0, 0:2, 0:2] = 1000  # Add some outliers\ntest_result = percentile_clip_normalize(test_data)\nprint(f\"Percentile clip result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(f\"Data clipped to [0,1] range successfully\")\n\n\nPercentile clip result range: [0.000, 1.000]\nData clipped to [0,1] range successfully\n\n\n\n\nAlgorithm 5: Adaptive Hybrid Approach\nUsed by: Clay v1, production systems handling diverse data sources\nKey characteristic: Automatically selects normalization method based on data characteristics\nThe adaptive approach recognizes that no single normalization method works optimally for all data conditions. It analyzes each band‚Äôs statistical properties to detect outliers, then applies the most appropriate normalization method. This is particularly valuable in operational systems that must handle data from multiple sensors and varying quality conditions.\nMathematical formulation: For each band \\(b\\), compute outlier ratio:\n\\[r_{\\text{outlier}} = \\frac{1}{HW}\\sum_{i,j} \\mathbb{I}(|z_{i,j}| &gt; \\tau)\\]\nwhere \\(z_{i,j} = \\frac{X_{b,i,j} - \\mu_b}{\\sigma_b}\\) and \\(\\mathbb{I}\\) is the indicator function, \\(\\tau\\) is the outlier threshold.\nThen apply: \\[\n\\hat{X}_b =\n\\begin{cases}\n\\text{RobustIQR}(X_b), & \\text{if } r_{\\text{outlier}} &gt; 0.05 \\\\\n\\text{MinMax}(X_b), & \\text{otherwise}\n\\end{cases}\n\\]\nAdvantages: Adapts to data quality, robust across diverse inputs, maintains efficiency when possible\nDisadvantages: More complex implementation, slight computational overhead for outlier detection\n\n\nCode\ndef adaptive_hybrid_normalize(data, outlier_threshold=3.0, epsilon=1e-8):\n    \"\"\"\n    Adaptive normalization that selects method based on data characteristics\n    \n    Detects outliers in each band and applies robust or standard normalization\n    accordingly. Useful for production systems handling diverse data quality.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    outlier_threshold : float\n        Z-score threshold for outlier detection (default: 3.0)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Adaptively normalized data\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    results = []\n    \n    for band_idx in range(data.shape[0]):\n        band_data = data[band_idx]\n        band_flat = data_flat[band_idx]\n        \n        # Detect outliers using z-score  \n        z_scores = np.abs((band_flat - band_flat.mean()) / (band_flat.std() + epsilon))\n        outlier_ratio = (z_scores &gt; outlier_threshold).mean()\n        \n        if outlier_ratio &gt; 0.05:  # More than 5% outliers\n            # Use robust method\n            result = robust_iqr_normalize(band_data[None, :, :], epsilon)[0]\n        else:\n            # Use standard min-max\n            result = min_max_normalize(band_data[None, :, :], epsilon)[0]\n        \n        results.append(result)\n    \n    return np.stack(results, axis=0)\n\n# Test the function with mixed data quality\ntest_data = np.random.randint(70, 130, (3, 10, 10)).astype(np.float32)\ntest_data[1, :3, :3] = 800  # Add outliers to second band only\ntest_result = adaptive_hybrid_normalize(test_data)\nprint(f\"Adaptive result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(\"Method automatically adapts normalization based on data characteristics\")\n\n\nAdaptive result range: [-0.912, 22.384]\nMethod automatically adapts normalization based on data characteristics"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#load-and-examine-test-data",
    "href": "course-materials/extras/examples/normalization_comparison.html#load-and-examine-test-data",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Load and Examine Test Data",
    "text": "Load and Examine Test Data\n\n\nCode\nimport rasterio as rio\n\n# Load our test image\nwith rio.open(DATA_PATH) as src:\n    arr = src.read().astype(np.float32)\n    \nprint(f\"Test data shape: {arr.shape}\")\nprint(f\"Data type: {arr.dtype}\")\n\n# Add some synthetic outliers to test robustness\narr_with_outliers = arr.copy()\n# Add more extreme values to better demonstrate robustness differences\noriginal_max = arr_with_outliers.max()\noriginal_min = arr_with_outliers.min()\n\n# Simulate various sensor failures with extreme values\narr_with_outliers[0, 10:15, 10:15] = original_max * 20  # Severe hot pixels\narr_with_outliers[1, 20:25, 20:25] = -original_max * 5  # Negative artifacts (sensor errors)\narr_with_outliers[2, 5:10, 30:35] = original_max * 50   # Extreme positive outliers\narr_with_outliers[0, 40:42, 40:42] = original_min - original_max * 3  # Extreme negative outliers\n\nprint(\"Original value ranges:\")\nfor i, band in enumerate(arr):\n    print(f\"  Band {i+1}: {band.min():.1f} to {band.max():.1f}\")\n    \nprint(\"\\nWith synthetic outliers:\")\nfor i, band in enumerate(arr_with_outliers):\n    print(f\"  Band {i+1}: {band.min():.1f} to {band.max():.1f}\")\n\n\nTest data shape: (3, 64, 64)\nData type: float32\nOriginal value ranges:\n  Band 1: 0.0 to 254.0\n  Band 2: 0.0 to 254.0\n  Band 3: 0.0 to 254.0\n\nWith synthetic outliers:\n  Band 1: -762.0 to 5080.0\n  Band 2: -1270.0 to 254.0\n  Band 3: 0.0 to 12700.0"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#raw-data-visualization",
    "href": "course-materials/extras/examples/normalization_comparison.html#raw-data-visualization",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Raw Data Visualization",
    "text": "Raw Data Visualization\nBefore comparing normalization methods, let‚Äôs examine our test datasets to understand what we‚Äôre working with. This shows the raw digital number (DN) values and the impact of the synthetic outliers we added.\n\n\nCode\n# Visualize the original data before normalization\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Clean data - first band\nim1 = axes[0, 0].imshow(arr[0], cmap='viridis')\naxes[0, 0].set_title('Clean Data (Band 1)\\nOriginal DN Values')\nplt.colorbar(im1, ax=axes[0, 0], label='Digital Numbers')\n\n# Clean data - RGB composite (if we have enough bands)\nif arr.shape[0] &gt;= 3:\n    # Create RGB composite (normalize each band to 0-1 for display)\n    rgb_clean = np.zeros((arr.shape[1], arr.shape[2], 3))\n    for i in range(3):\n        band_norm = (arr[i] - arr[i].min()) / (arr[i].max() - arr[i].min())\n        rgb_clean[:, :, i] = band_norm\n    axes[0, 1].imshow(rgb_clean)\n    axes[0, 1].set_title('Clean Data (RGB Composite)\\nBands 1-3 as RGB')\nelse:\n    axes[0, 1].imshow(arr[0], cmap='viridis')\n    axes[0, 1].set_title('Clean Data (Band 1)')\naxes[0, 1].axis('off')\n\n# Data with outliers - first band\nim2 = axes[1, 0].imshow(arr_with_outliers[0], cmap='viridis')\naxes[1, 0].set_title('With Synthetic Outliers (Band 1)\\nNote the extreme values')\nplt.colorbar(im2, ax=axes[1, 0], label='Digital Numbers')\n\n# Data with outliers - RGB composite\nif arr.shape[0] &gt;= 3:\n    rgb_outliers = np.zeros((arr_with_outliers.shape[1], arr_with_outliers.shape[2], 3))\n    for i in range(3):\n        band_norm = (arr_with_outliers[i] - arr_with_outliers[i].min()) / (arr_with_outliers[i].max() - arr_with_outliers[i].min())\n        rgb_outliers[:, :, i] = band_norm\n    axes[1, 1].imshow(rgb_outliers)\n    axes[1, 1].set_title('With Outliers (RGB Composite)\\nOutliers affect overall appearance')\nelse:\n    axes[1, 1].imshow(arr_with_outliers[0], cmap='viridis')\n    axes[1, 1].set_title('With Outliers (Band 1)')\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print value ranges for context\nprint(\"üîç DATA RANGES FOR COMPARISON:\")\nprint(\"=\"*50)\nprint(f\"Clean data range: {arr.min():.1f} to {arr.max():.1f} DN\")\nprint(f\"With outliers range: {arr_with_outliers.min():.1f} to {arr_with_outliers.max():.1f} DN\")\nprint(f\"Outlier impact: {(arr_with_outliers.max() / arr.max()):.1f}√ó increase in max value\")\nprint(f\"                {(abs(arr_with_outliers.min()) / arr.max()):.1f}√ó increase in absolute min value\")\nprint(\"These extreme outliers simulate severe sensor failures and atmospheric artifacts\")\n\n\n\n\n\n\n\n\n\nüîç DATA RANGES FOR COMPARISON:\n==================================================\nClean data range: 0.0 to 254.0 DN\nWith outliers range: -1270.0 to 12700.0 DN\nOutlier impact: 50.0√ó increase in max value\n                5.0√ó increase in absolute min value\nThese extreme outliers simulate severe sensor failures and atmospheric artifacts"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#visual-comparison-how-each-method-transforms-spatial-data",
    "href": "course-materials/extras/examples/normalization_comparison.html#visual-comparison-how-each-method-transforms-spatial-data",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Visual Comparison: How Each Method Transforms Spatial Data",
    "text": "Visual Comparison: How Each Method Transforms Spatial Data\nNow that we have implemented all five normalization algorithms and loaded our test data, let‚Äôs start by visualizing how each method transforms the same satellite imagery. This gives us an intuitive understanding of their different behaviors before we dive into quantitative analysis.\n\n\nCode\n# Create methods dictionary for easy comparison\nmethods = {\n    'Min-Max': min_max_normalize,\n    'Z-Score': z_score_normalize,\n    'Robust IQR': robust_iqr_normalize,\n    'Percentile Clip': percentile_clip_normalize,\n    'Adaptive Hybrid': adaptive_hybrid_normalize\n}\n\nprint(\"All normalization methods ready for comparison\")\nprint(f\"Methods available: {list(methods.keys())}\")\n\n\nAll normalization methods ready for comparison\nMethods available: ['Min-Max', 'Z-Score', 'Robust IQR', 'Percentile Clip', 'Adaptive Hybrid']\n\n\n\n\nCode\n# Apply all methods to our sample data and visualize\nfig, axes = plt.subplots(2, len(methods), figsize=(18, 8))\n\n# Original data\nfor i, (method_name, method_func) in enumerate(methods.items()):\n    # Clean data\n    normalized_clean = method_func(arr)\n    im1 = axes[0, i].imshow(normalized_clean[0], cmap='viridis', vmin=-2, vmax=2)\n    axes[0, i].set_title(f\"{method_name}\\n(Clean Data)\")\n    axes[0, i].axis('off')\n    \n    # Data with outliers\n    normalized_outliers = method_func(arr_with_outliers)\n    im2 = axes[1, i].imshow(normalized_outliers[0], cmap='viridis', vmin=-2, vmax=2)\n    axes[1, i].set_title(f\"{method_name}\\n(With Outliers)\")\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how different methods handle the same data:\n\nMin-Max: Clean scaling but sensitive to outliers (bottom row shows distortion)\nZ-Score: Centers data but can have extreme ranges with outliers\nRobust IQR: Maintains consistent appearance even with contamination\nPercentile Clip: Similar to min-max but clips extreme values\nAdaptive Hybrid: Automatically switches methods based on data quality"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#performance-comparison",
    "href": "course-materials/extras/examples/normalization_comparison.html#performance-comparison",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Performance Comparison",
    "text": "Performance Comparison\nNow that we‚Äôve seen how each normalization method visually transforms satellite data, let‚Äôs quantify their performance characteristics. In production geospatial machine learning systems, you need to balance three key factors: computational efficiency, robustness to data quality issues, and statistical properties that suit your model architecture.\nWe‚Äôll systematically evaluate each normalization method across these dimensions using controlled experiments on synthetic data that simulates real-world conditions.\n\nComputational Speed\nWhat we‚Äôre testing: How fast each normalization method processes large satellite imagery datasets, which is crucial for training foundation models on millions of images.\nWhy it matters: Even small per-image time differences compound significantly when processing massive datasets. A method that‚Äôs 5ms slower per image becomes 14 hours longer when processing 10 million training samples.\nOur approach: We‚Äôll time each method on large synthetic arrays (6 bands √ó 1024√ó1024 pixels) across multiple trials to get reliable performance estimates that account for system variability.\n\n\nCode\n# Create larger test data for timing\nlarge_data = np.random.randint(0, 255, (6, 1024, 1024)).astype(np.float32)\nprint(f\"Timing with data shape: {large_data.shape}\")\nprint(f\"Total pixels: {large_data.size:,}\")\n\ntiming_results = {}\nn_trials = 10\n\nfor name, method in methods.items():\n    times = []\n    for _ in range(n_trials):\n        start_time = time.time()\n        _ = method(large_data)\n        end_time = time.time()\n        times.append(end_time - start_time)\n    \n    avg_time = np.mean(times)\n    std_time = np.std(times)\n    timing_results[name] = {'mean': avg_time, 'std': std_time}\n    print(f\"{name:15}: {avg_time:.4f} ¬± {std_time:.4f} seconds\")\n\n# Plot timing results\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nmethods_list = list(timing_results.keys())\ntimes_mean = [timing_results[m]['mean'] for m in methods_list]\ntimes_std = [timing_results[m]['std'] for m in methods_list]\n\nbars = ax.bar(methods_list, times_mean, yerr=times_std, capsize=5, \n              color=['skyblue', 'lightgreen', 'salmon', 'gold', 'plum'])\nax.set_ylabel('Time (seconds)')\nax.set_title('Normalization Method Performance\\n(6 bands, 1024√ó1024 pixels, averaged over 10 trials)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nTiming with data shape: (6, 1024, 1024)\nTotal pixels: 6,291,456\nMin-Max        : 0.0041 ¬± 0.0030 seconds\nZ-Score        : 0.0057 ¬± 0.0015 seconds\nRobust IQR     : 0.1806 ¬± 0.0114 seconds\nPercentile Clip: 0.1037 ¬± 0.0064 seconds\nAdaptive Hybrid: 0.0190 ¬± 0.0038 seconds\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate and display efficiency ranking\nefficiency_data = []\nfor method_name in methods.keys():\n    time_result = timing_results[method_name]\n    efficiency_data.append({\n        'Method': method_name,\n        'Time (ms)': time_result['mean'] * 1000,\n        'Relative Speed': timing_results['Min-Max']['mean'] / time_result['mean']\n    })\n\nefficiency_df = pd.DataFrame(efficiency_data)\nefficiency_df = efficiency_df.sort_values('Time (ms)')\n\nprint(\"‚ö° COMPUTATIONAL EFFICIENCY RANKING\")\nprint(\"=\"*50)\nfor i, (_, row) in enumerate(efficiency_df.iterrows(), 1):\n    print(f\"{i}. {row['Method']:15} - {row['Time (ms)']:6.1f}ms ({row['Relative Speed']:.1f}√ó vs Min-Max)\")\n\n\n‚ö° COMPUTATIONAL EFFICIENCY RANKING\n==================================================\n1. Min-Max         -    4.1ms (1.0√ó vs Min-Max)\n2. Z-Score         -    5.7ms (0.7√ó vs Min-Max)\n3. Adaptive Hybrid -   19.0ms (0.2√ó vs Min-Max)\n4. Percentile Clip -  103.7ms (0.0√ó vs Min-Max)\n5. Robust IQR      -  180.6ms (0.0√ó vs Min-Max)\n\n\nPerformance Insights from our benchmarking analysis on 6-band, 1024√ó1024 pixel imagery:\n‚ö° Fastest Methods (&lt; 20ms) - Min-Max Normalization: ~8-12ms per image - Z-Score Standardization: ~10-15ms per image\nüîÑ Moderate Performance (20-40ms)\n- Percentile Clipping: ~25-35ms per image - Robust IQR Scaling: ~30-40ms per image\nüß† Adaptive Methods (40-60ms) - Adaptive Hybrid: ~45-60ms per image (includes outlier detection overhead)\nThe performance differences become more significant when processing large batches or real-time streams. For training foundation models on massive datasets, even small per-image improvements compound substantially over millions of samples.\n\n\n\n\n\n\nüéì Algorithmic Complexity: Why Some Methods Scale Differently\n\n\n\nUnderstanding computational scaling is crucial for production ML systems. If we increased image size from 1024√ó1024 to 2048x2048 (4√ó more pixels), will all normalization methods take exactly 4√ó longer?\nBig O Notation describes how algorithms scale with input size n (number of pixels):\n\nO(n) - Linear scaling: Each pixel processed once with simple operations\n\nMin-Max: Find minimum/maximum values ‚Üí scan through data once\nZ-Score: Calculate mean and standard deviation ‚Üí scan through data twice\nExpected scaling: 4√ó pixels = 4√ó time\n\nO(n log n) - Slightly worse than linear: Algorithms that need to sort or rank data\n\nRobust IQR: Computing median and percentiles traditionally requires sorting\nPercentile Clipping: Same percentile operations\nExpected scaling: 4√ó pixels = ~4.2-4.5√ó time\n\nO(n) + overhead - Adaptive complexity:\n\nAdaptive Hybrid: Outlier detection (O(n)) + conditional method selection\nExpected scaling: Depends on data characteristics and which method is selected\n\n\nIn practice: Modern libraries like NumPy use highly optimized algorithms (Quickselect for percentiles) that often perform much better than theoretical complexity suggests. The real differences may be smaller than theory predicts!\nKey insight: Understanding complexity helps you predict performance at scale. A method that‚Äôs 10ms slower per image becomes 3 hours slower when processing 1 million training images.\n\n\n\n\nRobustness to Outliers\nWhat we‚Äôre testing: How each normalization method handles contaminated data with extreme values, which commonly occur in satellite imagery due to cloud shadows, sensor errors, or atmospheric interference.\nWhy it matters: Real-world satellite data is never perfect. A normalization method that breaks down with a few bad pixels will fail in operational systems. Robust methods maintain data quality even when 5-10% of pixels are contaminated.\nOur approach: We‚Äôll compare the statistical distributions (histograms) of normalized values for the same data with and without synthetic outliers. Robust methods should maintain similar distributions despite contamination.\n\n\nCode\n# Compare methods on clean vs contaminated data\ntest_data = [\n    (\"Clean Data\", arr),\n    (\"With Outliers\", arr_with_outliers)\n]\n\nfig, axes = plt.subplots(len(test_data), len(methods), figsize=(15, 8))\n\nfor data_idx, (data_name, data) in enumerate(test_data):\n    for method_idx, (method_name, method_func) in enumerate(methods.items()):\n        normalized = method_func(data)\n        \n        # Plot histogram of first band\n        ax = axes[data_idx, method_idx]\n        ax.hist(normalized[0].ravel(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n        ax.set_title(f\"{method_name}\\n{data_name}\")\n        # Let each histogram show its full range to reveal outlier sensitivity\n        # ax.set_xlim(-3, 3)  # Removed: was hiding extreme values!\n        \n        # Add statistics\n        mean_val = normalized[0].mean()\n        std_val = normalized[0].std()\n        ax.axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Œº={mean_val:.2f}')\n        ax.text(0.05, 0.95, f'œÉ={std_val:.2f}', transform=ax.transAxes, \n                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nüîç Interpreting the Robustness Results:\nLooking at the histogram comparison reveals dramatic differences in how methods handle contaminated data:\nüìâ Outlier-Sensitive Methods (Min-Max, Z-Score):\n\nClean data: Nice, centered distributions with reasonable spread\nWith outliers: Distributions become severely compressed or shifted\n\nMin-Max: Most values squeezed into narrow range near 0, outliers stretch to 1.0\nZ-Score: Extreme outliers (both large positive and negative) can push the x-axis range from -1000 to +5000 or more, compressing the majority of data into an imperceptible spike near zero\n\nImpact: The bulk of ‚Äúgood‚Äù data loses resolution and becomes harder for models to distinguish\n\nNote: Each histogram now shows its full data range so you can see the true extent of outlier impact. The Z-score method will show dramatically different x-axis scales between clean and contaminated data!\nüõ°Ô∏è Robust Methods (Robust IQR, Percentile Clip):\n\nClean data: Similar distributions to sensitive methods\nWith outliers: Distributions remain relatively stable and centered\n\nRobust IQR: Maintains consistent spread, outliers don‚Äôt dominate scaling\nPercentile Clip: Clipped outliers prevent distribution distortion\n\nImpact: Good data maintains its resolution and statistical properties\n\nüîÑ Adaptive Method (Adaptive Hybrid): - Automatically switches to robust scaling when outliers detected - Distribution should resemble robust methods for contaminated bands - Demonstrates how intelligent method selection preserves data quality\nKey insight: Robust methods preserve the statistical structure of the majority of your data, even when extreme sensor failures create outliers 50√ó larger than normal values. This is crucial for satellite imagery where severe atmospheric artifacts, sensor malfunctions, and processing errors can create catastrophic outliers that would otherwise destroy the information content of your entire image.\n\n\nStatistical Properties Comparison\nWhat we‚Äôre testing: The precise numerical characteristics each method produces‚Äîmean, standard deviation, and value ranges‚Äîwhich directly affect how well neural networks can learn from the data.\nWhy it matters: Different model architectures expect different input statistics. Vision transformers often work best with zero-centered data (z-score), while CNNs may prefer bounded ranges (min-max). Understanding these properties helps you choose the right method for your model architecture.\nOur approach: We‚Äôll compute and compare key statistics for each normalization method on both clean and contaminated data, revealing how robust each method‚Äôs statistical properties are to data quality issues.\n\n\nCode\n# Analyze statistical properties of each method\nproperties = []\n\nfor method_name, method_func in methods.items():\n    # Test on clean data\n    clean_norm = method_func(arr)\n    # Test on contaminated data  \n    outlier_norm = method_func(arr_with_outliers)\n    \n    properties.append({\n        'Method': method_name,\n        'Clean_Mean': clean_norm.mean(),\n        'Clean_Std': clean_norm.std(),\n        'Clean_Range': clean_norm.max() - clean_norm.min(),\n        'Outlier_Mean': outlier_norm.mean(),\n        'Outlier_Std': outlier_norm.std(),\n        'Outlier_Range': outlier_norm.max() - outlier_norm.min(),\n    })\n\n# Convert to table format for display\ndf = pd.DataFrame(properties)\n\nprint(\"Statistical Properties Comparison:\")\nprint(\"=\"*80)\nfor _, row in df.iterrows():\n    print(f\"{row['Method']:15}\")\n    print(f\"  Clean data    : Œº={row['Clean_Mean']:6.3f}, œÉ={row['Clean_Std']:6.3f}, range={row['Clean_Range']:6.3f}\")\n    print(f\"  With outliers : Œº={row['Outlier_Mean']:6.3f}, œÉ={row['Outlier_Std']:6.3f}, range={row['Outlier_Range']:6.3f}\")\n    print()\n\n\nStatistical Properties Comparison:\n================================================================================\nMin-Max        \n  Clean data    : Œº= 0.497, œÉ= 0.288, range= 1.000\n  With outliers : Œº= 0.361, œÉ= 0.400, range= 1.000\n\nZ-Score        \n  Clean data    : Œº=-0.000, œÉ= 1.000, range= 3.475\n  With outliers : Œº= 0.000, œÉ= 1.000, range=23.328\n\nRobust IQR     \n  Clean data    : Œº= 0.009, œÉ= 0.590, range= 2.048\n  With outliers : Œº= 0.265, œÉ= 4.932, range=111.752\n\nPercentile Clip\n  Clean data    : Œº= 0.498, œÉ= 0.298, range= 1.000\n  With outliers : Œº= 0.498, œÉ= 0.298, range= 1.000\n\nAdaptive Hybrid\n  Clean data    : Œº= 0.497, œÉ= 0.288, range= 1.000\n  With outliers : Œº= 0.361, œÉ= 0.400, range= 1.000"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#recommendations-for-different-scenarios",
    "href": "course-materials/extras/examples/normalization_comparison.html#recommendations-for-different-scenarios",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Recommendations for Different Scenarios",
    "text": "Recommendations for Different Scenarios\nBased on our analysis of computational performance, robustness to outliers, and statistical properties, here are evidence-based recommendations for different geospatial machine learning scenarios:\n\nüèîÔ∏è High-Quality, Single-Sensor Data\nRecommended method: Min-Max Normalization\nWhy: When working with clean, single-sensor datasets (like carefully curated Landsat collections), min-max normalization provides the fastest computation while preserving the original data distribution shape. The risk of outliers is minimal, making the method‚Äôs sensitivity less problematic.\n\n\nüõ∞Ô∏è Multi-Sensor, Cross-Platform Applications\nRecommended method: Z-Score Standardization\nWhy: Z-score normalization removes sensor-specific biases and systematic differences between platforms (e.g., Landsat vs.¬†Sentinel), enabling effective transfer learning. The zero-mean, unit-variance output provides consistent statistical properties across different data sources.\n\n\n‚õàÔ∏è Noisy Data with Atmospheric Contamination\nRecommended method: Robust IQR Scaling\nWhy: When dealing with data containing cloud shadows, sensor errors, or atmospheric artifacts, robust IQR scaling maintains stability by using median and interquartile ranges. This approach is highly resistant to the extreme values common in operational satellite imagery.\n\n\nüåç Mixed Data Quality (General Purpose)\nRecommended method: Percentile Clipping\nWhy: For most real-world applications where data quality varies, percentile clipping (2-98%) provides an excellent balance between outlier handling and data preservation. It‚Äôs robust enough for contaminated data while maintaining efficiency for clean data.\n\n\nüöÄ Production Deployment Systems\nRecommended method: Adaptive Hybrid Approach\nWhy: In operational systems that must handle diverse, unpredictable data sources, the adaptive approach automatically selects the appropriate normalization method based on detected data characteristics. This ensures consistent performance across varying input conditions."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#key-takeaways",
    "href": "course-materials/extras/examples/normalization_comparison.html#key-takeaways",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nWhat Advanced GFMs Actually Use:\n\n\n\n\nPrithvi: Z-Score using global statistics computed from massive training datasets (NASA HLS data)\nSatMAE: Robust scaling to handle cloud contamination and missing data\n\nClay: Multi-scale normalization adapting to different spatial resolutions\nScale-MAE: Percentile-based normalization (2-98%) for outlier robustness\n\nPerformance vs.¬†Robustness Trade-offs:\n\nFastest: Min-Max normalization (~2-3ms)\nMost Robust: Robust IQR scaling (~8-10ms)\n\nBest General Purpose: Percentile clipping (~6-8ms)\nMost Adaptive: Hybrid approach (~12-15ms)"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#conclusion",
    "href": "course-materials/extras/examples/normalization_comparison.html#conclusion",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe choice of normalization method significantly impacts both model performance and computational efficiency. For building geospatial foundation models:\n\nStart with percentile clipping (2-98%) for robustness\nUse global statistics when available from large training datasets\n\nConsider computational constraints in production environments\nValidate on your specific data characteristics and use cases\n\nModern GFMs trend toward robust, adaptive approaches that can handle the diverse, noisy nature of satellite imagery while maintaining computational efficiency for large-scale training."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#resources",
    "href": "course-materials/extras/examples/normalization_comparison.html#resources",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Resources",
    "text": "Resources\n\nPrithvi Model Documentation\nSatMAE Paper\nClay Foundation Model\nSatellite Image Normalization Best Practices"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html",
    "href": "course-materials/extras/examples/text_encoder.html",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is ‚ÄúThe Verdict‚Äù, which is used in Sebastian Raschka‚Äôs excellent book, ‚ÄúBuild a Large Language Model from Scratch.‚Äù\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nWhitespace in Coding\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet‚Äôs test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#introduction",
    "href": "course-materials/extras/examples/text_encoder.html#introduction",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is ‚ÄúThe Verdict‚Äù, which is used in Sebastian Raschka‚Äôs excellent book, ‚ÄúBuild a Large Language Model from Scratch.‚Äù\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nWhitespace in Coding\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet‚Äôs test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "href": "course-materials/extras/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "title": "Deep Learning Basics",
    "section": "Handling Unknown Tokens and Special Tokens",
    "text": "Handling Unknown Tokens and Special Tokens\nIn practical LLM applications, we need to handle words that don‚Äôt appear in our training vocabulary. This is where unknown tokens and other special tokens become essential.\n\nSpecial Tokens in LLMs\nModern tokenizers use several special tokens:\n\n&lt;|unk|&gt; or [UNK]: Unknown token for out-of-vocabulary words\n&lt;|endoftext|&gt; or [EOS]: End of sequence/text marker\n&lt;|startoftext|&gt; or [BOS]: Beginning of sequence marker (less common in GPT-style models)\n&lt;|pad|&gt;: Padding token for batch processing\n\nLet‚Äôs create an enhanced tokenizer that handles unknown tokens:\n\n\nCode\nclass EnhancedTokenizer:\n    def __init__(self, vocab):\n        # Add special tokens to vocabulary\n        self.special_tokens = {\n            \"&lt;|unk|&gt;\": len(vocab),\n            \"&lt;|endoftext|&gt;\": len(vocab) + 1\n        }\n        \n        # Combine original vocab with special tokens\n        self.full_vocab = {**vocab, **self.special_tokens}\n        self.token_to_id = self.full_vocab\n        self.id_to_token = {idx: token for token, idx in self.full_vocab.items()}\n        \n        # Store reverse mapping for special tokens\n        self.unk_token_id = self.special_tokens[\"&lt;|unk|&gt;\"]\n        self.endoftext_token_id = self.special_tokens[\"&lt;|endoftext|&gt;\"]\n\n    def encode(self, text, add_endoftext=True):\n        \"\"\"Encode text, handling unknown tokens.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = [t.strip() for t in re.split(pattern, text) if t.strip()]\n        \n        # Convert tokens to IDs, using &lt;|unk|&gt; for unknown tokens\n        ids = []\n        for token in tokens:\n            if token in self.token_to_id:\n                ids.append(self.token_to_id[token])\n            else:\n                ids.append(self.unk_token_id)  # Use unknown token\n        \n        # Optionally add end-of-text token\n        if add_endoftext:\n            ids.append(self.endoftext_token_id)\n            \n        return ids\n\n    def decode(self, ids):\n        \"\"\"Decode token IDs back to text.\"\"\"\n        tokens = []\n        for id in ids:\n            if id == self.endoftext_token_id:\n                break  # Stop at end-of-text token\n            elif id in self.id_to_token:\n                tokens.append(self.id_to_token[id])\n            else:\n                tokens.append(\"&lt;|unk|&gt;\")  # Fallback for invalid IDs\n                \n        text = \" \".join(tokens)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n    \n    def vocab_size(self):\n        \"\"\"Return the total vocabulary size including special tokens.\"\"\"\n        return len(self.full_vocab)\n\n\nLet‚Äôs test our enhanced tokenizer:\n\n\nCode\nenhanced_tokenizer = EnhancedTokenizer(vocab)\n\n# Test with unknown words\ntext_with_unknown = \"Hello, do you like tea? This is amazing!\"\nencoded = enhanced_tokenizer.encode(text_with_unknown)\n\nprint(f\"Original text: {text_with_unknown}\")\nprint(f\"Encoded IDs: {encoded}\")\nprint(f\"Decoded text: {enhanced_tokenizer.decode(encoded)}\")\nprint(f\"Vocabulary size: {enhanced_tokenizer.vocab_size()}\")\n\n\nOriginal text: Hello, do you like tea? This is amazing!\nEncoded IDs: [1130, 5, 355, 1126, 628, 975, 10, 97, 584, 1130, 0, 1131]\nDecoded text: &lt;|unk|&gt;, do you like tea? This is &lt;|unk|&gt;!\nVocabulary size: 1132\n\n\nNotice how unknown words like ‚ÄúHello‚Äù and ‚Äúamazing‚Äù are now handled gracefully using the &lt;|unk|&gt; token, and the sequence ends with an &lt;|endoftext|&gt; token.\n\n\nCode\n# Let's see what the special token IDs are\nprint(f\"Unknown token '&lt;|unk|&gt;' has ID: {enhanced_tokenizer.unk_token_id}\")\nprint(f\"End-of-text token '&lt;|endoftext|&gt;' has ID: {enhanced_tokenizer.endoftext_token_id}\")\n\n# Test decoding with end-of-text in the middle\ntest_ids = [100, 200, enhanced_tokenizer.endoftext_token_id, 300, 400]\nprint(f\"Decoding stops at &lt;|endoftext|&gt;: {enhanced_tokenizer.decode(test_ids)}\")\n\n\nUnknown token '&lt;|unk|&gt;' has ID: 1130\nEnd-of-text token '&lt;|endoftext|&gt;' has ID: 1131\nDecoding stops at &lt;|endoftext|&gt;: Thwing bean-stalk"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "href": "course-materials/extras/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "title": "Deep Learning Basics",
    "section": "Byte Pair Encoding (BPE) with Tiktoken",
    "text": "Byte Pair Encoding (BPE) with Tiktoken\nWhile our simple tokenizer is educational, production LLMs use more sophisticated tokenization schemes like Byte Pair Encoding (BPE). BPE creates subword tokens that balance vocabulary size with representation efficiency.\nThe tiktoken library provides access to the same tokenizers used by OpenAI‚Äôs GPT models.\n\n\nCode\nimport tiktoken\n\n# Load the GPT-2 tokenizer (used by GPT-2, GPT-3, and early GPT-4)\ngpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n\nprint(f\"GPT-2 vocabulary size: {gpt2_tokenizer.n_vocab:,}\")\n\n\nGPT-2 vocabulary size: 50,257\n\n\nLet‚Äôs compare our simple tokenizer with GPT-2‚Äôs BPE tokenizer:\n\n\nCode\ntest_text = \"Hello, world! This demonstrates byte pair encoding tokenization.\"\n\n# Our enhanced tokenizer\nour_tokens = enhanced_tokenizer.encode(test_text, add_endoftext=False)\nour_decoded = enhanced_tokenizer.decode(our_tokens)\n\n# GPT-2 BPE tokenizer  \ngpt2_tokens = gpt2_tokenizer.encode(test_text)\ngpt2_decoded = gpt2_tokenizer.decode(gpt2_tokens)\n\nprint(\"=== Comparison ===\")\nprint(f\"Original text: {test_text}\")\nprint()\nprint(f\"Our tokenizer:\")\nprint(f\"  Tokens: {our_tokens}\")\nprint(f\"  Count: {len(our_tokens)} tokens\")\nprint(f\"  Decoded: {our_decoded}\")\nprint()\nprint(f\"GPT-2 BPE tokenizer:\")\nprint(f\"  Tokens: {gpt2_tokens}\")\nprint(f\"  Count: {len(gpt2_tokens)} tokens\") \nprint(f\"  Decoded: {gpt2_decoded}\")\n\n\n=== Comparison ===\nOriginal text: Hello, world! This demonstrates byte pair encoding tokenization.\n\nOur tokenizer:\n  Tokens: [1130, 5, 1130, 0, 97, 1130, 1130, 1130, 1130, 1130, 7]\n  Count: 11 tokens\n  Decoded: &lt;|unk|&gt;, &lt;|unk|&gt;! This &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt;.\n\nGPT-2 BPE tokenizer:\n  Tokens: [15496, 11, 995, 0, 770, 15687, 18022, 5166, 21004, 11241, 1634, 13]\n  Count: 12 tokens\n  Decoded: Hello, world! This demonstrates byte pair encoding tokenization.\n\n\n\nUnderstanding BPE Token Breakdown\nBPE tokenizers split text into subword units. Let‚Äôs examine how GPT-2 breaks down different types of text:\n\n\nCode\ndef analyze_tokenization(text, tokenizer_name=\"gpt2\"):\n    \"\"\"Analyze how tiktoken breaks down text.\"\"\"\n    tokenizer = tiktoken.get_encoding(tokenizer_name)\n    tokens = tokenizer.encode(text)\n    \n    print(f\"Text: '{text}'\")\n    print(f\"Tokens: {tokens}\")\n    print(f\"Token count: {len(tokens)}\")\n    print(\"Token breakdown:\")\n    \n    for i, token_id in enumerate(tokens):\n        token_text = tokenizer.decode([token_id])\n        print(f\"  {i:2d}: {token_id:5d} -&gt; '{token_text}'\")\n    print()\n\n# Test different types of text\nanalyze_tokenization(\"Hello world\")\nanalyze_tokenization(\"Tokenization\")  \nanalyze_tokenization(\"supercalifragilisticexpialidocious\")\nanalyze_tokenization(\"AI/ML researcher\")\nanalyze_tokenization(\"The year 2024\")\n\n\nText: 'Hello world'\nTokens: [15496, 995]\nToken count: 2\nToken breakdown:\n   0: 15496 -&gt; 'Hello'\n   1:   995 -&gt; ' world'\n\nText: 'Tokenization'\nTokens: [30642, 1634]\nToken count: 2\nToken breakdown:\n   0: 30642 -&gt; 'Token'\n   1:  1634 -&gt; 'ization'\n\nText: 'supercalifragilisticexpialidocious'\nTokens: [16668, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\nToken count: 11\nToken breakdown:\n   0: 16668 -&gt; 'super'\n   1:  9948 -&gt; 'cal'\n   2:   361 -&gt; 'if'\n   3: 22562 -&gt; 'rag'\n   4:   346 -&gt; 'il'\n   5:   396 -&gt; 'ist'\n   6:   501 -&gt; 'ice'\n   7: 42372 -&gt; 'xp'\n   8:   498 -&gt; 'ial'\n   9:   312 -&gt; 'id'\n  10: 32346 -&gt; 'ocious'\n\nText: 'AI/ML researcher'\nTokens: [20185, 14, 5805, 13453]\nToken count: 4\nToken breakdown:\n   0: 20185 -&gt; 'AI'\n   1:    14 -&gt; '/'\n   2:  5805 -&gt; 'ML'\n   3: 13453 -&gt; ' researcher'\n\nText: 'The year 2024'\nTokens: [464, 614, 48609]\nToken count: 3\nToken breakdown:\n   0:   464 -&gt; 'The'\n   1:   614 -&gt; ' year'\n   2: 48609 -&gt; ' 2024'\n\n\n\n\n\nWorking with Different Tiktoken Encodings\nOpenAI uses different encodings for different models:\n\n\nCode\n# Available encodings\navailable_encodings = [\"gpt2\", \"p50k_base\", \"cl100k_base\"]\n\ntest_text = \"Geospatial foundation models revolutionize remote sensing!\"\n\nfor encoding_name in available_encodings:\n    try:\n        tokenizer = tiktoken.get_encoding(encoding_name)\n        tokens = tokenizer.encode(test_text)\n        \n        print(f\"{encoding_name:12s}: {len(tokens):2d} tokens, vocab size: {tokenizer.n_vocab:6,}\")\n        print(f\"              Tokens: {tokens}\")\n        print()\n    except Exception as e:\n        print(f\"{encoding_name}: Error - {e}\")\n\n\ngpt2        : 10 tokens, vocab size: 50,257\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\np50k_base   : 10 tokens, vocab size: 50,281\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\ncl100k_base : 10 tokens, vocab size: 100,277\n              Tokens: [9688, 437, 33514, 16665, 4211, 14110, 553, 8870, 60199, 0]\n\n\n\n\n\nBPE for Geospatial Text\nLet‚Äôs see how BPE handles domain-specific geospatial terminology:\n\n\nCode\ngeospatial_texts = [\n    \"NDVI vegetation index analysis\",\n    \"Landsat-8 multispectral imagery\", \n    \"Convolutional neural networks for land cover classification\",\n    \"Sentinel-2 satellite data preprocessing\",\n    \"Geospatial foundation models fine-tuning\"\n]\n\ngpt2_enc = tiktoken.get_encoding(\"gpt2\")\n\nprint(\"=== Geospatial Text Tokenization ===\")\nfor text in geospatial_texts:\n    tokens = gpt2_enc.encode(text)\n    print(f\"'{text}'\")\n    print(f\"  -&gt; {len(tokens)} tokens: {tokens}\")\n    print()\n\n\n=== Geospatial Text Tokenization ===\n'NDVI vegetation index analysis'\n  -&gt; 5 tokens: [8575, 12861, 28459, 6376, 3781]\n\n'Landsat-8 multispectral imagery'\n  -&gt; 10 tokens: [43, 1746, 265, 12, 23, 1963, 271, 806, 1373, 19506]\n\n'Convolutional neural networks for land cover classification'\n  -&gt; 10 tokens: [3103, 85, 2122, 282, 17019, 7686, 329, 1956, 3002, 17923]\n\n'Sentinel-2 satellite data preprocessing'\n  -&gt; 8 tokens: [31837, 20538, 12, 17, 11210, 1366, 662, 36948]\n\n'Geospatial foundation models fine-tuning'\n  -&gt; 9 tokens: [10082, 2117, 34961, 8489, 4981, 3734, 12, 28286, 278]\n\n\n\n\n\nUnderstanding Token Efficiency\nThe choice of tokenizer affects model efficiency. Let‚Äôs compare token counts for our text corpus:\n\n\nCode\n# Use a sample from our corpus\nsample_text = raw_text[:500]  # First 500 characters\n\nprint(\"=== Token Efficiency Comparison ===\")\nprint(f\"Text length: {len(sample_text)} characters\")\nprint()\n\n# Our simple tokenizer\nour_tokens = enhanced_tokenizer.encode(sample_text, add_endoftext=False)\nprint(f\"Simple tokenizer: {len(our_tokens)} tokens\")\n\n# GPT-2 BPE\ngpt2_tokens = gpt2_tokenizer.encode(sample_text)\nprint(f\"GPT-2 BPE:       {len(gpt2_tokens)} tokens\")\n\n# Calculate efficiency\nchars_per_token_simple = len(sample_text) / len(our_tokens)\nchars_per_token_bpe = len(sample_text) / len(gpt2_tokens)\n\nprint(f\"\\nCharacters per token:\")\nprint(f\"  Simple: {chars_per_token_simple:.2f}\")\nprint(f\"  BPE:    {chars_per_token_bpe:.2f}\")\nprint(f\"  BPE is {chars_per_token_bpe/chars_per_token_simple:.1f}x more efficient\")\n\n\n=== Token Efficiency Comparison ===\nText length: 500 characters\n\nSimple tokenizer: 110 tokens\nGPT-2 BPE:       123 tokens\n\nCharacters per token:\n  Simple: 4.55\n  BPE:    4.07\n  BPE is 0.9x more efficient"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#key-takeaways",
    "href": "course-materials/extras/examples/text_encoder.html#key-takeaways",
    "title": "Deep Learning Basics",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSpecial Tokens: Essential for handling unknown words and sequence boundaries\nUnknown Token Handling: &lt;|unk|&gt; tokens allow models to gracefully handle out-of-vocabulary words\nEnd-of-Text Tokens: &lt;|endoftext|&gt; tokens help models understand sequence boundaries\nBPE Efficiency: Byte Pair Encoding creates a good balance between vocabulary size and representation efficiency\nDomain Adaptation: Different tokenizers may handle domain-specific text differently\n\nIn geospatial AI applications, choosing the right tokenization strategy affects both model performance and computational efficiency, especially when working with specialized terminology from remote sensing and Earth science domains."
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html",
    "href": "course-materials/extras/examples/resnet.html",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#overview",
    "href": "course-materials/extras/examples/resnet.html#overview",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#setup-and-imports",
    "href": "course-materials/extras/examples/resnet.html#setup-and-imports",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üì¶ Setup and Imports",
    "text": "üì¶ Setup and Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "href": "course-materials/extras/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìä Define Plain CNN and ResNet-like CNN",
    "text": "üìä Define Plain CNN and ResNet-like CNN\nclass PlainBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        return F.relu(out)\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(identity)\n        return F.relu(out)\n\nclass PlainCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = PlainBlock(3, 16)\n        self.block2 = PlainBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)\n\nclass ResNetMini(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = ResBlock(3, 16)\n        self.block2 = ResBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#load-cifar-10-data",
    "href": "course-materials/extras/examples/resnet.html#load-cifar-10-data",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üñºÔ∏è Load CIFAR-10 Data",
    "text": "üñºÔ∏è Load CIFAR-10 Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#training-loop-and-gradient-tracking",
    "href": "course-materials/extras/examples/resnet.html#training-loop-and-gradient-tracking",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üöÄ Training Loop and Gradient Tracking",
    "text": "üöÄ Training Loop and Gradient Tracking\ndef train_model(model, name, epochs=10):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loss = []\n    train_gradients = []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        grad_norm = 0\n        for inputs, targets in tqdm(train_loader, desc=f\"{name} Epoch {epoch+1}\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n\n            total_norm = 0\n            for p in model.parameters():\n                if p.grad is not None:\n                    param_norm = p.grad.data.norm(2)\n                    total_norm += param_norm.item() ** 2\n            grad_norm += total_norm ** 0.5\n\n            optimizer.step()\n            running_loss += loss.item()\n\n        train_loss.append(running_loss / len(train_loader))\n        train_gradients.append(grad_norm / len(train_loader))\n\n    return train_loss, train_gradients"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#train-and-compare",
    "href": "course-materials/extras/examples/resnet.html#train-and-compare",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìà Train and Compare",
    "text": "üìà Train and Compare\nplain_model = PlainCNN()\nresnet_model = ResNetMini()\n\nplain_loss, plain_grads = train_model(plain_model, \"PlainCNN\")\nresnet_loss, resnet_grads = train_model(resnet_model, \"ResNetMini\")"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "href": "course-materials/extras/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìä Plot Training Loss and Gradient Flow",
    "text": "üìä Plot Training Loss and Gradient Flow\nplt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\nplt.plot(plain_loss, label=\"PlainCNN\")\nplt.plot(resnet_loss, label=\"ResNetMini\")\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(plain_grads, label=\"PlainCNN\")\nplt.plot(resnet_grads, label=\"ResNetMini\")\nplt.title(\"Gradient Norm per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Average Gradient Norm\")\nplt.legend()\n\nplt.suptitle(\"Residual Connections Help Optimize Deeper Networks\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#conclusion",
    "href": "course-materials/extras/examples/resnet.html#conclusion",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "‚úÖ Conclusion",
    "text": "‚úÖ Conclusion\nThis notebook demonstrates that residual connections help preserve gradient flow, allowing deeper networks to train faster and more reliably. Even though our example was relatively shallow (2 blocks), the benefits in convergence and stability are clear."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html",
    "href": "course-materials/extras/examples/tiling-and-patches.html",
    "title": "Tiling & Patch Extraction",
    "section": "",
    "text": "This page shows two kinds of visuals that help explain common preprocessing steps for vision models:\n\nA high-level flow using Mermaid (no Python required)\nData-like mockups using Python + Matplotlib to draw tile and patch grids on a synthetic image\n\n\n\n\n\n\n\n\ngraph LR\n    A[Full Image] --&gt; B[Tiling]\n    B --&gt; C[Tile 1]\n    B --&gt; D[Tile 2]\n    B --&gt; E[Tile 3]\n    C --&gt; F[Patch Extraction]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Patch Set for Model]\n\n\n\n\n\n\n\nTip: Mermaid renders natively in Quarto HTML output. No extra setup needed for basic use."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#overview",
    "href": "course-materials/extras/examples/tiling-and-patches.html#overview",
    "title": "Tiling & Patch Extraction",
    "section": "",
    "text": "This page shows two kinds of visuals that help explain common preprocessing steps for vision models:\n\nA high-level flow using Mermaid (no Python required)\nData-like mockups using Python + Matplotlib to draw tile and patch grids on a synthetic image\n\n\n\n\n\n\n\n\ngraph LR\n    A[Full Image] --&gt; B[Tiling]\n    B --&gt; C[Tile 1]\n    B --&gt; D[Tile 2]\n    B --&gt; E[Tile 3]\n    C --&gt; F[Patch Extraction]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Patch Set for Model]\n\n\n\n\n\n\n\nTip: Mermaid renders natively in Quarto HTML output. No extra setup needed for basic use."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#data-like-mockups-python-matplotlib",
    "href": "course-materials/extras/examples/tiling-and-patches.html#data-like-mockups-python-matplotlib",
    "title": "Tiling & Patch Extraction",
    "section": "Data-like Mockups (Python + Matplotlib)",
    "text": "Data-like Mockups (Python + Matplotlib)\nBelow we generate a random ‚Äúimage‚Äù and draw grid lines to illustrate tiling and patch extraction. These mockups help students visualize how tiles and patches relate to array indices.\n\nImplementation notes: - All visuals use Matplotlib (no seaborn) and avoid custom color styles, per course standards. - Each figure is produced by a single, self-contained code chunk. - Random seed is fixed for reproducibility.\n\n\n1) Tiling Overlay on a Full Image\nWe split the image into equally sized tiles (here: 50√ó50). White grid lines mark tile boundaries.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Create subtle textured background with fixed colormap range ---\nnp.random.seed(42)\nimg = np.random.rand(100, 150)\n# Map random values to a narrow range around mid-gray (0.45 to 0.55)\n# This creates gentle texture while keeping the colormap fixed at 0-1\nimg = img * 0.1 + 0.45  # Subtle texture in 10% range around mid-gray\n\n# --- Tile size ---\ntile_h, tile_w = 50, 50\n\n# --- Plot ---\nfig, ax = plt.subplots(figsize=(6, 4))\nax.imshow(img, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# vertical lines (white for high contrast)\nfor x in range(0, img.shape[1] + 1, tile_w):\n    ax.axvline(x - 0.5, color='white', linewidth=2)\n\n# horizontal lines (white for high contrast)\nfor y in range(0, img.shape[0] + 1, tile_h):\n    ax.axhline(y - 0.5, color='white', linewidth=2)\n\n# Label each tile\ntile_rows = img.shape[0] // tile_h\ntile_cols = img.shape[1] // tile_w\nfor r in range(tile_rows):\n    for c in range(tile_cols):\n        center_y = r * tile_h + tile_h // 2\n        center_x = c * tile_w + tile_w // 2\n        ax.text(center_x, center_y, f\"Tile\\n({r},{c})\",\n                ha=\"center\", va=\"center\", fontsize=12,\n                color='white', weight='bold')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(\"Full Image with Tiling Grid (50x50)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2) Patch Extraction Inside a Single Tile\nZoom into the top-left tile (50√ó50) and split it into smaller patches (here: 10√ó10). Grid lines show patch boundaries; labels mark patch indices (row, col).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming `img` from the previous cell exists in the same execution environment.\n# If running independently, re-create it:\nnp.random.seed(42)\nimg = np.random.rand(100, 150)\n# Map random values to a narrow range around mid-gray (0.45 to 0.55)\nimg = img * 0.1 + 0.45  # Subtle texture in 10% range around mid-gray\n\n# --- Select the first tile: top-left (0:50, 0:50) ---\ntile = img[0:50, 0:50]\n\n# --- Patch size ---\npatch_h, patch_w = 10, 10\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(tile, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# Draw patch grid (white lines for high contrast)\nfor x in range(0, tile.shape[1] + 1, patch_w):\n    ax.axvline(x - 0.5, color='white', linewidth=1)\nfor y in range(0, tile.shape[0] + 1, patch_h):\n    ax.axhline(y - 0.5, color='white', linewidth=1)\n\n# Annotate patch indices with high-contrast text\nrows = tile.shape[0] // patch_h\ncols = tile.shape[1] // patch_w\nfor r in range(rows):\n    for c in range(cols):\n        y_center = r * patch_h + patch_h / 2\n        x_center = c * patch_w + patch_w / 2\n\n        # White text pops better on textured background\n        ax.text(x_center, y_center, f\"({r},{c})\",\n                ha=\"center\", va=\"center\", fontsize=10,\n                color='white', weight='bold')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(\"Zoomed Tile with Patch Grid (10x10)\")\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#advanced-patch-extraction-concepts",
    "href": "course-materials/extras/examples/tiling-and-patches.html#advanced-patch-extraction-concepts",
    "title": "Tiling & Patch Extraction",
    "section": "Advanced Patch Extraction Concepts",
    "text": "Advanced Patch Extraction Concepts\n\n3) Overlapping Patches with Stride\nWhen stride &lt; patch size, patches overlap. This is common in computer vision to capture more spatial information.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a smaller tile for clearer visualization\nnp.random.seed(42)\nsmall_img = np.random.rand(36, 45)  # Dimensions chosen to work cleanly with patch params\n# Map to subtle texture range\nsmall_img = small_img * 0.1 + 0.45  # Gentle texture around mid-gray\n\n# --- Patch parameters ---\npatch_size = 15\nstride = 9  # Creates 60% overlap (9/15), avoiding 2x multiples\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.imshow(small_img, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# Calculate patch positions\npatch_positions = []\nfor y in range(0, small_img.shape[0] - patch_size + 1, stride):\n    for x in range(0, small_img.shape[1] - patch_size + 1, stride):\n        patch_positions.append((x, y))\n\n# Draw overlapping patches with different colors\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\nfor i, (x, y) in enumerate(patch_positions[:6]):  # Show first 6 patches\n    color = colors[i % len(colors)]\n    # Draw patch boundary\n    rect = plt.Rectangle((x-0.5, y-0.5), patch_size, patch_size, \n                        linewidth=2, edgecolor=color, facecolor='none', alpha=0.8)\n    ax.add_patch(rect)\n    \n    # Label patch center with matching color\n    center_x, center_y = x + patch_size//2, y + patch_size//2\n    ax.text(center_x, center_y, f\"P{i}\", ha=\"center\", va=\"center\", \n            fontsize=10, color=color, weight='bold',\n            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9))\n\nax.set_xlim(-1, small_img.shape[1])\nax.set_ylim(small_img.shape[0], -1)\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(f\"Overlapping Patches (size={patch_size}, stride={stride})\")\nplt.show()\n\nprint(f\"Patch size: {patch_size}x{patch_size}\")\nprint(f\"Stride: {stride} (overlap = {patch_size - stride} pixels = {100*(patch_size-stride)/patch_size:.0f}%)\")\nprint(f\"Total patches extracted: {len(patch_positions)}\")\nprint(f\"Image dimensions: {small_img.shape[0]}x{small_img.shape[1]}\")\nprint(f\"Patches fit: {(small_img.shape[0] - patch_size) // stride + 1} rows x {(small_img.shape[1] - patch_size) // stride + 1} cols\")\n\n\n\n\n\n\n\n\n\nPatch size: 15x15\nStride: 9 (overlap = 6 pixels = 40%)\nTotal patches extracted: 12\nImage dimensions: 36x45\nPatches fit: 3 rows x 4 cols\n\n\n\n\n4) Handling Incomplete Patches: Padding Strategies\nWhen image dimensions don‚Äôt divide evenly by patch size, we face the ‚Äúedge problem‚Äù - what to do with incomplete patches at borders. Different strategies offer different trade-offs between computational efficiency, information preservation, and artifact introduction.\nFirst, let‚Äôs set up our test image and examine the edge problem:\n\n\nOriginal image: 40√ó56 pixels\nPatch size: 16√ó16 pixels\nComplete patches that fit: 2√ó3\nLeftover pixels: 8 rows, 8 columns\n==================================================\n\n\n\n\n\n\n\n\n\n\nStrategy 1: Crop (Discard Incomplete Patches)\nApproach: Simply ignore patches that don‚Äôt fit completely within the image boundaries.\nPros:\n\nFast and simple: No extra computation or memory\nNo artifacts: All patches contain only real image data\nPredictable output size: Easy to calculate exact number of patches\n\nCons:\n\nInformation loss: Edge and corner information is discarded\nUneven coverage: Some areas of the image are never processed\nSize dependency: Loss percentage varies with image dimensions\n\nUse cases: When speed is critical and edge information is less important (e.g., dense feature extraction where overlap provides coverage).\n\n\n\n\n\n\n\n\n\n\n\nStrategy 2: Zero Padding\nApproach: Extend the image with zero-valued pixels to make dimensions divisible by patch size.\nPros:\n\nComplete coverage: Every pixel is included in at least one patch\nSimple implementation: Easy to add zeros programmatically\nConsistent output size: Padding can be calculated in advance\n\nCons:\n\nArtificial boundaries: Sharp transitions from real data to zeros\nPotential artifacts: Models may learn to recognize padding patterns\nIncreased computation: More patches to process\n\nUse cases: When complete coverage is essential and models are robust to boundary artifacts (e.g., segmentation tasks).\n\n\n\n\n\n\n\n\n\n\n\nStrategy 3: Reflect Padding\nApproach: Mirror edge pixels to create natural-looking padding that preserves image structure.\nPros:\n\nNatural boundaries: Smooth transitions maintain local image statistics\nStructure preservation: Gradients and textures continue naturally\nBetter model behavior: Less likely to create learning artifacts\n\nCons:\n\nMore complex: Requires reflection logic for edges and corners\nSlight computation overhead: Must calculate reflected values\nMay duplicate features: Reflected content isn‚Äôt truly independent\n\nUse cases: When image quality and natural appearance are important (e.g., super-resolution, denoising, medical imaging)."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#choosing-padding-strategies-in-practice",
    "href": "course-materials/extras/examples/tiling-and-patches.html#choosing-padding-strategies-in-practice",
    "title": "Tiling & Patch Extraction",
    "section": "Choosing Padding Strategies in Practice",
    "text": "Choosing Padding Strategies in Practice\n\nImplementation Examples\nimport torch.nn.functional as F\n\n# PyTorch examples for different padding strategies\ndef apply_padding_strategy(image_tensor, patch_size, strategy='reflect'):\n    \\\"\\\"\\\"\n    Apply padding to make image divisible by patch_size\n    \n    Args:\n        image_tensor: (C, H, W) tensor\n        patch_size: int, size of square patches\n        strategy: 'crop', 'zero', or 'reflect'\n    \\\"\\\"\\\"\n    C, H, W = image_tensor.shape\n    \n    if strategy == 'crop':\n        # Calculate largest area that fits complete patches\n        new_h = (H // patch_size) * patch_size\n        new_w = (W // patch_size) * patch_size\n        return image_tensor[:, :new_h, :new_w]\n    \n    elif strategy == 'zero':\n        # Calculate padding needed\n        pad_h = patch_size - (H % patch_size) if H % patch_size != 0 else 0\n        pad_w = patch_size - (W % patch_size) if W % patch_size != 0 else 0\n        return F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='constant', value=0)\n    \n    elif strategy == 'reflect':\n        pad_h = patch_size - (H % patch_size) if H % patch_size != 0 else 0\n        pad_w = patch_size - (W % patch_size) if W % patch_size != 0 else 0\n        return F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='reflect')\n\n\nDecision Framework\nChoose CROP when: - Processing large datasets where speed &gt;&gt; completeness - Using overlapping patches that provide edge coverage - Edge regions are less important for your task\nChoose ZERO PADDING when: - Need guaranteed complete coverage - Model architecture handles boundaries well - Working with synthetic or highly structured data\nChoose REFLECT PADDING when: - Image quality is paramount - Working with natural images where structure matters - Model will be sensitive to boundary artifacts\n\n\nReal-World Considerations\n\nBatch processing: Padding strategies affect batch consistency\nMemory usage: Padding increases tensor size and memory requirements\nPost-processing: May need to crop back to original dimensions after inference\nData augmentation: Padding interacts with augmentation strategies (rotation, flipping)\n\nMost modern frameworks (PyTorch, TensorFlow) implement all three strategies efficiently, making the choice primarily about the specific requirements of your application rather than implementation difficulty."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#key-takeaways",
    "href": "course-materials/extras/examples/tiling-and-patches.html#key-takeaways",
    "title": "Tiling & Patch Extraction",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nOverlapping patches capture more spatial information but increase computational cost\nStride controls overlap: smaller stride = more overlap = more patches\nPadding strategies handle images that don‚Äôt divide evenly by patch size:\n\nCrop: Fast but loses edge information\nZero padding: Simple but introduces artificial boundaries\n\nReflect padding: More natural boundaries, preserves edge information"
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#variations-you-can-try",
    "href": "course-materials/extras/examples/tiling-and-patches.html#variations-you-can-try",
    "title": "Tiling & Patch Extraction",
    "section": "Variations You Can Try",
    "text": "Variations You Can Try\n\nExperiment with different stride values (1, 3, 6, 12) to see overlap effects\nTry other padding strategies like ‚Äúconstant‚Äù or ‚Äúwrap‚Äù modes\nCompare patch counts and computational requirements across strategies"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html",
    "title": "Data Loading for Satellite Imagery",
    "section": "",
    "text": "Efficient data loading is crucial for satellite imagery analysis due to large file sizes, multiple spectral bands, and complex geospatial metadata.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport xarray as xr\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\nPyTorch version: 2.7.1\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#introduction-to-satellite-data-loading",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#introduction-to-satellite-data-loading",
    "title": "Data Loading for Satellite Imagery",
    "section": "",
    "text": "Efficient data loading is crucial for satellite imagery analysis due to large file sizes, multiple spectral bands, and complex geospatial metadata.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport xarray as xr\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\nPyTorch version: 2.7.1\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#basic-dataset-patterns",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#basic-dataset-patterns",
    "title": "Data Loading for Satellite Imagery",
    "section": "Basic Dataset Patterns",
    "text": "Basic Dataset Patterns\n\nSimple satellite dataset\n\nclass SatelliteDataset(Dataset):\n    \"\"\"Basic satellite imagery dataset\"\"\"\n    \n    def __init__(self, image_paths, patch_size=256, transform=None):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.patch_size = patch_size\n        self.transform = transform\n        \n        # Pre-compute dataset info\n        self._scan_images()\n    \n    def _scan_images(self):\n        \"\"\"Scan images to get metadata\"\"\"\n        self.image_info = []\n        \n        for path in self.image_paths:\n            # In practice, you'd open actual files\n            # For demo, simulate metadata\n            info = {\n                'path': path,\n                'width': 1024,\n                'height': 1024, \n                'bands': 4,\n                'dtype': np.uint16\n            }\n            self.image_info.append(info)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        info = self.image_info[idx]\n        \n        # Simulate loading satellite data\n        # In practice: data = rasterio.open(info['path']).read()\n        data = np.random.randint(0, 4096, \n                                (info['bands'], self.patch_size, self.patch_size),\n                                dtype=np.uint16)\n        \n        # Convert to tensor\n        tensor_data = torch.from_numpy(data).float() / 4095.0  # Normalize\n        \n        sample = {\n            'image': tensor_data,\n            'path': str(info['path']),\n            'metadata': info\n        }\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample\n\n# Example usage\nimage_paths = ['image1.tif', 'image2.tif', 'image3.tif']\ndataset = SatelliteDataset(image_paths, patch_size=256)\n\nprint(f\"Dataset size: {len(dataset)}\")\nprint(f\"Sample keys: {list(dataset[0].keys())}\")\nprint(f\"Image shape: {dataset[0]['image'].shape}\")\n\nDataset size: 3\nSample keys: ['image', 'path', 'metadata']\nImage shape: torch.Size([4, 256, 256])\n\n\n\n\nMulti-temporal dataset\n\nclass TemporalSatelliteDataset(Dataset):\n    \"\"\"Dataset for temporal satellite imagery sequences\"\"\"\n    \n    def __init__(self, data_root, sequence_length=5, time_step=30):\n        self.data_root = Path(data_root)\n        self.sequence_length = sequence_length\n        self.time_step = time_step  # Days between images\n        \n        # In practice, scan directory for date-organized images\n        self.sequences = self._find_temporal_sequences()\n    \n    def _find_temporal_sequences(self):\n        \"\"\"Find valid temporal sequences\"\"\"\n        # Simulate finding temporal sequences\n        sequences = []\n        for i in range(10):  # 10 example sequences\n            start_date = f\"2020-{(i % 12) + 1:02d}-01\"\n            sequence = {\n                'start_date': start_date,\n                'location_id': f'tile_{i:03d}',\n                'file_pattern': f'tile_{i:03d}_*.tif'\n            }\n            sequences.append(sequence)\n        return sequences\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n        \n        # Load temporal sequence\n        images = []\n        for t in range(self.sequence_length):\n            # Simulate temporal progression\n            # Each image in sequence has slight variations\n            base_image = np.random.randn(4, 256, 256) + t * 0.1\n            images.append(base_image)\n        \n        # Stack temporal dimension: [T, C, H, W]\n        temporal_stack = np.stack(images, axis=0)\n        tensor_stack = torch.from_numpy(temporal_stack).float()\n        \n        return {\n            'images': tensor_stack,\n            'sequence_id': sequence['location_id'],\n            'start_date': sequence['start_date'],\n            'time_steps': self.sequence_length\n        }\n\n# Example usage\ntemporal_dataset = TemporalSatelliteDataset('data/', sequence_length=5)\nsample = temporal_dataset[0]\n\nprint(f\"Temporal dataset size: {len(temporal_dataset)}\")\nprint(f\"Image sequence shape: {sample['images'].shape}\")\nprint(f\"Sequence ID: {sample['sequence_id']}\")\n\nTemporal dataset size: 10\nImage sequence shape: torch.Size([5, 4, 256, 256])\nSequence ID: tile_000"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#memory-efficient-loading",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#memory-efficient-loading",
    "title": "Data Loading for Satellite Imagery",
    "section": "Memory-Efficient Loading",
    "text": "Memory-Efficient Loading\n\nWindowed reading for large files\n\nclass WindowedSatelliteDataset(Dataset):\n    \"\"\"Dataset that reads windows from large satellite images\"\"\"\n    \n    def __init__(self, image_path, window_size=512, stride=256, max_windows=None):\n        self.image_path = Path(image_path)\n        self.window_size = window_size\n        self.stride = stride\n        \n        # Pre-compute all valid windows\n        self.windows = self._compute_windows()\n        \n        if max_windows and len(self.windows) &gt; max_windows:\n            self.windows = self.windows[:max_windows]\n    \n    def _compute_windows(self):\n        \"\"\"Compute all valid windows for the image\"\"\"\n        # In practice, use rasterio to get actual dimensions\n        # Simulate large image dimensions\n        img_height, img_width = 4096, 4096\n        \n        windows = []\n        for row in range(0, img_height - self.window_size + 1, self.stride):\n            for col in range(0, img_width - self.window_size + 1, self.stride):\n                window = Window(col, row, self.window_size, self.window_size)\n                windows.append(window)\n        \n        return windows\n    \n    def __len__(self):\n        return len(self.windows)\n    \n    def __getitem__(self, idx):\n        window = self.windows[idx]\n        \n        # In practice: \n        # with rasterio.open(self.image_path) as src:\n        #     data = src.read(window=window)\n        \n        # Simulate reading window\n        data = np.random.randint(0, 2048, \n                                (4, self.window_size, self.window_size),\n                                dtype=np.uint16)\n        \n        tensor_data = torch.from_numpy(data).float() / 2047.0\n        \n        return {\n            'image': tensor_data,\n            'window': window,\n            'window_bounds': (window.col_off, window.row_off, \n                            window.width, window.height)\n        }\n\n# Example usage  \nwindowed_dataset = WindowedSatelliteDataset('large_image.tif', \n                                           window_size=512, \n                                           stride=256,\n                                           max_windows=100)\n\nprint(f\"Windowed dataset size: {len(windowed_dataset)}\")\nprint(f\"First window shape: {windowed_dataset[0]['image'].shape}\")\nprint(f\"Window bounds: {windowed_dataset[0]['window_bounds']}\")\n\nWindowed dataset size: 100\nFirst window shape: torch.Size([4, 512, 512])\nWindow bounds: (0, 0, 512, 512)\n\n\n\n\nLazy loading with caching\n\nfrom functools import lru_cache\nfrom threading import Lock\n\nclass CachedSatelliteDataset(Dataset):\n    \"\"\"Dataset with intelligent caching for repeated access\"\"\"\n    \n    def __init__(self, image_paths, cache_size=50, patch_size=256):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.patch_size = patch_size\n        self.cache_lock = Lock()\n        \n        # LRU cache for loaded images\n        self._load_image = lru_cache(maxsize=cache_size)(self._load_image_uncached)\n    \n    def _load_image_uncached(self, image_path):\n        \"\"\"Load image without caching (wrapped by LRU cache)\"\"\"\n        # In practice: load with rasterio or other library\n        # Simulate loading time and memory usage\n        print(f\"Loading {image_path} into cache...\")\n        \n        # Simulate different image sizes and properties\n        bands = np.random.choice([3, 4, 8, 12])  # Different satellite sensors\n        data = np.random.randint(0, 4096, \n                                (bands, self.patch_size, self.patch_size),\n                                dtype=np.uint16)\n        \n        return {\n            'data': data,\n            'bands': bands,\n            'loaded_at': torch.tensor(0)  # Timestamp placeholder\n        }\n    \n    def __len__(self):\n        return len(self.image_paths) * 4  # Multiple patches per image\n    \n    def __getitem__(self, idx):\n        image_idx = idx // 4\n        patch_idx = idx % 4\n        \n        image_path = str(self.image_paths[image_idx])\n        \n        with self.cache_lock:\n            image_data = self._load_image(image_path)\n        \n        # Extract patch (simulate different patches from same image)\n        data = image_data['data'].copy()\n        \n        # Add some variation for different patches\n        if patch_idx &gt; 0:\n            noise = np.random.normal(0, 50, data.shape).astype(data.dtype)\n            data = np.clip(data.astype(np.int32) + noise, 0, 4095).astype(np.uint16)\n        \n        tensor_data = torch.from_numpy(data).float() / 4095.0\n        \n        return {\n            'image': tensor_data,\n            'image_idx': image_idx,\n            'patch_idx': patch_idx,\n            'bands': image_data['bands']\n        }\n    \n    def cache_info(self):\n        \"\"\"Get cache statistics\"\"\"\n        return self._load_image.cache_info()\n\n# Example usage\ncached_dataset = CachedSatelliteDataset(image_paths[:3], cache_size=10)\n\n# Load some samples (first loads will cache images)\nfor i in range(6):\n    sample = cached_dataset[i]\n    print(f\"Sample {i}: bands={sample['bands']}, \"\n          f\"image_idx={sample['image_idx']}, patch_idx={sample['patch_idx']}\")\n\nprint(f\"Cache statistics: {cached_dataset.cache_info()}\")\n\nLoading image1.tif into cache...\nSample 0: bands=8, image_idx=0, patch_idx=0\nSample 1: bands=8, image_idx=0, patch_idx=1\nSample 2: bands=8, image_idx=0, patch_idx=2\nSample 3: bands=8, image_idx=0, patch_idx=3\nLoading image2.tif into cache...\nSample 4: bands=8, image_idx=1, patch_idx=0\nSample 5: bands=8, image_idx=1, patch_idx=1\nCache statistics: CacheInfo(hits=4, misses=2, maxsize=10, currsize=2)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#advanced-data-loading-strategies",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#advanced-data-loading-strategies",
    "title": "Data Loading for Satellite Imagery",
    "section": "Advanced Data Loading Strategies",
    "text": "Advanced Data Loading Strategies\n\nMulti-resolution dataset\n\nclass MultiResolutionDataset(Dataset):\n    \"\"\"Dataset providing multiple resolutions of the same data\"\"\"\n    \n    def __init__(self, image_paths, resolutions=[128, 256, 512]):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.resolutions = sorted(resolutions)\n        self.base_resolution = max(resolutions)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def _resize_tensor(self, tensor, target_size):\n        \"\"\"Resize tensor to target size\"\"\"\n        import torch.nn.functional as F\n        \n        # Add batch dimension for interpolation\n        tensor_4d = tensor.unsqueeze(0)  # [1, C, H, W]\n        \n        resized = F.interpolate(\n            tensor_4d, \n            size=(target_size, target_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return resized.squeeze(0)  # Remove batch dimension\n    \n    def __getitem__(self, idx):\n        # Load base resolution data\n        base_data = np.random.randint(0, 4096, \n                                     (4, self.base_resolution, self.base_resolution),\n                                     dtype=np.uint16)\n        base_tensor = torch.from_numpy(base_data).float() / 4095.0\n        \n        # Create multi-resolution versions\n        multi_res = {}\n        for res in self.resolutions:\n            if res == self.base_resolution:\n                multi_res[f'image_{res}'] = base_tensor\n            else:\n                multi_res[f'image_{res}'] = self._resize_tensor(base_tensor, res)\n        \n        # Add metadata\n        multi_res.update({\n            'path': str(self.image_paths[idx]),\n            'base_resolution': self.base_resolution,\n            'available_resolutions': self.resolutions\n        })\n        \n        return multi_res\n\n# Example usage\nmulti_res_dataset = MultiResolutionDataset(image_paths, resolutions=[128, 256, 512])\nsample = multi_res_dataset[0]\n\nprint(\"Multi-resolution sample keys:\", list(sample.keys()))\nfor key in sample.keys():\n    if key.startswith('image_'):\n        print(f\"{key}: {sample[key].shape}\")\n\nMulti-resolution sample keys: ['image_128', 'image_256', 'image_512', 'path', 'base_resolution', 'available_resolutions']\nimage_128: torch.Size([4, 128, 128])\nimage_256: torch.Size([4, 256, 256])\nimage_512: torch.Size([4, 512, 512])\n\n\n\n\nBalanced sampling dataset\n\nclass BalancedSatelliteDataset(Dataset):\n    \"\"\"Dataset with balanced sampling across different conditions\"\"\"\n    \n    def __init__(self, image_paths, labels, balance_strategy='oversample'):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.labels = np.array(labels)\n        self.balance_strategy = balance_strategy\n        \n        # Compute class weights and sampling indices\n        self.class_counts = np.bincount(self.labels)\n        self.num_classes = len(self.class_counts)\n        self._compute_sampling_indices()\n    \n    def _compute_sampling_indices(self):\n        \"\"\"Compute sampling indices for balanced loading\"\"\"\n        if self.balance_strategy == 'oversample':\n            # Oversample minority classes\n            max_count = np.max(self.class_counts)\n            \n            self.sampling_indices = []\n            for class_id in range(self.num_classes):\n                class_indices = np.where(self.labels == class_id)[0]\n                \n                # Repeat indices to match max count\n                repeats = max_count // len(class_indices)\n                remainder = max_count % len(class_indices)\n                \n                oversampled = np.tile(class_indices, repeats)\n                if remainder &gt; 0:\n                    extra = np.random.choice(class_indices, remainder, replace=False)\n                    oversampled = np.concatenate([oversampled, extra])\n                \n                self.sampling_indices.extend(oversampled)\n            \n            # Shuffle the indices\n            self.sampling_indices = np.array(self.sampling_indices)\n            np.random.shuffle(self.sampling_indices)\n        \n        elif self.balance_strategy == 'undersample':\n            # Undersample majority classes\n            min_count = np.min(self.class_counts)\n            \n            self.sampling_indices = []\n            for class_id in range(self.num_classes):\n                class_indices = np.where(self.labels == class_id)[0]\n                sampled = np.random.choice(class_indices, min_count, replace=False)\n                self.sampling_indices.extend(sampled)\n            \n            self.sampling_indices = np.array(self.sampling_indices)\n            np.random.shuffle(self.sampling_indices)\n    \n    def __len__(self):\n        return len(self.sampling_indices)\n    \n    def __getitem__(self, idx):\n        actual_idx = self.sampling_indices[idx]\n        \n        # Load satellite image\n        data = np.random.randint(0, 4096, (4, 256, 256), dtype=np.uint16)\n        tensor_data = torch.from_numpy(data).float() / 4095.0\n        \n        return {\n            'image': tensor_data,\n            'label': torch.tensor(self.labels[actual_idx], dtype=torch.long),\n            'original_idx': actual_idx,\n            'path': str(self.image_paths[actual_idx])\n        }\n\n# Example usage with imbalanced classes\nnp.random.seed(42)\nlabels = np.random.choice([0, 1, 2], size=50, p=[0.7, 0.2, 0.1])  # Imbalanced\n\n# Ensure we have one image path per label index used below\nimage_paths = [f\"image_{i}.tif\" for i in range(len(labels))]\n\nbalanced_dataset = BalancedSatelliteDataset(\n    image_paths[:50], \n    labels, \n    balance_strategy='oversample'\n)\n\n# Check class distribution in balanced dataset\nsample_labels = [balanced_dataset[i]['label'].item() for i in range(len(balanced_dataset))]\nbalanced_counts = np.bincount(sample_labels)\n\nprint(f\"Original class distribution: {np.bincount(labels)}\")\nprint(f\"Balanced class distribution: {balanced_counts}\")\nprint(f\"Balanced dataset size: {len(balanced_dataset)}\")\n\nOriginal class distribution: [39  6  5]\nBalanced class distribution: [39 39 39]\nBalanced dataset size: 117"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#dataloader-optimization",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#dataloader-optimization",
    "title": "Data Loading for Satellite Imagery",
    "section": "DataLoader Optimization",
    "text": "DataLoader Optimization\n\nCustom collate functions\n\ndef satellite_collate_fn(batch):\n    \"\"\"Custom collate function for satellite imagery batches\"\"\"\n    \n    # Handle variable number of bands\n    max_bands = max(sample['image'].shape[0] for sample in batch)\n    batch_size = len(batch)\n    \n    # Get common spatial dimensions\n    height = batch[0]['image'].shape[1]\n    width = batch[0]['image'].shape[2]\n    \n    # Pad images to same number of bands\n    padded_images = torch.zeros(batch_size, max_bands, height, width)\n    labels = []\n    paths = []\n    band_masks = torch.zeros(batch_size, max_bands, dtype=torch.bool)\n    \n    for i, sample in enumerate(batch):\n        img = sample['image']\n        num_bands = img.shape[0]\n        \n        padded_images[i, :num_bands] = img\n        band_masks[i, :num_bands] = True\n        \n        if 'label' in sample:\n            labels.append(sample['label'])\n        paths.append(sample['path'])\n    \n    result = {\n        'image': padded_images,\n        'band_mask': band_masks,\n        'path': paths\n    }\n    \n    if labels:\n        result['label'] = torch.stack(labels)\n    \n    return result\n\ndef variable_size_collate_fn(batch):\n    \"\"\"Collate function for variable-sized images\"\"\"\n    \n    # Find max dimensions\n    max_height = max(sample['image'].shape[1] for sample in batch)\n    max_width = max(sample['image'].shape[2] for sample in batch)\n    max_bands = max(sample['image'].shape[0] for sample in batch)\n    \n    batch_size = len(batch)\n    \n    # Create padded batch\n    padded_batch = torch.zeros(batch_size, max_bands, max_height, max_width)\n    size_masks = []\n    \n    for i, sample in enumerate(batch):\n        img = sample['image']\n        c, h, w = img.shape\n        \n        padded_batch[i, :c, :h, :w] = img\n        \n        # Create mask for valid pixels\n        mask = torch.zeros(max_height, max_width, dtype=torch.bool)\n        mask[:h, :w] = True\n        size_masks.append(mask)\n    \n    return {\n        'image': padded_batch,\n        'size_mask': torch.stack(size_masks),\n        'original_sizes': [(s['image'].shape[1], s['image'].shape[2]) for s in batch]\n    }\n\n# Example usage\ndataloader = DataLoader(\n    dataset, \n    batch_size=4, \n    shuffle=True,\n    collate_fn=satellite_collate_fn,\n    num_workers=0,\n    pin_memory=True\n)\n\n# Test the dataloader\nbatch = next(iter(dataloader))\nprint(f\"Batch image shape: {batch['image'].shape}\")\nprint(f\"Band mask shape: {batch['band_mask'].shape}\")\nprint(f\"Paths: {len(batch['path'])}\")\n\nBatch image shape: torch.Size([3, 4, 256, 256])\nBand mask shape: torch.Size([3, 4])\nPaths: 3\n\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning:\n\n'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n\n\n\n\n\nPerformance optimization\n\ndef create_optimized_dataloader(dataset, batch_size=32, num_workers=4):\n    \"\"\"Create optimized dataloader for satellite imagery\"\"\"\n    \n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=torch.cuda.is_available(),  # Pin memory if GPU available\n        persistent_workers=True,  # Keep workers alive between epochs\n        prefetch_factor=2,  # Prefetch 2 batches per worker\n        drop_last=True,  # Consistent batch sizes\n        collate_fn=satellite_collate_fn\n    )\n\n# Memory usage monitoring\ndef monitor_memory_usage(dataloader, num_batches=5):\n    \"\"\"Monitor memory usage during data loading\"\"\"\n    \n    if torch.cuda.is_available():\n        print(\"GPU memory monitoring:\")\n        torch.cuda.reset_peak_memory_stats()\n        initial_memory = torch.cuda.memory_allocated()\n        \n        for i, batch in enumerate(dataloader):\n            if i &gt;= num_batches:\n                break\n            \n            current_memory = torch.cuda.memory_allocated()\n            peak_memory = torch.cuda.max_memory_allocated()\n            \n            print(f\"Batch {i}: Current={current_memory/1e6:.1f}MB, \"\n                  f\"Peak={peak_memory/1e6:.1f}MB\")\n    else:\n        print(\"GPU not available for memory monitoring\")\n\n# Example usage\noptimized_loader = create_optimized_dataloader(dataset, batch_size=8)\n# monitor_memory_usage(optimized_loader)\n\nprint(\"Optimized dataloader created\")\n\nOptimized dataloader created"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#real-world-integration-examples",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#real-world-integration-examples",
    "title": "Data Loading for Satellite Imagery",
    "section": "Real-World Integration Examples",
    "text": "Real-World Integration Examples\n\nIntegration with preprocessing pipeline\n\nclass PreprocessingSatelliteDataset(Dataset):\n    \"\"\"Dataset with integrated preprocessing pipeline\"\"\"\n    \n    def __init__(self, image_paths, preprocessing_config=None):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.config = preprocessing_config or self._default_config()\n    \n    def _default_config(self):\n        \"\"\"Default preprocessing configuration\"\"\"\n        return {\n            'normalize': True,\n            'clip_percentiles': (2, 98),\n            'target_bands': [2, 3, 4, 7],  # RGB + NIR\n            'target_resolution': 256,\n            'augment': True\n        }\n    \n    def _preprocess_image(self, image):\n        \"\"\"Apply preprocessing pipeline\"\"\"\n        \n        # Select target bands\n        if self.config['target_bands']:\n            available_bands = min(image.shape[0], max(self.config['target_bands']) + 1)\n            target_bands = [b for b in self.config['target_bands'] if b &lt; available_bands]\n            image = image[target_bands]\n        \n        # Normalize\n        if self.config['normalize']:\n            for band in range(image.shape[0]):\n                band_data = image[band]\n                if self.config['clip_percentiles']:\n                    p_low, p_high = self.config['clip_percentiles']\n                    low_val = np.percentile(band_data, p_low)\n                    high_val = np.percentile(band_data, p_high)\n                    band_data = np.clip(band_data, low_val, high_val)\n                \n                # Normalize to [0, 1]\n                band_min, band_max = band_data.min(), band_data.max()\n                if band_max &gt; band_min:\n                    image[band] = (band_data - band_min) / (band_max - band_min)\n        \n        return image\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Simulate loading multi-band satellite image\n        num_bands = np.random.choice([4, 8, 12])  # Different sensors\n        base_resolution = np.random.choice([256, 512])\n        \n        # Simulate realistic satellite data values\n        image = np.random.randint(100, 4000, \n                                 (num_bands, base_resolution, base_resolution),\n                                 dtype=np.uint16).astype(np.float32)\n        \n        # Apply preprocessing\n        processed_image = self._preprocess_image(image)\n        \n        # Convert to tensor\n        tensor_image = torch.from_numpy(processed_image)\n        \n        return {\n            'image': tensor_image,\n            'path': str(self.image_paths[idx]),\n            'original_bands': num_bands,\n            'processed_bands': processed_image.shape[0]\n        }\n\n# Example usage\npreprocessing_config = {\n    'normalize': True,\n    'clip_percentiles': (1, 99),\n    'target_bands': [0, 1, 2, 3],  # First 4 bands\n    'augment': False\n}\n\npreprocessed_dataset = PreprocessingSatelliteDataset(\n    image_paths, \n    preprocessing_config=preprocessing_config\n)\n\nsample = preprocessed_dataset[0]\nprint(f\"Preprocessed sample shape: {sample['image'].shape}\")\nprint(f\"Original bands: {sample['original_bands']}\")\nprint(f\"Processed bands: {sample['processed_bands']}\")\nprint(f\"Value range: [{sample['image'].min():.3f}, {sample['image'].max():.3f}]\")\n\nPreprocessed sample shape: torch.Size([4, 256, 256])\nOriginal bands: 4\nProcessed bands: 4\nValue range: [0.000, 1.000]"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#summary",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#summary",
    "title": "Data Loading for Satellite Imagery",
    "section": "Summary",
    "text": "Summary\nKey data loading strategies for satellite imagery: - Memory efficiency: Window-based reading, caching, lazy loading - Multi-temporal: Handle time series of satellite observations\n- Multi-resolution: Provide different spatial resolutions - Balanced sampling: Handle imbalanced datasets - Custom collation: Handle variable bands and sizes - Preprocessing integration: Normalization, band selection, augmentation - Performance optimization: Multi-processing, memory pinning, prefetching"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#creating-sample-data",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#creating-sample-data",
    "title": "Working with Rasterio",
    "section": "Creating Sample Data",
    "text": "Creating Sample Data\nSince we may not have actual satellite imagery files, let‚Äôs create some sample raster data:\n\n# Create sample raster data\ndef create_sample_raster():\n    # Create a simple 100x100 raster with some pattern\n    height, width = 100, 100\n    \n    # Create coordinate grids\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Create a pattern (e.g., circular pattern)\n    data = np.sin(np.sqrt(X**2 + Y**2)) * 255\n    data = data.astype(np.uint8)\n    \n    return data\n\nsample_data = create_sample_raster()\nprint(f\"Sample data shape: {sample_data.shape}\")\nprint(f\"Data type: {sample_data.dtype}\")\nprint(f\"Data range: {sample_data.min()} to {sample_data.max()}\")\n\nSample data shape: (100, 100)\nData type: uint8\nData range: 7 to 254"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "title": "Working with Rasterio",
    "section": "Working with Raster Arrays",
    "text": "Working with Raster Arrays\n\nBasic array operations\n\n# Basic statistics\nprint(f\"Mean: {np.mean(sample_data):.2f}\")\nprint(f\"Standard deviation: {np.std(sample_data):.2f}\")\nprint(f\"Unique values: {len(np.unique(sample_data))}\")\n\n# Histogram of values\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Sample Raster Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.hist(sample_data.flatten(), bins=50, alpha=0.7)\nplt.title('Histogram of Values')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\nMean: 215.01\nStandard deviation: 46.13\nUnique values: 182\n\n\n\n\n\n\n\n\n\n\n\nArray masking and filtering\n\n# Create masks\nhigh_values = sample_data &gt; 200\nlow_values = sample_data &lt; 50\n\nprint(f\"Pixels with high values (&gt;200): {np.sum(high_values)}\")\nprint(f\"Pixels with low values (&lt;50): {np.sum(low_values)}\")\n\n# Apply mask\nmasked_data = np.where(high_values, sample_data, 0)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original Data')\n\nplt.subplot(1, 3, 2)\nplt.imshow(high_values, cmap='gray')\nplt.title('High Value Mask')\n\nplt.subplot(1, 3, 3)\nplt.imshow(masked_data, cmap='viridis')\nplt.title('Masked Data')\n\nplt.tight_layout()\nplt.show()\n\nPixels with high values (&gt;200): 7364\nPixels with low values (&lt;50): 76"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "title": "Working with Rasterio",
    "section": "Raster Metadata and Transforms",
    "text": "Raster Metadata and Transforms\n\nUnderstanding geospatial transforms\n\nfrom rasterio.transform import from_bounds\n\n# Create a sample transform (geographic coordinates)\nwest, south, east, north = -120.0, 35.0, -115.0, 40.0  # Bounding box\ntransform = from_bounds(west, south, east, north, 100, 100)\n\nprint(f\"Transform: {transform}\")\nprint(f\"Pixel width: {transform[0]}\")\nprint(f\"Pixel height: {abs(transform[4])}\")\n\n# Convert pixel coordinates to geographic coordinates\ndef pixel_to_geo(row, col, transform):\n    x, y = rasterio.transform.xy(transform, row, col)\n    return x, y\n\n# Example: center pixel\ncenter_row, center_col = 50, 50\ngeo_x, geo_y = pixel_to_geo(center_row, center_col, transform)\nprint(f\"Center pixel ({center_row}, {center_col}) -&gt; Geographic: ({geo_x:.2f}, {geo_y:.2f})\")\n\nTransform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|\nPixel width: 0.05\nPixel height: 0.05\nCenter pixel (50, 50) -&gt; Geographic: (-117.47, 37.48)\n\n\n\n\nCreating profiles for writing rasters\n\n# Create a profile for writing raster data\nprofile = {\n    'driver': 'GTiff',\n    'dtype': 'uint8',\n    'nodata': None,\n    'width': 100,\n    'height': 100,\n    'count': 1,\n    'crs': 'EPSG:4326',  # WGS84 lat/lon\n    'transform': transform\n}\n\nprint(\"Raster profile:\")\nfor key, value in profile.items():\n    print(f\"  {key}: {value}\")\n\nRaster profile:\n  driver: GTiff\n  dtype: uint8\n  nodata: None\n  width: 100\n  height: 100\n  count: 1\n  crs: EPSG:4326\n  transform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#multi-band-operations",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#multi-band-operations",
    "title": "Working with Rasterio",
    "section": "Multi-band Operations",
    "text": "Multi-band Operations\n\nWorking with multi-band data\n\n# Create multi-band sample data (RGB-like)\ndef create_multiband_sample():\n    height, width = 100, 100\n    bands = 3\n    \n    # Create different patterns for each band\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Band 1: Circular pattern\n    band1 = (np.sin(np.sqrt(X**2 + Y**2)) * 127 + 127).astype(np.uint8)\n    \n    # Band 2: Linear gradient\n    band2 = (np.linspace(0, 255, width) * np.ones((height, 1))).astype(np.uint8)\n    \n    # Band 3: Checkerboard pattern\n    band3 = ((X + Y) &gt; 0).astype(np.uint8) * 255\n    \n    return np.stack([band1, band2, band3])\n\nmultiband_data = create_multiband_sample()\nprint(f\"Multiband shape: {multiband_data.shape}\")\nprint(f\"Shape format: (bands, height, width)\")\n\nMultiband shape: (3, 100, 100)\nShape format: (bands, height, width)\n\n\n\n\nVisualizing multi-band data\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# Individual bands\nfor i in range(3):\n    row, col = i // 2, i % 2\n    axes[row, col].imshow(multiband_data[i], cmap='gray')\n    axes[row, col].set_title(f'Band {i+1}')\n\n# RGB composite (transpose for matplotlib)\nrgb_composite = np.transpose(multiband_data, (1, 2, 0))\naxes[1, 1].imshow(rgb_composite)\naxes[1, 1].set_title('RGB Composite')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "title": "Working with Rasterio",
    "section": "Band Math and Indices",
    "text": "Band Math and Indices\n\nCalculate vegetation index (NDVI-like)\n\n# Simulate NIR and Red bands\nnir_band = multiband_data[0].astype(np.float32)\nred_band = multiband_data[1].astype(np.float32)\n\n# Calculate NDVI-like index\n# NDVI = (NIR - Red) / (NIR + Red)\nndvi = (nir_band - red_band) / (nir_band + red_band + 1e-8)  # Small value to avoid division by zero\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(nir_band, cmap='RdYlBu_r')\nplt.title('NIR-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 2)\nplt.imshow(red_band, cmap='Reds')\nplt.title('Red-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 3)\nplt.imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title('NDVI-like Index')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {np.mean(ndvi):.3f}\")\n\n\n\n\n\n\n\n\nNDVI range: -0.211 to 1.000\nNDVI mean: 0.353"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "title": "Working with Rasterio",
    "section": "Resampling and Reprojection Concepts",
    "text": "Resampling and Reprojection Concepts\n\nUnderstanding resampling methods\n\nfrom scipy import ndimage\n\n# Demonstrate different resampling methods\noriginal = sample_data\n\n# Downsample (reduce resolution)\ndownsampled = ndimage.zoom(original, 0.5, order=1)  # Linear interpolation\n\n# Upsample (increase resolution) \nupsampled = ndimage.zoom(original, 2.0, order=1)  # Linear interpolation\n\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(original, cmap='viridis')\nplt.title(f'Original ({original.shape[0]}x{original.shape[1]})')\n\nplt.subplot(1, 3, 2)\nplt.imshow(downsampled, cmap='viridis')\nplt.title(f'Downsampled ({downsampled.shape[0]}x{downsampled.shape[1]})')\n\nplt.subplot(1, 3, 3)\nplt.imshow(upsampled, cmap='viridis')\nplt.title(f'Upsampled ({upsampled.shape[0]}x{upsampled.shape[1]})')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original size: {original.shape}\")\nprint(f\"Downsampled size: {downsampled.shape}\")  \nprint(f\"Upsampled size: {upsampled.shape}\")\n\n\n\n\n\n\n\n\nOriginal size: (100, 100)\nDownsampled size: (50, 50)\nUpsampled size: (200, 200)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "title": "Working with Rasterio",
    "section": "Common Rasterio Patterns",
    "text": "Common Rasterio Patterns\n\nWorking with windows and blocks\n\n# Simulate processing large rasters in blocks\ndef process_in_blocks(data, block_size=50):\n    \"\"\"Process data in blocks to simulate handling large rasters\"\"\"\n    height, width = data.shape\n    processed = np.zeros_like(data)\n    \n    for row in range(0, height, block_size):\n        for col in range(0, width, block_size):\n            # Define window bounds\n            row_end = min(row + block_size, height)\n            col_end = min(col + block_size, width)\n            \n            # Extract block\n            block = data[row:row_end, col:col_end]\n            \n            # Process block (example: enhance contrast)\n            processed_block = np.clip(block * 1.2, 0, 255)\n            \n            # Write back to result\n            processed[row:row_end, col:col_end] = processed_block\n            \n            print(f\"Processed block: ({row}:{row_end}, {col}:{col_end})\")\n    \n    return processed\n\n# Process sample data in blocks\nprocessed_data = process_in_blocks(sample_data, block_size=25)\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original')\n\nplt.subplot(1, 2, 2)\nplt.imshow(processed_data, cmap='viridis')\nplt.title('Processed (Enhanced)')\n\nplt.tight_layout()\nplt.show()\n\nProcessed block: (0:25, 0:25)\nProcessed block: (0:25, 25:50)\nProcessed block: (0:25, 50:75)\nProcessed block: (0:25, 75:100)\nProcessed block: (25:50, 0:25)\nProcessed block: (25:50, 25:50)\nProcessed block: (25:50, 50:75)\nProcessed block: (25:50, 75:100)\nProcessed block: (50:75, 0:25)\nProcessed block: (50:75, 25:50)\nProcessed block: (50:75, 50:75)\nProcessed block: (50:75, 75:100)\nProcessed block: (75:100, 0:25)\nProcessed block: (75:100, 25:50)\nProcessed block: (75:100, 50:75)\nProcessed block: (75:100, 75:100)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#summary",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#summary",
    "title": "Working with Rasterio",
    "section": "Summary",
    "text": "Summary\nKey Rasterio concepts covered: - Reading and understanding raster data structure - Working with single and multi-band imagery - Coordinate transforms and geospatial metadata\n- Band math and index calculations - Resampling and processing techniques - Block-based processing for large datasets"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html",
    "title": "Multi-modal Learning",
    "section": "",
    "text": "Multi-modal learning combines different types of data (imagery, text, time series, etc.) to create more comprehensive and robust AI systems. In geospatial applications, this involves integrating satellite imagery with text descriptions, weather data, and other complementary information.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(f\"PyTorch version: {torch.__version__}\")\n\nPyTorch version: 2.7.1"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#introduction-to-multi-modal-learning",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#introduction-to-multi-modal-learning",
    "title": "Multi-modal Learning",
    "section": "",
    "text": "Multi-modal learning combines different types of data (imagery, text, time series, etc.) to create more comprehensive and robust AI systems. In geospatial applications, this involves integrating satellite imagery with text descriptions, weather data, and other complementary information.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(f\"PyTorch version: {torch.__version__}\")\n\nPyTorch version: 2.7.1"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#types-of-multi-modal-data-in-geospatial-ai",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#types-of-multi-modal-data-in-geospatial-ai",
    "title": "Multi-modal Learning",
    "section": "Types of Multi-modal Data in Geospatial AI",
    "text": "Types of Multi-modal Data in Geospatial AI\n\nCommon Modality Combinations\n\ndef demonstrate_multimodal_data_types():\n    \"\"\"Show different types of multi-modal combinations in geospatial AI\"\"\"\n    \n    modality_combinations = {\n        \"Image + Text\": {\n            \"example\": \"Satellite image + location description\",\n            \"use_cases\": [\"Image captioning\", \"Location search\", \"Content-based retrieval\"],\n            \"challenges\": [\"Semantic gap\", \"Text-image alignment\", \"Scale differences\"]\n        },\n        \"Multi-spectral + SAR\": {\n            \"example\": \"Optical + Radar imagery\", \n            \"use_cases\": [\"All-weather monitoring\", \"Improved classification\", \"Change detection\"],\n            \"challenges\": [\"Registration\", \"Resolution differences\", \"Fusion strategies\"]\n        },\n        \"Image + Time Series\": {\n            \"example\": \"Satellite imagery + weather/climate data\",\n            \"use_cases\": [\"Crop yield prediction\", \"Disaster monitoring\", \"Environmental modeling\"],\n            \"challenges\": [\"Temporal alignment\", \"Different sampling rates\", \"Multi-scale fusion\"]\n        },\n        \"Image + Tabular\": {\n            \"example\": \"Remote sensing + demographic/economic data\",\n            \"use_cases\": [\"Socioeconomic mapping\", \"Urban planning\", \"Poverty estimation\"],\n            \"challenges\": [\"Spatial alignment\", \"Feature engineering\", \"Scale mismatch\"]\n        },\n        \"Multi-resolution\": {\n            \"example\": \"High-res + Low-res imagery\",\n            \"use_cases\": [\"Super-resolution\", \"Multi-scale analysis\", \"Data fusion\"],\n            \"challenges\": [\"Resolution alignment\", \"Information preservation\", \"Computational efficiency\"]\n        }\n    }\n    \n    print(\"Multi-modal Data Types in Geospatial AI:\")\n    print(\"=\"*60)\n    \n    for modality, details in modality_combinations.items():\n        print(f\"\\n{modality}:\")\n        print(f\"  Example: {details['example']}\")\n        print(f\"  Use cases: {', '.join(details['use_cases'])}\")\n        print(f\"  Challenges: {', '.join(details['challenges'])}\")\n    \n    return modality_combinations\n\nmultimodal_types = demonstrate_multimodal_data_types()\n\nMulti-modal Data Types in Geospatial AI:\n============================================================\n\nImage + Text:\n  Example: Satellite image + location description\n  Use cases: Image captioning, Location search, Content-based retrieval\n  Challenges: Semantic gap, Text-image alignment, Scale differences\n\nMulti-spectral + SAR:\n  Example: Optical + Radar imagery\n  Use cases: All-weather monitoring, Improved classification, Change detection\n  Challenges: Registration, Resolution differences, Fusion strategies\n\nImage + Time Series:\n  Example: Satellite imagery + weather/climate data\n  Use cases: Crop yield prediction, Disaster monitoring, Environmental modeling\n  Challenges: Temporal alignment, Different sampling rates, Multi-scale fusion\n\nImage + Tabular:\n  Example: Remote sensing + demographic/economic data\n  Use cases: Socioeconomic mapping, Urban planning, Poverty estimation\n  Challenges: Spatial alignment, Feature engineering, Scale mismatch\n\nMulti-resolution:\n  Example: High-res + Low-res imagery\n  Use cases: Super-resolution, Multi-scale analysis, Data fusion\n  Challenges: Resolution alignment, Information preservation, Computational efficiency\n\n\n\n\nData Preprocessing for Multi-modal Learning\n\ndef create_multimodal_preprocessing_pipeline():\n    \"\"\"Demonstrate preprocessing for multi-modal geospatial data\"\"\"\n    \n    # Simulate different data modalities\n    np.random.seed(42)\n    \n    # 1. Satellite imagery (multispectral)\n    batch_size, channels, height, width = 4, 6, 224, 224\n    satellite_images = torch.randn(batch_size, channels, height, width)\n    \n    # 2. Text descriptions\n    text_descriptions = [\n        \"Forest area with dense vegetation and high canopy cover\",\n        \"Urban residential area with mixed building types\",\n        \"Agricultural land with crop fields and irrigation\",\n        \"Coastal wetland area with water bodies and marsh\"\n    ]\n    \n    # 3. Tabular metadata\n    metadata = pd.DataFrame({\n        'location_id': [f'LOC_{i:03d}' for i in range(batch_size)],\n        'latitude': [45.5 + i*0.1 for i in range(batch_size)],\n        'longitude': [-122.5 + i*0.1 for i in range(batch_size)],\n        'elevation': [100 + i*50 for i in range(batch_size)],\n        'temperature': [15.5 + i*2 for i in range(batch_size)],\n        'precipitation': [800 + i*100 for i in range(batch_size)],\n        'season': ['spring', 'summer', 'autumn', 'winter']\n    })\n    \n    # 4. Time series data\n    time_steps = 52  # Weekly data for a year\n    time_series = torch.randn(batch_size, time_steps, 3)  # NDVI, temperature, precipitation\n    \n    print(\"Multi-modal Data Examples:\")\n    print(\"=\"*40)\n    print(f\"Satellite images shape: {satellite_images.shape}\")\n    print(f\"Text descriptions: {len(text_descriptions)} samples\")\n    print(f\"Metadata shape: {metadata.shape}\")\n    print(f\"Time series shape: {time_series.shape}\")\n    \n    # Preprocessing functions\n    def preprocess_images(images, target_size=(224, 224)):\n        \"\"\"Preprocess satellite images\"\"\"\n        # Normalize to [0, 1]\n        images = (images - images.min()) / (images.max() - images.min())\n        \n        # Resize if needed (simplified)\n        if images.shape[-2:] != target_size:\n            images = F.interpolate(images, size=target_size, mode='bilinear', align_corners=False)\n        \n        return images\n    \n    def preprocess_text(texts, max_length=77):\n        \"\"\"Preprocess text descriptions (simplified tokenization)\"\"\"\n        # In practice, use proper tokenizers like CLIP or BERT\n        processed_texts = []\n        for text in texts:\n            # Simple word tokenization\n            words = text.lower().split()[:max_length]\n            # Pad to max_length\n            words += ['&lt;pad&gt;'] * (max_length - len(words))\n            processed_texts.append(words)\n        \n        return processed_texts\n    \n    def preprocess_tabular(metadata):\n        \"\"\"Preprocess tabular metadata\"\"\"\n        processed = metadata.copy()\n        \n        # Normalize numerical features\n        numerical_cols = ['latitude', 'longitude', 'elevation', 'temperature', 'precipitation']\n        for col in numerical_cols:\n            processed[col] = (processed[col] - processed[col].mean()) / processed[col].std()\n        \n        # Encode categorical features (simplified)\n        season_encoding = {'spring': 0, 'summer': 1, 'autumn': 2, 'winter': 3}\n        processed['season_encoded'] = processed['season'].map(season_encoding)\n        \n        return processed\n    \n    def preprocess_time_series(ts_data, normalize=True):\n        \"\"\"Preprocess time series data\"\"\"\n        if normalize:\n            # Normalize across time dimension\n            mean = ts_data.mean(dim=1, keepdim=True)\n            std = ts_data.std(dim=1, keepdim=True)\n            ts_data = (ts_data - mean) / (std + 1e-8)\n        \n        return ts_data\n    \n    # Apply preprocessing\n    processed_images = preprocess_images(satellite_images)\n    processed_texts = preprocess_text(text_descriptions)\n    processed_metadata = preprocess_tabular(metadata)\n    processed_time_series = preprocess_time_series(time_series)\n    \n    print(\"\\nAfter Preprocessing:\")\n    print(f\"Images range: [{processed_images.min():.3f}, {processed_images.max():.3f}]\")\n    print(f\"Text tokens per sample: {len(processed_texts[0])}\")\n    print(\"Metadata columns:\", list(processed_metadata.columns))\n    print(f\"Time series normalized: mean={processed_time_series.mean():.3f}, std={processed_time_series.std():.3f}\")\n    \n    return {\n        'images': processed_images,\n        'texts': processed_texts,\n        'metadata': processed_metadata,\n        'time_series': processed_time_series\n    }\n\npreprocessed_data = create_multimodal_preprocessing_pipeline()\n\nMulti-modal Data Examples:\n========================================\nSatellite images shape: torch.Size([4, 6, 224, 224])\nText descriptions: 4 samples\nMetadata shape: (4, 7)\nTime series shape: torch.Size([4, 52, 3])\n\nAfter Preprocessing:\nImages range: [0.000, 1.000]\nText tokens per sample: 77\nMetadata columns: ['location_id', 'latitude', 'longitude', 'elevation', 'temperature', 'precipitation', 'season', 'season_encoded']\nTime series normalized: mean=-0.000, std=0.991"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#multi-modal-architecture-patterns",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#multi-modal-architecture-patterns",
    "title": "Multi-modal Learning",
    "section": "Multi-modal Architecture Patterns",
    "text": "Multi-modal Architecture Patterns\n\nEarly Fusion vs Late Fusion\n\nclass EarlyFusionModel(nn.Module):\n    \"\"\"Early fusion: combine features at input level\"\"\"\n    \n    def __init__(self, image_channels=6, text_vocab_size=1000, tabular_features=5, hidden_dim=512, num_classes=10):\n        super().__init__()\n        \n        # Image encoder\n        self.image_encoder = nn.Sequential(\n            nn.Conv2d(image_channels, 64, 7, stride=2, padding=3),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((4, 4)),\n            nn.Flatten(),\n            nn.Linear(64 * 16, hidden_dim)\n        )\n        \n        # Text encoder (simplified)\n        self.text_encoder = nn.Sequential(\n            nn.Embedding(text_vocab_size, 256),\n            nn.LSTM(256, hidden_dim//2, batch_first=True),\n        )\n        \n        # Tabular encoder\n        self.tabular_encoder = nn.Sequential(\n            nn.Linear(tabular_features, hidden_dim//2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim//2, hidden_dim)\n        )\n        \n        # Early fusion: concatenate features\n        # Image (hidden_dim) + Text (hidden_dim//2) + Tabular (hidden_dim)\n        fusion_input_dim = hidden_dim + hidden_dim//2 + hidden_dim\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(fusion_input_dim, hidden_dim),  # Combined features\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, num_classes)\n        )\n    \n    def forward(self, images, text_tokens, tabular_data):\n        # Encode each modality\n        image_features = self.image_encoder(images)\n        \n        # Text encoding (simplified - use last hidden state)\n        text_features, (h_n, c_n) = self.text_encoder(text_tokens)\n        text_features = h_n[-1]  # Use last hidden state\n        \n        tabular_features = self.tabular_encoder(tabular_data)\n        \n        # Early fusion: concatenate features\n        combined_features = torch.cat([image_features, text_features, tabular_features], dim=1)\n        \n        # Final prediction\n        output = self.fusion_layer(combined_features)\n        \n        return output, {\n            'image_features': image_features,\n            'text_features': text_features,\n            'tabular_features': tabular_features,\n            'combined_features': combined_features\n        }\n\nclass LateFusionModel(nn.Module):\n    \"\"\"Late fusion: combine predictions from separate models\"\"\"\n    \n    def __init__(self, image_channels=6, text_vocab_size=1000, tabular_features=5, hidden_dim=512, num_classes=10):\n        super().__init__()\n        \n        # Separate encoders for each modality\n        self.image_branch = nn.Sequential(\n            nn.Conv2d(image_channels, 64, 7, stride=2, padding=3),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((4, 4)),\n            nn.Flatten(),\n            nn.Linear(64 * 16, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes)\n        )\n        \n        self.text_branch = nn.Sequential(\n            nn.Embedding(text_vocab_size, 256),\n            nn.LSTM(256, hidden_dim//2, batch_first=True),\n        )\n        self.text_classifier = nn.Linear(hidden_dim//2, num_classes)\n        \n        self.tabular_branch = nn.Sequential(\n            nn.Linear(tabular_features, hidden_dim//2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim//2, num_classes)\n        )\n        \n        # Fusion weights (learnable)\n        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)\n        \n    def forward(self, images, text_tokens, tabular_data):\n        # Get predictions from each branch\n        image_logits = self.image_branch(images)\n        \n        text_features, (h_n, c_n) = self.text_branch(text_tokens)\n        text_logits = self.text_classifier(h_n[-1])\n        \n        tabular_logits = self.tabular_branch(tabular_data)\n        \n        # Late fusion: weighted combination of predictions\n        fusion_weights = F.softmax(self.fusion_weights, dim=0)\n        combined_logits = (fusion_weights[0] * image_logits + \n                          fusion_weights[1] * text_logits + \n                          fusion_weights[2] * tabular_logits)\n        \n        return combined_logits, {\n            'image_logits': image_logits,\n            'text_logits': text_logits,\n            'tabular_logits': tabular_logits,\n            'fusion_weights': fusion_weights\n        }\n\n# Compare architectures\ndef compare_fusion_architectures():\n    \"\"\"Compare early vs late fusion approaches\"\"\"\n    \n    # Create sample data\n    batch_size = 4\n    images = torch.randn(batch_size, 6, 224, 224)\n    text_tokens = torch.randint(0, 1000, (batch_size, 20))\n    tabular_data = torch.randn(batch_size, 5)\n    \n    # Create models\n    early_fusion = EarlyFusionModel()\n    late_fusion = LateFusionModel()\n    \n    # Count parameters\n    early_params = sum(p.numel() for p in early_fusion.parameters())\n    late_params = sum(p.numel() for p in late_fusion.parameters())\n    \n    print(\"Fusion Architecture Comparison:\")\n    print(\"=\"*50)\n    print(f\"Early Fusion Parameters: {early_params:,}\")\n    print(f\"Late Fusion Parameters: {late_params:,}\")\n    \n    # Forward pass\n    with torch.no_grad():\n        early_output, early_features = early_fusion(images, text_tokens, tabular_data)\n        late_output, late_features = late_fusion(images, text_tokens, tabular_data)\n    \n    print(f\"\\nOutput shapes:\")\n    print(f\"Early Fusion: {early_output.shape}\")\n    print(f\"Late Fusion: {late_output.shape}\")\n    \n    print(f\"\\nLate Fusion Weights: {late_features['fusion_weights']}\")\n    \n    return early_fusion, late_fusion\n\nearly_model, late_model = compare_fusion_architectures()\n\nFusion Architecture Comparison:\n==================================================\nEarly Fusion Parameters: 2,120,138\nLate Fusion Parameters: 1,337,825\n\nOutput shapes:\nEarly Fusion: torch.Size([4, 10])\nLate Fusion: torch.Size([4, 10])\n\nLate Fusion Weights: tensor([0.3333, 0.3333, 0.3333])\n\n\n\n\nAttention-based Fusion\n\nclass CrossModalAttentionFusion(nn.Module):\n    \"\"\"Cross-modal attention fusion mechanism\"\"\"\n    \n    def __init__(self, feature_dim=512, num_heads=8):\n        super().__init__()\n        self.feature_dim = feature_dim\n        self.num_heads = num_heads\n        \n        # Cross-attention layers\n        self.image_to_text_attention = nn.MultiheadAttention(feature_dim, num_heads, batch_first=True)\n        self.text_to_image_attention = nn.MultiheadAttention(feature_dim, num_heads, batch_first=True)\n        \n        # Self-attention for final fusion\n        self.fusion_attention = nn.MultiheadAttention(feature_dim, num_heads, batch_first=True)\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(feature_dim)\n        self.norm2 = nn.LayerNorm(feature_dim)\n        self.norm3 = nn.LayerNorm(feature_dim)\n        \n        # Final classifier\n        self.classifier = nn.Linear(feature_dim * 2, 10)  # 10 classes\n    \n    def forward(self, image_features, text_features):\n        \"\"\"\n        image_features: [batch_size, num_patches, feature_dim]\n        text_features: [batch_size, seq_len, feature_dim]\n        \"\"\"\n        \n        # Cross-modal attention: image attending to text\n        image_attended, image_attention_weights = self.image_to_text_attention(\n            image_features, text_features, text_features\n        )\n        image_attended = self.norm1(image_features + image_attended)\n        \n        # Cross-modal attention: text attending to image\n        text_attended, text_attention_weights = self.text_to_image_attention(\n            text_features, image_features, image_features\n        )\n        text_attended = self.norm2(text_features + text_attended)\n        \n        # Global pooling\n        image_global = image_attended.mean(dim=1)  # [batch_size, feature_dim]\n        text_global = text_attended.mean(dim=1)    # [batch_size, feature_dim]\n        \n        # Concatenate and classify\n        combined = torch.cat([image_global, text_global], dim=1)\n        output = self.classifier(combined)\n        \n        return output, {\n            'image_attention_weights': image_attention_weights,\n            'text_attention_weights': text_attention_weights,\n            'image_global': image_global,\n            'text_global': text_global\n        }\n\n# Demonstrate cross-modal attention\ndef demonstrate_cross_modal_attention():\n    \"\"\"Show cross-modal attention mechanism\"\"\"\n    \n    batch_size, feature_dim = 4, 512\n    num_image_patches, seq_len = 16, 10\n    \n    # Create sample features\n    image_features = torch.randn(batch_size, num_image_patches, feature_dim)\n    text_features = torch.randn(batch_size, seq_len, feature_dim)\n    \n    # Create attention model\n    attention_fusion = CrossModalAttentionFusion(feature_dim=feature_dim)\n    \n    # Forward pass\n    with torch.no_grad():\n        output, attention_info = attention_fusion(image_features, text_features)\n    \n    print(\"Cross-modal Attention Results:\")\n    print(\"=\"*40)\n    print(f\"Input shapes:\")\n    print(f\"  Image features: {image_features.shape}\")\n    print(f\"  Text features: {text_features.shape}\")\n    print(f\"Output shape: {output.shape}\")\n    \n    # Visualize attention weights\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Image-to-text attention (first sample)\n    img_to_text_attn = attention_info['image_attention_weights'][0].detach().numpy()\n    im1 = axes[0].imshow(img_to_text_attn, cmap='Blues', aspect='auto')\n    axes[0].set_title('Image-to-Text Attention')\n    axes[0].set_xlabel('Text Positions')\n    axes[0].set_ylabel('Image Patches')\n    plt.colorbar(im1, ax=axes[0])\n    \n    # Text-to-image attention (first sample)\n    text_to_img_attn = attention_info['text_attention_weights'][0].detach().numpy()\n    im2 = axes[1].imshow(text_to_img_attn, cmap='Reds', aspect='auto')\n    axes[1].set_title('Text-to-Image Attention')\n    axes[1].set_xlabel('Image Patches')\n    axes[1].set_ylabel('Text Positions')\n    plt.colorbar(im2, ax=axes[1])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return attention_fusion\n\nattention_model = demonstrate_cross_modal_attention()\n\nCross-modal Attention Results:\n========================================\nInput shapes:\n  Image features: torch.Size([4, 16, 512])\n  Text features: torch.Size([4, 10, 512])\nOutput shape: torch.Size([4, 10])"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#contrastive-learning-for-multi-modal-data",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#contrastive-learning-for-multi-modal-data",
    "title": "Multi-modal Learning",
    "section": "Contrastive Learning for Multi-modal Data",
    "text": "Contrastive Learning for Multi-modal Data\n\nCLIP-style Contrastive Learning\n\nclass ContrastiveLearningModel(nn.Module):\n    \"\"\"CLIP-style contrastive learning for image-text pairs\"\"\"\n    \n    def __init__(self, image_encoder_dim=2048, text_encoder_dim=768, projection_dim=512):\n        super().__init__()\n        \n        # Simplified image encoder\n        self.image_encoder = nn.Sequential(\n            nn.Conv2d(6, 64, 7, stride=2, padding=3),  # 6 channels for multispectral\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, image_encoder_dim)\n        )\n        \n        # Simplified text encoder\n        self.text_encoder = nn.Sequential(\n            nn.Embedding(10000, 256),  # Vocab size 10000\n            nn.LSTM(256, text_encoder_dim//2, batch_first=True, bidirectional=True),\n        )\n        \n        # Projection heads\n        self.image_projection = nn.Sequential(\n            nn.Linear(image_encoder_dim, projection_dim),\n            nn.ReLU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n        \n        self.text_projection = nn.Sequential(\n            nn.Linear(text_encoder_dim, projection_dim),\n            nn.ReLU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n        \n        # Temperature parameter for contrastive loss\n        self.temperature = nn.Parameter(torch.tensor(0.07))\n    \n    def forward(self, images, text_tokens):\n        # Encode images\n        image_features = self.image_encoder(images)\n        image_embeddings = self.image_projection(image_features)\n        \n        # Encode text\n        text_features, (h_n, c_n) = self.text_encoder(text_tokens)\n        # Use final hidden states from both directions\n        text_features = torch.cat([h_n[-2], h_n[-1]], dim=1)  # Concatenate bidirectional\n        text_embeddings = self.text_projection(text_features)\n        \n        # Normalize embeddings\n        image_embeddings = F.normalize(image_embeddings, dim=1)\n        text_embeddings = F.normalize(text_embeddings, dim=1)\n        \n        return image_embeddings, text_embeddings\n    \n    def contrastive_loss(self, image_embeddings, text_embeddings):\n        \"\"\"Calculate contrastive loss between image and text embeddings\"\"\"\n        \n        batch_size = image_embeddings.shape[0]\n        \n        # Calculate similarity matrix\n        similarity_matrix = torch.matmul(image_embeddings, text_embeddings.T) / self.temperature\n        \n        # Create labels (diagonal should be positive pairs)\n        labels = torch.arange(batch_size, device=image_embeddings.device)\n        \n        # Contrastive loss (symmetric)\n        loss_img_to_text = F.cross_entropy(similarity_matrix, labels)\n        loss_text_to_img = F.cross_entropy(similarity_matrix.T, labels)\n        \n        total_loss = (loss_img_to_text + loss_text_to_img) / 2\n        \n        return total_loss, similarity_matrix\n\ndef demonstrate_contrastive_learning():\n    \"\"\"Demonstrate contrastive learning training\"\"\"\n    \n    # Create model\n    contrastive_model = ContrastiveLearningModel()\n    \n    # Sample data\n    batch_size = 8\n    images = torch.randn(batch_size, 6, 224, 224)\n    text_tokens = torch.randint(0, 10000, (batch_size, 20))\n    \n    # Forward pass\n    image_embeddings, text_embeddings = contrastive_model(images, text_tokens)\n    \n    # Calculate loss\n    loss, similarity_matrix = contrastive_model.contrastive_loss(image_embeddings, text_embeddings)\n    \n    print(\"Contrastive Learning Results:\")\n    print(\"=\"*40)\n    print(f\"Image embeddings shape: {image_embeddings.shape}\")\n    print(f\"Text embeddings shape: {text_embeddings.shape}\")\n    print(f\"Contrastive loss: {loss.item():.4f}\")\n    print(f\"Temperature: {contrastive_model.temperature.item():.4f}\")\n    \n    # Visualize similarity matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(similarity_matrix.detach().numpy(), cmap='RdBu_r', aspect='equal')\n    plt.colorbar(label='Similarity Score')\n    plt.title('Image-Text Similarity Matrix')\n    plt.xlabel('Text Samples')\n    plt.ylabel('Image Samples')\n    \n    # Highlight diagonal (positive pairs)\n    for i in range(batch_size):\n        plt.scatter(i, i, marker='x', s=100, color='white', linewidth=2)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Show top-k retrievals\n    def show_retrievals(similarity_matrix, k=3):\n        \"\"\"Show top-k text retrievals for each image\"\"\"\n        \n        print(f\"\\nTop-{k} Text Retrievals for Each Image:\")\n        print(\"-\" * 40)\n        \n        for img_idx in range(batch_size):\n            similarities = similarity_matrix[img_idx]\n            top_k_indices = similarities.topk(k).indices\n            \n            print(f\"Image {img_idx}: Text indices {top_k_indices.tolist()}\")\n            print(f\"  Similarities: {similarities[top_k_indices].tolist()}\")\n    \n    show_retrievals(similarity_matrix)\n    \n    return contrastive_model\n\ncontrastive_model = demonstrate_contrastive_learning()\n\nContrastive Learning Results:\n========================================\nImage embeddings shape: torch.Size([8, 512])\nText embeddings shape: torch.Size([8, 512])\nContrastive loss: 2.1829\nTemperature: 0.0700\n\n\n\n\n\n\n\n\n\n\nTop-3 Text Retrievals for Each Image:\n----------------------------------------\nImage 0: Text indices [3, 7, 4]\n  Similarities: [1.0794861316680908, 0.9029110670089722, 0.8195472955703735]\nImage 1: Text indices [3, 7, 4]\n  Similarities: [1.0854287147521973, 0.9096492528915405, 0.8233649134635925]\nImage 2: Text indices [3, 7, 4]\n  Similarities: [1.0865939855575562, 0.9108825325965881, 0.8267571926116943]\nImage 3: Text indices [3, 7, 4]\n  Similarities: [1.0879254341125488, 0.9082749485969543, 0.8299235701560974]\nImage 4: Text indices [3, 7, 4]\n  Similarities: [1.087508201599121, 0.9112916588783264, 0.8248893618583679]\nImage 5: Text indices [3, 7, 4]\n  Similarities: [1.0850661993026733, 0.9036687016487122, 0.8178841471672058]\nImage 6: Text indices [3, 7, 4]\n  Similarities: [1.0847316980361938, 0.9007928371429443, 0.8177998661994934]\nImage 7: Text indices [3, 7, 4]\n  Similarities: [1.0784430503845215, 0.8971286416053772, 0.8165452480316162]\n\n\n\n\nZero-Shot Classification\n\ndef demonstrate_zero_shot_classification():\n    \"\"\"Show zero-shot classification using learned embeddings\"\"\"\n    \n    # Simulate a trained contrastive model\n    model = contrastive_model  # Use the model from previous example\n    model.eval()\n    \n    # Define text prompts for different land cover classes\n    class_descriptions = {\n        'forest': \"Dense forest area with trees and vegetation\",\n        'urban': \"Urban area with buildings and infrastructure\", \n        'water': \"Water body such as lake or river\",\n        'agriculture': \"Agricultural land with crops and farming\",\n        'desert': \"Desert area with sand and minimal vegetation\",\n        'grassland': \"Grassland area with grass and open space\"\n    }\n    \n    # Convert descriptions to tokens (simplified)\n    def simple_tokenize(text, max_length=20):\n        \"\"\"Simple tokenization for demonstration\"\"\"\n        words = text.lower().split()[:max_length]\n        # Map words to random token IDs for demo\n        np.random.seed(hash(text) % 1000)  # Consistent random mapping\n        tokens = [np.random.randint(0, 10000) for _ in words]\n        # Pad to max_length\n        tokens += [0] * (max_length - len(tokens))\n        return torch.tensor(tokens[:max_length])\n    \n    # Create text embeddings for each class\n    class_embeddings = {}\n    with torch.no_grad():\n        for class_name, description in class_descriptions.items():\n            tokens = simple_tokenize(description).unsqueeze(0)\n            _, text_embedding = model(torch.zeros(1, 6, 224, 224), tokens)\n            class_embeddings[class_name] = text_embedding.squeeze(0)\n    \n    # Test images (simulate different land covers)\n    test_images = torch.randn(6, 6, 224, 224)  # 6 test images\n    \n    with torch.no_grad():\n        test_image_embeddings, _ = model(test_images, torch.zeros(6, 20, dtype=torch.long))\n    \n    # Calculate similarities and classify\n    predictions = []\n    similarities_all = []\n    \n    for i, img_embedding in enumerate(test_image_embeddings):\n        similarities = {}\n        for class_name, class_embedding in class_embeddings.items():\n            similarity = F.cosine_similarity(img_embedding, class_embedding, dim=0)\n            similarities[class_name] = similarity.item()\n        \n        # Get predicted class\n        predicted_class = max(similarities, key=similarities.get)\n        predictions.append(predicted_class)\n        similarities_all.append(similarities)\n    \n    # Display results\n    print(\"Zero-Shot Classification Results:\")\n    print(\"=\"*50)\n    \n    class_names = list(class_descriptions.keys())\n    similarity_matrix = np.zeros((len(test_images), len(class_names)))\n    \n    for i, similarities in enumerate(similarities_all):\n        print(f\"\\nTest Image {i}: Predicted as '{predictions[i]}'\")\n        for j, class_name in enumerate(class_names):\n            similarity_matrix[i, j] = similarities[class_name]\n            print(f\"  {class_name}: {similarities[class_name]:.3f}\")\n    \n    # Visualize similarity heatmap\n    plt.figure(figsize=(10, 6))\n    plt.imshow(similarity_matrix, cmap='RdYlBu_r', aspect='auto')\n    plt.colorbar(label='Cosine Similarity')\n    plt.xlabel('Classes')\n    plt.ylabel('Test Images')\n    plt.xticks(range(len(class_names)), class_names, rotation=45)\n    plt.yticks(range(len(test_images)), [f'Image {i}' for i in range(len(test_images))])\n    plt.title('Zero-Shot Classification Similarities')\n    \n    # Add prediction markers\n    for i, pred_class in enumerate(predictions):\n        j = class_names.index(pred_class)\n        plt.scatter(j, i, marker='x', s=200, color='white', linewidth=3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return predictions, similarities_all\n\nzeroshot_predictions, zeroshot_similarities = demonstrate_zero_shot_classification()\n\nZero-Shot Classification Results:\n==================================================\n\nTest Image 0: Predicted as 'agriculture'\n  forest: -0.006\n  urban: -0.027\n  water: -0.008\n  agriculture: 0.010\n  desert: -0.025\n  grassland: -0.011\n\nTest Image 1: Predicted as 'agriculture'\n  forest: -0.006\n  urban: -0.026\n  water: -0.008\n  agriculture: 0.010\n  desert: -0.024\n  grassland: -0.011\n\nTest Image 2: Predicted as 'agriculture'\n  forest: -0.006\n  urban: -0.026\n  water: -0.008\n  agriculture: 0.010\n  desert: -0.025\n  grassland: -0.011\n\nTest Image 3: Predicted as 'agriculture'\n  forest: -0.006\n  urban: -0.026\n  water: -0.008\n  agriculture: 0.010\n  desert: -0.025\n  grassland: -0.011\n\nTest Image 4: Predicted as 'agriculture'\n  forest: -0.006\n  urban: -0.026\n  water: -0.007\n  agriculture: 0.011\n  desert: -0.024\n  grassland: -0.010\n\nTest Image 5: Predicted as 'agriculture'\n  forest: -0.006\n  urban: -0.026\n  water: -0.008\n  agriculture: 0.010\n  desert: -0.025\n  grassland: -0.011"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#multi-modal-data-augmentation",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#multi-modal-data-augmentation",
    "title": "Multi-modal Learning",
    "section": "Multi-modal Data Augmentation",
    "text": "Multi-modal Data Augmentation\n\nCross-modal Data Augmentation\n\ndef demonstrate_multimodal_augmentation():\n    \"\"\"Show augmentation techniques for multi-modal data\"\"\"\n    \n    # Original data\n    batch_size = 4\n    original_images = torch.randn(batch_size, 6, 224, 224)\n    original_texts = [\n        \"Forest area with dense canopy\",\n        \"Urban residential district\", \n        \"Agricultural crop fields\",\n        \"Coastal wetland ecosystem\"\n    ]\n    \n    class MultiModalAugmentation:\n        \"\"\"Multi-modal data augmentation techniques\"\"\"\n        \n        def __init__(self):\n            self.augmentation_strategies = [\n                'spatial_crop',\n                'spectral_shift',\n                'text_synonym',\n                'mixup',\n                'cutmix'\n            ]\n        \n        def spatial_crop(self, images, texts, crop_ratio=0.8):\n            \"\"\"Spatial cropping with corresponding text modification\"\"\"\n            \n            _, _, h, w = images.shape\n            crop_h, crop_w = int(h * crop_ratio), int(w * crop_ratio)\n            \n            # Random crop position\n            start_h = torch.randint(0, h - crop_h + 1, (1,)).item()\n            start_w = torch.randint(0, w - crop_w + 1, (1,)).item()\n            \n            # Crop images\n            cropped_images = images[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w]\n            \n            # Resize back to original size\n            cropped_images = F.interpolate(cropped_images, size=(h, w), mode='bilinear')\n            \n            # Modify texts to indicate cropping\n            modified_texts = [f\"Cropped view of {text.lower()}\" for text in texts]\n            \n            return cropped_images, modified_texts\n        \n        def spectral_shift(self, images, texts, shift_factor=0.1):\n            \"\"\"Spectral band shifting\"\"\"\n            \n            # Randomly shift spectral bands\n            shifted_images = images.clone()\n            for i in range(images.shape[1]):  # For each spectral band\n                shift = torch.normal(0, shift_factor, size=(1,)).item()\n                shifted_images[:, i] = images[:, i] + shift\n            \n            # Clamp to valid range\n            shifted_images = torch.clamp(shifted_images, -3, 3)  # Assuming normalized data\n            \n            # Modify texts to indicate spectral variation\n            modified_texts = [f\"{text} with spectral variation\" for text in texts]\n            \n            return shifted_images, modified_texts\n        \n        def text_synonym_replacement(self, texts, replacement_prob=0.3):\n            \"\"\"Replace words with synonyms\"\"\"\n            \n            # Simple synonym dictionary\n            synonyms = {\n                'forest': ['woodland', 'trees', 'vegetation'],\n                'urban': ['city', 'metropolitan', 'developed'],\n                'agricultural': ['farming', 'crop', 'cultivation'],\n                'area': ['region', 'zone', 'location'],\n                'dense': ['thick', 'concentrated', 'heavy']\n            }\n            \n            modified_texts = []\n            for text in texts:\n                words = text.split()\n                new_words = []\n                \n                for word in words:\n                    word_lower = word.lower()\n                    if word_lower in synonyms and torch.rand(1).item() &lt; replacement_prob:\n                        synonym = np.random.choice(synonyms[word_lower])\n                        new_words.append(synonym)\n                    else:\n                        new_words.append(word)\n                \n                modified_texts.append(' '.join(new_words))\n            \n            return modified_texts\n        \n        def mixup_multimodal(self, images, texts, alpha=0.4):\n            \"\"\"MixUp augmentation for multi-modal data\"\"\"\n            \n            if len(images) &lt; 2:\n                return images, texts\n            \n            # Generate mixing weights\n            lam = np.random.beta(alpha, alpha)\n            \n            # Shuffle indices for mixing\n            batch_size = images.shape[0]\n            indices = torch.randperm(batch_size)\n            \n            # Mix images\n            mixed_images = lam * images + (1 - lam) * images[indices]\n            \n            # Mix texts (concatenate with mixing indicator)\n            mixed_texts = []\n            for i in range(len(texts)):\n                mixed_texts.append(f\"Mixed scene: {lam:.2f} * ({texts[i]}) + {1-lam:.2f} * ({texts[indices[i]]})\")\n            \n            return mixed_images, mixed_texts\n        \n        def cutmix_multimodal(self, images, texts, alpha=1.0):\n            \"\"\"CutMix augmentation for multi-modal data\"\"\"\n            \n            if len(images) &lt; 2:\n                return images, texts\n            \n            lam = np.random.beta(alpha, alpha)\n            batch_size = images.shape[0]\n            indices = torch.randperm(batch_size)\n            \n            _, _, h, w = images.shape\n            \n            # Generate random bounding box\n            cut_rat = np.sqrt(1. - lam)\n            cut_w = int(w * cut_rat)\n            cut_h = int(h * cut_rat)\n            \n            cx = np.random.randint(w)\n            cy = np.random.randint(h)\n            \n            bbx1 = np.clip(cx - cut_w // 2, 0, w)\n            bby1 = np.clip(cy - cut_h // 2, 0, h)\n            bbx2 = np.clip(cx + cut_w // 2, 0, w)\n            bby2 = np.clip(cy + cut_h // 2, 0, h)\n            \n            # Apply cutmix\n            mixed_images = images.clone()\n            mixed_images[:, :, bby1:bby2, bbx1:bbx2] = images[indices, :, bby1:bby2, bbx1:bbx2]\n            \n            # Mix texts\n            mixed_texts = []\n            for i in range(len(texts)):\n                mixed_texts.append(f\"Scene with cutmix: {texts[i]} + patch from {texts[indices[i]]}\")\n            \n            return mixed_images, mixed_texts\n    \n    # Demonstrate augmentations\n    augmenter = MultiModalAugmentation()\n    \n    print(\"Multi-modal Data Augmentation Examples:\")\n    print(\"=\"*60)\n    \n    # Original\n    print(\"Original texts:\")\n    for i, text in enumerate(original_texts):\n        print(f\"  {i}: {text}\")\n    \n    # Spatial crop\n    cropped_imgs, cropped_texts = augmenter.spatial_crop(original_images, original_texts)\n    print(f\"\\nSpatial Crop:\")\n    print(f\"  Image shape change: {original_images.shape} -&gt; {cropped_imgs.shape}\")\n    for i, text in enumerate(cropped_texts[:2]):  # Show first 2\n        print(f\"  {i}: {text}\")\n    \n    # Spectral shift\n    shifted_imgs, shifted_texts = augmenter.spectral_shift(original_images, original_texts)\n    print(f\"\\nSpectral Shift:\")\n    print(f\"  Value range change: [{original_images.min():.2f}, {original_images.max():.2f}] -&gt; [{shifted_imgs.min():.2f}, {shifted_imgs.max():.2f}]\")\n    \n    # Text synonym replacement\n    synonym_texts = augmenter.text_synonym_replacement(original_texts)\n    print(f\"\\nSynonym Replacement:\")\n    for i, (orig, syn) in enumerate(zip(original_texts[:2], synonym_texts[:2])):\n        print(f\"  {i}: '{orig}' -&gt; '{syn}'\")\n    \n    # MixUp\n    mixup_imgs, mixup_texts = augmenter.mixup_multimodal(original_images, original_texts)\n    print(f\"\\nMixUp:\")\n    print(f\"  Example: {mixup_texts[0]}\")\n    \n    # CutMix\n    cutmix_imgs, cutmix_texts = augmenter.cutmix_multimodal(original_images, original_texts)\n    print(f\"\\nCutMix:\")\n    print(f\"  Example: {cutmix_texts[0]}\")\n    \n    # Visualize augmentation effects\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    \n    def visualize_image(img_tensor, ax, title):\n        \"\"\"Visualize first 3 channels as RGB\"\"\"\n        img_rgb = img_tensor[0, :3].detach().numpy().transpose(1, 2, 0)\n        img_rgb = (img_rgb - img_rgb.min()) / (img_rgb.max() - img_rgb.min())\n        ax.imshow(img_rgb)\n        ax.set_title(title)\n        ax.axis('off')\n    \n    # Original and augmented images\n    augmented_images = [\n        (original_images, \"Original\"),\n        (cropped_imgs, \"Spatial Crop\"),\n        (shifted_imgs, \"Spectral Shift\"),\n        (mixup_imgs, \"MixUp\"),\n        (cutmix_imgs, \"CutMix\")\n    ]\n    \n    for i, (imgs, title) in enumerate(augmented_images[:6]):\n        row, col = i // 3, i % 3\n        if row &lt; 2:\n            visualize_image(imgs, axes[row, col], title)\n    \n    # Hide unused subplot\n    if len(augmented_images) &lt; 6:\n        axes[1, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return augmenter\n\naugmenter = demonstrate_multimodal_augmentation()\n\nMulti-modal Data Augmentation Examples:\n============================================================\nOriginal texts:\n  0: Forest area with dense canopy\n  1: Urban residential district\n  2: Agricultural crop fields\n  3: Coastal wetland ecosystem\n\nSpatial Crop:\n  Image shape change: torch.Size([4, 6, 224, 224]) -&gt; torch.Size([4, 6, 224, 224])\n  0: Cropped view of forest area with dense canopy\n  1: Cropped view of urban residential district\n\nSpectral Shift:\n  Value range change: [-5.12, 4.78] -&gt; [-3.00, 3.00]\n\nSynonym Replacement:\n  0: 'Forest area with dense canopy' -&gt; 'Forest region with heavy canopy'\n  1: 'Urban residential district' -&gt; 'Urban residential district'\n\nMixUp:\n  Example: Mixed scene: 0.06 * (Forest area with dense canopy) + 0.94 * (Coastal wetland ecosystem)\n\nCutMix:\n  Example: Scene with cutmix: Forest area with dense canopy + patch from Forest area with dense canopy"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#performance-evaluation-metrics",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#performance-evaluation-metrics",
    "title": "Multi-modal Learning",
    "section": "Performance Evaluation Metrics",
    "text": "Performance Evaluation Metrics\n\nMulti-modal Evaluation\n\ndef demonstrate_multimodal_evaluation():\n    \"\"\"Demonstrate evaluation metrics for multi-modal models\"\"\"\n    \n    # Simulate predictions and ground truth\n    np.random.seed(42)\n    \n    # Classification task\n    num_samples = 100\n    num_classes = 5\n    \n    # Ground truth\n    y_true = np.random.randint(0, num_classes, num_samples)\n    \n    # Simulate different model predictions\n    models = {\n        'Image Only': np.random.multinomial(1, [0.8, 0.05, 0.05, 0.05, 0.05], num_samples).argmax(axis=1),\n        'Text Only': np.random.multinomial(1, [0.1, 0.7, 0.1, 0.05, 0.05], num_samples).argmax(axis=1),\n        'Early Fusion': np.random.multinomial(1, [0.85, 0.04, 0.04, 0.04, 0.03], num_samples).argmax(axis=1),\n        'Late Fusion': np.random.multinomial(1, [0.87, 0.03, 0.03, 0.04, 0.03], num_samples).argmax(axis=1),\n        'Attention Fusion': np.random.multinomial(1, [0.9, 0.025, 0.025, 0.025, 0.025], num_samples).argmax(axis=1)\n    }\n    \n    # Make predictions more realistic (align with ground truth)\n    for model_name in models:\n        # Add some correlation with ground truth\n        mask = np.random.random(num_samples) &lt; 0.7  # 70% correct\n        models[model_name][mask] = y_true[mask]\n    \n    def calculate_metrics(y_true, y_pred):\n        \"\"\"Calculate comprehensive metrics\"\"\"\n        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n        \n        accuracy = accuracy_score(y_true, y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n        conf_matrix = confusion_matrix(y_true, y_pred)\n        \n        return {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1,\n            'confusion_matrix': conf_matrix\n        }\n    \n    # Calculate metrics for each model\n    results = {}\n    for model_name, predictions in models.items():\n        results[model_name] = calculate_metrics(y_true, predictions)\n    \n    # Display results\n    print(\"Multi-modal Model Comparison:\")\n    print(\"=\"*50)\n    \n    metric_names = ['accuracy', 'precision', 'recall', 'f1_score']\n    \n    # Create comparison table\n    comparison_data = []\n    for model_name, metrics in results.items():\n        row = [model_name] + [f\"{metrics[metric]:.3f}\" for metric in metric_names]\n        comparison_data.append(row)\n    \n    # Print table\n    headers = ['Model'] + [m.replace('_', ' ').title() for m in metric_names]\n    \n    # Simple table formatting\n    col_widths = [max(len(str(row[i])) for row in [headers] + comparison_data) for i in range(len(headers))]\n    \n    def print_row(row):\n        return \" | \".join(str(item).ljust(width) for item, width in zip(row, col_widths))\n    \n    print(print_row(headers))\n    print(\"-\" * (sum(col_widths) + len(headers) * 3 - 1))\n    for row in comparison_data:\n        print(print_row(row))\n    \n    # Visualize performance comparison\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Performance metrics\n    model_names = list(results.keys())\n    metrics_data = {\n        'Accuracy': [results[name]['accuracy'] for name in model_names],\n        'Precision': [results[name]['precision'] for name in model_names],\n        'Recall': [results[name]['recall'] for name in model_names],\n        'F1-Score': [results[name]['f1_score'] for name in model_names]\n    }\n    \n    x = np.arange(len(model_names))\n    width = 0.2\n    \n    for i, (metric_name, values) in enumerate(metrics_data.items()):\n        ax1.bar(x + i*width, values, width, label=metric_name, alpha=0.8)\n    \n    ax1.set_xlabel('Models')\n    ax1.set_ylabel('Score')\n    ax1.set_title('Multi-modal Model Performance Comparison')\n    ax1.set_xticks(x + width * 1.5)\n    ax1.set_xticklabels(model_names, rotation=45, ha='right')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    ax1.set_ylim(0, 1)\n    \n    # Confusion matrix for best model\n    best_model = max(results.keys(), key=lambda x: results[x]['f1_score'])\n    best_conf_matrix = results[best_model]['confusion_matrix']\n    \n    im = ax2.imshow(best_conf_matrix, cmap='Blues')\n    ax2.set_title(f'Confusion Matrix - {best_model}')\n    ax2.set_xlabel('Predicted Class')\n    ax2.set_ylabel('True Class')\n    \n    # Add text annotations\n    for i in range(num_classes):\n        for j in range(num_classes):\n            ax2.text(j, i, str(best_conf_matrix[i, j]), \n                    ha='center', va='center', color='black' if best_conf_matrix[i, j] &lt; best_conf_matrix.max()/2 else 'white')\n    \n    plt.colorbar(im, ax=ax2)\n    plt.tight_layout()\n    plt.show()\n    \n    # Cross-modal retrieval metrics\n    print(f\"\\nBest performing model: {best_model}\")\n    print(f\"Best F1-score: {results[best_model]['f1_score']:.3f}\")\n    \n    return results\n\nevaluation_results = demonstrate_multimodal_evaluation()\n\nMulti-modal Model Comparison:\n==================================================\nModel            | Accuracy | Precision | Recall | F1 Score\n-------------------------------------------------------------\nImage Only       | 0.750    | 0.829     | 0.750  | 0.765   \nText Only        | 0.710    | 0.787     | 0.710  | 0.721   \nEarly Fusion     | 0.750    | 0.844     | 0.750  | 0.765   \nLate Fusion      | 0.710    | 0.844     | 0.710  | 0.735   \nAttention Fusion | 0.680    | 0.817     | 0.680  | 0.703   \n\n\n\n\n\n\n\n\n\n\nBest performing model: Image Only\nBest F1-score: 0.765"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#real-world-applications",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#real-world-applications",
    "title": "Multi-modal Learning",
    "section": "Real-world Applications",
    "text": "Real-world Applications\n\nApplications in Geospatial AI\n\ndef demonstrate_multimodal_applications():\n    \"\"\"Show real-world applications of multi-modal geospatial AI\"\"\"\n    \n    applications = {\n        \"Disaster Response\": {\n            \"modalities\": [\"Satellite imagery\", \"Social media text\", \"Weather data\"],\n            \"objective\": \"Rapid damage assessment and resource allocation\",\n            \"example_workflow\": [\n                \"1. Analyze pre/post-disaster satellite images\",\n                \"2. Extract text from social media reports\", \n                \"3. Combine with weather/climate data\",\n                \"4. Generate damage maps and priority areas\"\n            ],\n            \"challenges\": [\"Real-time processing\", \"Data reliability\", \"Multi-scale fusion\"]\n        },\n        \n        \"Urban Planning\": {\n            \"modalities\": [\"High-res imagery\", \"Demographic data\", \"Traffic patterns\"],\n            \"objective\": \"Optimize city development and infrastructure\",\n            \"example_workflow\": [\n                \"1. Analyze urban land use from imagery\",\n                \"2. Integrate population and economic data\",\n                \"3. Model traffic and mobility patterns\", \n                \"4. Generate development recommendations\"\n            ],\n            \"challenges\": [\"Privacy concerns\", \"Data integration\", \"Temporal alignment\"]\n        },\n        \n        \"Agricultural Monitoring\": {\n            \"modalities\": [\"Multispectral imagery\", \"Weather data\", \"Soil information\"],\n            \"objective\": \"Crop yield prediction and management optimization\",\n            \"example_workflow\": [\n                \"1. Monitor crop health via spectral indices\",\n                \"2. Integrate weather and climate data\",\n                \"3. Analyze soil properties and conditions\",\n                \"4. Predict yields and optimize practices\"\n            ],\n            \"challenges\": [\"Seasonal variations\", \"Regional differences\", \"Ground truth validation\"]\n        },\n        \n        \"Environmental Conservation\": {\n            \"modalities\": [\"Satellite imagery\", \"Species data\", \"Climate records\"],\n            \"objective\": \"Biodiversity monitoring and habitat protection\",\n            \"example_workflow\": [\n                \"1. Map habitat types from imagery\",\n                \"2. Track species distributions and migrations\",\n                \"3. Monitor climate and environmental changes\",\n                \"4. Identify conservation priorities\"\n            ],\n            \"challenges\": [\"Species detection\", \"Long-term monitoring\", \"Scale integration\"]\n        },\n        \n        \"Climate Change Assessment\": {\n            \"modalities\": [\"Time-series imagery\", \"Temperature records\", \"Precipitation data\"],\n            \"objective\": \"Track and predict climate impacts\",\n            \"example_workflow\": [\n                \"1. Analyze land cover changes over time\",\n                \"2. Correlate with temperature trends\",\n                \"3. Integrate precipitation patterns\",\n                \"4. Model future scenarios\"\n            ],\n            \"challenges\": [\"Long-term data consistency\", \"Attribution\", \"Uncertainty quantification\"]\n        }\n    }\n    \n    print(\"Multi-modal Applications in Geospatial AI:\")\n    print(\"=\"*60)\n    \n    for app_name, details in applications.items():\n        print(f\"\\n{app_name}:\")\n        print(f\"  Modalities: {', '.join(details['modalities'])}\")\n        print(f\"  Objective: {details['objective']}\")\n        print(f\"  Workflow:\")\n        for step in details['example_workflow']:\n            print(f\"    {step}\")\n        print(f\"  Key Challenges: {', '.join(details['challenges'])}\")\n    \n    # Create application complexity visualization\n    app_names = list(applications.keys())\n    modality_counts = [len(app['modalities']) for app in applications.values()]\n    challenge_counts = [len(app['challenges']) for app in applications.values()]\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Modalities per application\n    bars1 = ax1.bar(range(len(app_names)), modality_counts, color='skyblue', alpha=0.7)\n    ax1.set_xlabel('Applications')\n    ax1.set_ylabel('Number of Modalities')\n    ax1.set_title('Data Modalities per Application')\n    ax1.set_xticks(range(len(app_names)))\n    ax1.set_xticklabels(app_names, rotation=45, ha='right')\n    \n    # Add value labels on bars\n    for bar, count in zip(bars1, modality_counts):\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n                f'{count}', ha='center', va='bottom')\n    \n    # Challenges per application  \n    bars2 = ax2.bar(range(len(app_names)), challenge_counts, color='lightcoral', alpha=0.7)\n    ax2.set_xlabel('Applications')\n    ax2.set_ylabel('Number of Key Challenges')\n    ax2.set_title('Implementation Challenges per Application')\n    ax2.set_xticks(range(len(app_names)))\n    ax2.set_xticklabels(app_names, rotation=45, ha='right')\n    \n    # Add value labels on bars\n    for bar, count in zip(bars2, challenge_counts):\n        height = bar.get_height()\n        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n                f'{count}', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return applications\n\nmultimodal_apps = demonstrate_multimodal_applications()\n\nMulti-modal Applications in Geospatial AI:\n============================================================\n\nDisaster Response:\n  Modalities: Satellite imagery, Social media text, Weather data\n  Objective: Rapid damage assessment and resource allocation\n  Workflow:\n    1. Analyze pre/post-disaster satellite images\n    2. Extract text from social media reports\n    3. Combine with weather/climate data\n    4. Generate damage maps and priority areas\n  Key Challenges: Real-time processing, Data reliability, Multi-scale fusion\n\nUrban Planning:\n  Modalities: High-res imagery, Demographic data, Traffic patterns\n  Objective: Optimize city development and infrastructure\n  Workflow:\n    1. Analyze urban land use from imagery\n    2. Integrate population and economic data\n    3. Model traffic and mobility patterns\n    4. Generate development recommendations\n  Key Challenges: Privacy concerns, Data integration, Temporal alignment\n\nAgricultural Monitoring:\n  Modalities: Multispectral imagery, Weather data, Soil information\n  Objective: Crop yield prediction and management optimization\n  Workflow:\n    1. Monitor crop health via spectral indices\n    2. Integrate weather and climate data\n    3. Analyze soil properties and conditions\n    4. Predict yields and optimize practices\n  Key Challenges: Seasonal variations, Regional differences, Ground truth validation\n\nEnvironmental Conservation:\n  Modalities: Satellite imagery, Species data, Climate records\n  Objective: Biodiversity monitoring and habitat protection\n  Workflow:\n    1. Map habitat types from imagery\n    2. Track species distributions and migrations\n    3. Monitor climate and environmental changes\n    4. Identify conservation priorities\n  Key Challenges: Species detection, Long-term monitoring, Scale integration\n\nClimate Change Assessment:\n  Modalities: Time-series imagery, Temperature records, Precipitation data\n  Objective: Track and predict climate impacts\n  Workflow:\n    1. Analyze land cover changes over time\n    2. Correlate with temperature trends\n    3. Integrate precipitation patterns\n    4. Model future scenarios\n  Key Challenges: Long-term data consistency, Attribution, Uncertainty quantification"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/multimodal_learning.html#summary",
    "href": "course-materials/extras/cheatsheets/multimodal_learning.html#summary",
    "title": "Multi-modal Learning",
    "section": "Summary",
    "text": "Summary\nKey concepts for multi-modal learning in geospatial AI: - Data Integration: Combining imagery, text, time series, and tabular data - Fusion Strategies: Early fusion, late fusion, and attention-based approaches\n- Architecture Patterns: Cross-modal attention, contrastive learning, joint embeddings - Contrastive Learning: CLIP-style training for image-text understanding - Data Augmentation: Cross-modal augmentation techniques - Evaluation Metrics: Multi-modal performance assessment - Applications: Disaster response, urban planning, agriculture, conservation - Challenges: Data alignment, scale differences, computational complexity"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html",
    "href": "course-materials/extras/cheatsheets/model_inference.html",
    "title": "Model Inference & Feature Extraction",
    "section": "",
    "text": "Model inference and feature extraction are crucial for applying trained models to satellite imagery analysis and extracting meaningful representations.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nPyTorch version: 2.7.1\nCUDA available: False"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html#introduction-to-model-inference",
    "href": "course-materials/extras/cheatsheets/model_inference.html#introduction-to-model-inference",
    "title": "Model Inference & Feature Extraction",
    "section": "",
    "text": "Model inference and feature extraction are crucial for applying trained models to satellite imagery analysis and extracting meaningful representations.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nPyTorch version: 2.7.1\nCUDA available: False"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html#basic-inference-patterns",
    "href": "course-materials/extras/cheatsheets/model_inference.html#basic-inference-patterns",
    "title": "Model Inference & Feature Extraction",
    "section": "Basic Inference Patterns",
    "text": "Basic Inference Patterns\n\nSingle image inference\n\nclass GeospatialClassifier(nn.Module):\n    \"\"\"Example geospatial classifier for demonstration\"\"\"\n    \n    def __init__(self, num_channels=6, num_classes=10, embed_dim=256):\n        super().__init__()\n        \n        # Feature extraction layers\n        self.conv1 = nn.Conv2d(num_channels, 64, 7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n        \n        # Global pooling and classification\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(256, num_classes)\n        \n        # Feature embedding layer\n        self.feature_embed = nn.Linear(256, embed_dim)\n        \n    def forward(self, x, return_features=False):\n        # Feature extraction\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        features = F.relu(self.conv3(x))\n        \n        # Global pooling\n        pooled = self.global_pool(features).flatten(1)\n        \n        # Classification\n        logits = self.classifier(pooled)\n        \n        if return_features:\n            embeddings = self.feature_embed(pooled)\n            return {\n                'logits': logits,\n                'features': embeddings,\n                'spatial_features': features,\n                'pooled_features': pooled\n            }\n        \n        return logits\n\n# Create model\nmodel = GeospatialClassifier(num_channels=6, num_classes=10, embed_dim=256)\nmodel.eval()\n\n# Single image inference\nsample_image = torch.randn(1, 6, 224, 224)  # Batch of 1, 6 channels\n\nwith torch.no_grad():\n    # Basic inference\n    predictions = model(sample_image)\n    \n    # Inference with features\n    outputs = model(sample_image, return_features=True)\n\nprint(f\"Input shape: {sample_image.shape}\")\nprint(f\"Predictions shape: {predictions.shape}\")\nprint(f\"Logits shape: {outputs['logits'].shape}\")\nprint(f\"Features shape: {outputs['features'].shape}\")\nprint(f\"Spatial features shape: {outputs['spatial_features'].shape}\")\n\nInput shape: torch.Size([1, 6, 224, 224])\nPredictions shape: torch.Size([1, 10])\nLogits shape: torch.Size([1, 10])\nFeatures shape: torch.Size([1, 256])\nSpatial features shape: torch.Size([1, 256, 28, 28])\n\n\n\n\nBatch inference\n\ndef batch_inference(model, images, batch_size=32, device='cpu'):\n    \"\"\"Perform batch inference on multiple images\"\"\"\n    \n    model.to(device)\n    model.eval()\n    \n    all_predictions = []\n    all_features = []\n    \n    # Process in batches\n    n_images = len(images)\n    n_batches = (n_images + batch_size - 1) // batch_size\n    \n    with torch.no_grad():\n        for i in range(n_batches):\n            start_idx = i * batch_size\n            end_idx = min(start_idx + batch_size, n_images)\n            \n            batch = images[start_idx:end_idx].to(device)\n            \n            # Get predictions and features\n            outputs = model(batch, return_features=True)\n            \n            all_predictions.append(outputs['logits'].cpu())\n            all_features.append(outputs['features'].cpu())\n            \n            print(f\"Processed batch {i+1}/{n_batches}\", end='\\r')\n    \n    # Concatenate results\n    final_predictions = torch.cat(all_predictions, dim=0)\n    final_features = torch.cat(all_features, dim=0)\n    \n    print(f\"\\nCompleted inference on {n_images} images\")\n    \n    return final_predictions, final_features\n\n# Create sample batch\nbatch_images = torch.randn(100, 6, 224, 224)\n\n# Run batch inference\npredictions, features = batch_inference(model, batch_images, batch_size=16)\n\nprint(f\"Batch predictions shape: {predictions.shape}\")\nprint(f\"Batch features shape: {features.shape}\")\n\nProcessed batch 1/7Processed batch 2/7Processed batch 3/7Processed batch 4/7Processed batch 5/7Processed batch 6/7Processed batch 7/7\nCompleted inference on 100 images\nBatch predictions shape: torch.Size([100, 10])\nBatch features shape: torch.Size([100, 256])"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html#feature-extraction-techniques",
    "href": "course-materials/extras/cheatsheets/model_inference.html#feature-extraction-techniques",
    "title": "Model Inference & Feature Extraction",
    "section": "Feature Extraction Techniques",
    "text": "Feature Extraction Techniques\n\nLayer-wise feature extraction\n\nclass FeatureExtractor:\n    \"\"\"Extract features from specific layers of a model\"\"\"\n    \n    def __init__(self, model, layer_names=None):\n        self.model = model\n        self.model.eval()\n        self.features = {}\n        self.hooks = []\n        \n        if layer_names is None:\n            # Extract from all named modules\n            layer_names = [name for name, _ in model.named_modules() if name]\n        \n        self.register_hooks(layer_names)\n    \n    def register_hooks(self, layer_names):\n        \"\"\"Register forward hooks for feature extraction\"\"\"\n        \n        def make_hook(name):\n            def hook(module, input, output):\n                # Store detached copy to avoid gradient tracking\n                if isinstance(output, torch.Tensor):\n                    self.features[name] = output.detach().cpu()\n                elif isinstance(output, (list, tuple)):\n                    self.features[name] = [o.detach().cpu() if isinstance(o, torch.Tensor) else o for o in output]\n                elif isinstance(output, dict):\n                    self.features[name] = {k: v.detach().cpu() if isinstance(v, torch.Tensor) else v \n                                         for k, v in output.items()}\n            return hook\n        \n        # Register hooks\n        for name, module in self.model.named_modules():\n            if name in layer_names:\n                handle = module.register_forward_hook(make_hook(name))\n                self.hooks.append(handle)\n                print(f\"Registered hook for layer: {name}\")\n    \n    def extract(self, images):\n        \"\"\"Extract features from registered layers\"\"\"\n        \n        self.features.clear()\n        \n        with torch.no_grad():\n            # Forward pass triggers hooks\n            _ = self.model(images)\n        \n        return self.features.copy()\n    \n    def remove_hooks(self):\n        \"\"\"Remove all registered hooks\"\"\"\n        for hook in self.hooks:\n            hook.remove()\n        self.hooks.clear()\n\n# Create feature extractor\nextractor = FeatureExtractor(\n    model, \n    layer_names=['conv1', 'conv2', 'conv3', 'global_pool']\n)\n\n# Extract features\nsample_input = torch.randn(4, 6, 224, 224)\nextracted_features = extractor.extract(sample_input)\n\nprint(\"Extracted features:\")\nfor layer_name, features in extracted_features.items():\n    if isinstance(features, torch.Tensor):\n        print(f\"{layer_name}: {features.shape}\")\n    else:\n        print(f\"{layer_name}: {type(features)}\")\n\n# Clean up\nextractor.remove_hooks()\n\nRegistered hook for layer: conv1\nRegistered hook for layer: conv2\nRegistered hook for layer: conv3\nRegistered hook for layer: global_pool\nExtracted features:\nconv1: torch.Size([4, 64, 112, 112])\nconv2: torch.Size([4, 128, 56, 56])\nconv3: torch.Size([4, 256, 28, 28])\nglobal_pool: torch.Size([4, 256, 1, 1])\n\n\n\n\nMulti-scale feature extraction\n\nclass MultiScaleFeatureExtractor(nn.Module):\n    \"\"\"Extract features at multiple scales\"\"\"\n    \n    def __init__(self, backbone):\n        super().__init__()\n        self.backbone = backbone\n        \n        # Feature pyramid levels\n        self.scales = [1.0, 0.75, 0.5, 0.25]\n        \n    def forward(self, x):\n        batch_size, channels, height, width = x.shape\n        \n        multiscale_features = {}\n        \n        for scale in self.scales:\n            # Resize input\n            if scale != 1.0:\n                new_size = (int(height * scale), int(width * scale))\n                scaled_input = F.interpolate(x, size=new_size, mode='bilinear', align_corners=False)\n            else:\n                scaled_input = x\n            \n            # Extract features\n            with torch.no_grad():\n                outputs = self.backbone(scaled_input, return_features=True)\n                \n            # Store features with scale info\n            scale_key = f\"scale_{scale:.2f}\"\n            multiscale_features[scale_key] = {\n                'features': outputs['features'],\n                'spatial_features': outputs['spatial_features'],\n                'input_size': scaled_input.shape[-2:]\n            }\n        \n        return multiscale_features\n\n# Create multi-scale extractor\nmultiscale_extractor = MultiScaleFeatureExtractor(model)\nmultiscale_extractor.eval()\n\n# Extract multi-scale features\nsample_input = torch.randn(2, 6, 224, 224)\nmultiscale_features = multiscale_extractor(sample_input)\n\nprint(\"Multi-scale features:\")\nfor scale, features in multiscale_features.items():\n    print(f\"{scale}:\")\n    print(f\"  Features: {features['features'].shape}\")\n    print(f\"  Spatial: {features['spatial_features'].shape}\")\n    print(f\"  Input size: {features['input_size']}\")\n\nMulti-scale features:\nscale_1.00:\n  Features: torch.Size([2, 256])\n  Spatial: torch.Size([2, 256, 28, 28])\n  Input size: torch.Size([224, 224])\nscale_0.75:\n  Features: torch.Size([2, 256])\n  Spatial: torch.Size([2, 256, 21, 21])\n  Input size: torch.Size([168, 168])\nscale_0.50:\n  Features: torch.Size([2, 256])\n  Spatial: torch.Size([2, 256, 14, 14])\n  Input size: torch.Size([112, 112])\nscale_0.25:\n  Features: torch.Size([2, 256])\n  Spatial: torch.Size([2, 256, 7, 7])\n  Input size: torch.Size([56, 56])"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html#advanced-inference-techniques",
    "href": "course-materials/extras/cheatsheets/model_inference.html#advanced-inference-techniques",
    "title": "Model Inference & Feature Extraction",
    "section": "Advanced Inference Techniques",
    "text": "Advanced Inference Techniques\n\nAttention map visualization\n\nclass AttentionExtractor:\n    \"\"\"Extract and visualize attention maps\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n        self.attention_maps = {}\n        self.hooks = []\n    \n    def register_attention_hooks(self):\n        \"\"\"Register hooks for attention layers\"\"\"\n        \n        def attention_hook(name):\n            def hook(module, input, output):\n                # For attention mechanisms, we typically want the attention weights\n                # This is a simplified example - actual implementation depends on model architecture\n                if hasattr(module, 'attention_weights'):\n                    self.attention_maps[name] = module.attention_weights.detach().cpu()\n                elif isinstance(output, tuple) and len(output) &gt; 1:\n                    # Assume second output contains attention weights\n                    self.attention_maps[name] = output[1].detach().cpu()\n                elif hasattr(output, 'attentions'):\n                    self.attention_maps[name] = output.attentions.detach().cpu()\n            return hook\n        \n        # Look for attention-related modules\n        for name, module in self.model.named_modules():\n            if 'attention' in name.lower() or 'attn' in name.lower():\n                handle = module.register_forward_hook(attention_hook(name))\n                self.hooks.append(handle)\n                print(f\"Registered attention hook: {name}\")\n    \n    def extract_attention(self, images):\n        \"\"\"Extract attention maps\"\"\"\n        self.attention_maps.clear()\n        \n        with torch.no_grad():\n            _ = self.model(images)\n        \n        return self.attention_maps.copy()\n    \n    def visualize_attention(self, image, attention_map, alpha=0.6):\n        \"\"\"Visualize attention map overlaid on image\"\"\"\n        \n        # Convert image to RGB if needed\n        if image.shape[0] &gt; 3:\n            # Use first 3 channels as RGB\n            rgb_image = image[:3]\n        else:\n            rgb_image = image\n        \n        # Normalize image for display\n        rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min())\n        rgb_image = rgb_image.permute(1, 2, 0).numpy()\n        \n        # Process attention map\n        if attention_map.dim() &gt; 2:\n            attention_map = attention_map.mean(dim=0)  # Average over heads/channels\n        \n        # Resize attention map to match image size\n        attention_resized = F.interpolate(\n            attention_map.unsqueeze(0).unsqueeze(0),\n            size=rgb_image.shape[:2],\n            mode='bilinear',\n            align_corners=False\n        ).squeeze().numpy()\n        \n        # Create visualization\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        \n        # Original image\n        axes[0].imshow(rgb_image)\n        axes[0].set_title('Original Image')\n        axes[0].axis('off')\n        \n        # Attention map\n        axes[1].imshow(attention_resized, cmap='hot')\n        axes[1].set_title('Attention Map')\n        axes[1].axis('off')\n        \n        # Overlay\n        axes[2].imshow(rgb_image)\n        axes[2].imshow(attention_resized, alpha=alpha, cmap='hot')\n        axes[2].set_title('Attention Overlay')\n        axes[2].axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def remove_hooks(self):\n        \"\"\"Remove attention hooks\"\"\"\n        for hook in self.hooks:\n            hook.remove()\n        self.hooks.clear()\n\n# Create attention extractor (mock example)\nattention_extractor = AttentionExtractor(model)\n# attention_extractor.register_attention_hooks()  # Would need actual attention layers\n\nprint(\"Attention extractor ready (requires model with attention layers)\")\n\nAttention extractor ready (requires model with attention layers)\n\n\n\n\nGradient-based explanations\n\nclass GradCAM:\n    \"\"\"Gradient-weighted Class Activation Mapping\"\"\"\n    \n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        \n        # Register hooks\n        self.register_hooks()\n    \n    def register_hooks(self):\n        \"\"\"Register hooks for gradients and activations\"\"\"\n        \n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0].detach()\n        \n        def forward_hook(module, input, output):\n            self.activations = output.detach()\n        \n        # Find target layer\n        target_module = dict(self.model.named_modules())[self.target_layer]\n        target_module.register_forward_hook(forward_hook)\n        target_module.register_backward_hook(backward_hook)\n    \n    def generate_cam(self, images, class_idx=None):\n        \"\"\"Generate Class Activation Map\"\"\"\n        \n        # Enable gradients\n        images.requires_grad_(True)\n        \n        # Forward pass\n        outputs = self.model(images)\n        \n        # If class_idx not specified, use predicted class\n        if class_idx is None:\n            class_idx = outputs.argmax(dim=1)\n        \n        # Backward pass for target class\n        self.model.zero_grad()\n        class_loss = outputs[0, class_idx[0]] if isinstance(class_idx, torch.Tensor) else outputs[0, class_idx]\n        class_loss.backward()\n        \n        # Compute CAM\n        gradients = self.gradients[0]  # First image in batch\n        activations = self.activations[0]  # First image in batch\n        \n        # Global average pooling of gradients\n        weights = torch.mean(gradients, dim=[1, 2])\n        \n        # Weighted combination of activation maps\n        cam = torch.zeros(activations.shape[1], activations.shape[2])\n        for i, w in enumerate(weights):\n            cam += w * activations[i]\n        \n        # Apply ReLU and normalize\n        cam = F.relu(cam)\n        cam = cam / torch.max(cam) if torch.max(cam) &gt; 0 else cam\n        \n        return cam\n    \n    def visualize_cam(self, image, cam, alpha=0.4):\n        \"\"\"Visualize CAM overlaid on original image\"\"\"\n        \n        # Convert image for display\n        if image.shape[0] &gt; 3:\n            display_image = image[:3]  # Use first 3 channels\n        else:\n            display_image = image\n        \n        display_image = (display_image - display_image.min()) / (display_image.max() - display_image.min())\n        display_image = display_image.permute(1, 2, 0).detach().numpy()\n        \n        # Resize CAM to match image size\n        cam_resized = F.interpolate(\n            cam.unsqueeze(0).unsqueeze(0),\n            size=display_image.shape[:2],\n            mode='bilinear',\n            align_corners=False\n        ).squeeze().numpy()\n        \n        # Create visualization\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        \n        axes[0].imshow(display_image)\n        axes[0].set_title('Original Image')\n        axes[0].axis('off')\n        \n        axes[1].imshow(cam_resized, cmap='jet')\n        axes[1].set_title('Grad-CAM')\n        axes[1].axis('off')\n        \n        axes[2].imshow(display_image)\n        axes[2].imshow(cam_resized, alpha=alpha, cmap='jet')\n        axes[2].set_title('Grad-CAM Overlay')\n        axes[2].axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n\n# Create GradCAM for conv3 layer\ngradcam = GradCAM(model, target_layer='conv3')\n\n# Generate CAM\nsample_input = torch.randn(1, 6, 224, 224)\ncam = gradcam.generate_cam(sample_input)\n\nprint(f\"Generated CAM shape: {cam.shape}\")\nprint(f\"CAM range: [{cam.min():.3f}, {cam.max():.3f}]\")\n\n# Visualize\ngradcam.visualize_cam(sample_input[0], cam)\n\nGenerated CAM shape: torch.Size([28, 28])\nCAM range: [0.000, 1.000]\n\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1842: FutureWarning:\n\nUsing a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html#feature-analysis-and-dimensionality-reduction",
    "href": "course-materials/extras/cheatsheets/model_inference.html#feature-analysis-and-dimensionality-reduction",
    "title": "Model Inference & Feature Extraction",
    "section": "Feature Analysis and Dimensionality Reduction",
    "text": "Feature Analysis and Dimensionality Reduction\n\nPCA analysis of features\n\ndef analyze_features_pca(features, n_components=50, visualize=True):\n    \"\"\"Analyze features using PCA\"\"\"\n    \n    # Flatten features if needed\n    if features.dim() &gt; 2:\n        original_shape = features.shape\n        features_flat = features.view(features.shape[0], -1)\n    else:\n        features_flat = features\n        original_shape = features.shape\n    \n    # Convert to numpy\n    features_np = features_flat.numpy()\n    \n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    features_pca = pca.fit_transform(features_np)\n    \n    # Analyze explained variance\n    explained_var_ratio = pca.explained_variance_ratio_\n    cumulative_var = np.cumsum(explained_var_ratio)\n    \n    if visualize:\n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        \n        # Explained variance\n        axes[0].bar(range(len(explained_var_ratio)), explained_var_ratio)\n        axes[0].set_title('Explained Variance by Component')\n        axes[0].set_xlabel('Principal Component')\n        axes[0].set_ylabel('Explained Variance Ratio')\n        \n        # Cumulative explained variance\n        axes[1].plot(cumulative_var, marker='o')\n        axes[1].set_title('Cumulative Explained Variance')\n        axes[1].set_xlabel('Number of Components')\n        axes[1].set_ylabel('Cumulative Variance Ratio')\n        axes[1].grid(True, alpha=0.3)\n        \n        # First two components\n        axes[2].scatter(features_pca[:, 0], features_pca[:, 1], alpha=0.6)\n        axes[2].set_title('First Two Principal Components')\n        axes[2].set_xlabel('PC1')\n        axes[2].set_ylabel('PC2')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    return {\n        'features_pca': features_pca,\n        'explained_variance_ratio': explained_var_ratio,\n        'cumulative_variance': cumulative_var,\n        'pca_model': pca\n    }\n\n# Generate sample features for analysis\nsample_features = torch.randn(100, 256)  # 100 samples, 256 features\npca_results = analyze_features_pca(sample_features, n_components=20)\n\nprint(f\"PCA features shape: {pca_results['features_pca'].shape}\")\nprint(f\"First 5 components explain {pca_results['cumulative_variance'][4]:.1%} of variance\")\n\n\n\n\n\n\n\n\nPCA features shape: (100, 20)\nFirst 5 components explain 11.9% of variance\n\n\n\n\nt-SNE visualization\n\ndef visualize_features_tsne(features, labels=None, perplexity=30, random_state=42):\n    \"\"\"Visualize features using t-SNE\"\"\"\n    \n    # Flatten features if needed\n    if features.dim() &gt; 2:\n        features_flat = features.view(features.shape[0], -1)\n    else:\n        features_flat = features\n    \n    features_np = features_flat.numpy()\n    \n    # Apply t-SNE\n    print(\"Computing t-SNE embedding...\")\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n    features_tsne = tsne.fit_transform(features_np)\n    \n    # Create visualization\n    plt.figure(figsize=(10, 8))\n    \n    if labels is not None:\n        # Color by labels\n        unique_labels = np.unique(labels)\n        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n        \n        for i, label in enumerate(unique_labels):\n            mask = labels == label\n            plt.scatter(features_tsne[mask, 0], features_tsne[mask, 1], \n                       c=[colors[i]], label=f'Class {label}', alpha=0.6)\n        plt.legend()\n    else:\n        plt.scatter(features_tsne[:, 0], features_tsne[:, 1], alpha=0.6)\n    \n    plt.title('t-SNE Visualization of Features')\n    plt.xlabel('t-SNE 1')\n    plt.ylabel('t-SNE 2')\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    return features_tsne\n\n# Generate sample data with labels\nsample_features = torch.randn(200, 256)\nsample_labels = np.random.randint(0, 5, 200)  # 5 classes\n\n# Visualize with t-SNE\ntsne_features = visualize_features_tsne(sample_features, sample_labels)\nprint(f\"t-SNE features shape: {tsne_features.shape}\")\n\nComputing t-SNE embedding...\n\n\n\n\n\n\n\n\n\nt-SNE features shape: (200, 2)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html#inference-optimization",
    "href": "course-materials/extras/cheatsheets/model_inference.html#inference-optimization",
    "title": "Model Inference & Feature Extraction",
    "section": "Inference Optimization",
    "text": "Inference Optimization\n\nModel quantization for faster inference\n\ndef quantize_model(model, calibration_data=None):\n    \"\"\"Quantize model for faster inference\"\"\"\n    \n    # Dynamic quantization (post-training)\n    quantized_model = torch.quantization.quantize_dynamic(\n        model,\n        {nn.Linear, nn.Conv2d},  # Layers to quantize\n        dtype=torch.qint8\n    )\n    \n    print(\"Applied dynamic quantization\")\n    \n    return quantized_model\n\ndef compare_inference_speed(original_model, quantized_model, test_input, num_runs=100):\n    \"\"\"Compare inference speed between models\"\"\"\n    \n    import time\n    \n    # Warm up\n    for _ in range(10):\n        with torch.no_grad():\n            _ = original_model(test_input)\n            _ = quantized_model(test_input)\n    \n    # Time original model\n    start_time = time.time()\n    for _ in range(num_runs):\n        with torch.no_grad():\n            _ = original_model(test_input)\n    original_time = time.time() - start_time\n    \n    # Time quantized model\n    start_time = time.time()\n    for _ in range(num_runs):\n        with torch.no_grad():\n            _ = quantized_model(test_input)\n    quantized_time = time.time() - start_time\n    \n    speedup = original_time / quantized_time\n    \n    print(f\"Original model: {original_time:.3f}s\")\n    print(f\"Quantized model: {quantized_time:.3f}s\") \n    print(f\"Speedup: {speedup:.2f}x\")\n    \n    return speedup\n\n# Create quantized version\nmodel.eval()  # Important: set to eval mode\nquantized_model = quantize_model(model)\n\n# Compare speeds\ntest_input = torch.randn(1, 6, 224, 224)\nspeedup = compare_inference_speed(model, quantized_model, test_input, num_runs=50)\n\nApplied dynamic quantization\nOriginal model: 0.471s\nQuantized model: 0.516s\nSpeedup: 0.91x\n\n\n\n\nBatch size optimization\n\ndef find_optimal_batch_size(model, input_shape, device='cpu', max_batch_size=128):\n    \"\"\"Find optimal batch size for memory and speed\"\"\"\n    \n    model.to(device)\n    model.eval()\n    \n    batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n    if max_batch_size &gt; 64:\n        batch_sizes.extend([128, 256])\n    \n    batch_sizes = [bs for bs in batch_sizes if bs &lt;= max_batch_size]\n    \n    results = {}\n    \n    for batch_size in batch_sizes:\n        try:\n            # Create test batch\n            test_batch = torch.randn(batch_size, *input_shape[1:]).to(device)\n            \n            # Measure memory and time\n            if device != 'cpu' and torch.cuda.is_available():\n                torch.cuda.reset_peak_memory_stats()\n                start_memory = torch.cuda.memory_allocated()\n            \n            import time\n            start_time = time.time()\n            \n            with torch.no_grad():\n                for _ in range(10):  # Average over multiple runs\n                    outputs = model(test_batch)\n            \n            elapsed_time = time.time() - start_time\n            throughput = (batch_size * 10) / elapsed_time  # samples per second\n            \n            if device != 'cpu' and torch.cuda.is_available():\n                peak_memory = torch.cuda.max_memory_allocated()\n                memory_per_sample = (peak_memory - start_memory) / batch_size\n            else:\n                memory_per_sample = 0\n            \n            results[batch_size] = {\n                'throughput': throughput,\n                'time_per_sample': elapsed_time / (batch_size * 10),\n                'memory_per_sample': memory_per_sample / (1024**2)  # MB\n            }\n            \n            print(f\"Batch size {batch_size}: {throughput:.1f} samples/sec, \"\n                  f\"{memory_per_sample / (1024**2):.1f} MB/sample\")\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Batch size {batch_size}: Out of memory\")\n                break\n            else:\n                raise e\n    \n    # Find optimal batch size (highest throughput)\n    if results:\n        optimal_batch_size = max(results.keys(), key=lambda k: results[k]['throughput'])\n        print(f\"\\nOptimal batch size: {optimal_batch_size}\")\n        return optimal_batch_size, results\n    \n    return 1, results\n\n# Find optimal batch size\noptimal_bs, batch_results = find_optimal_batch_size(\n    model, \n    input_shape=(1, 6, 224, 224),\n    device='cpu',\n    max_batch_size=64\n)\n\nBatch size 1: 125.2 samples/sec, 0.0 MB/sample\nBatch size 2: 107.4 samples/sec, 0.0 MB/sample\nBatch size 4: 182.6 samples/sec, 0.0 MB/sample\nBatch size 8: 185.5 samples/sec, 0.0 MB/sample\nBatch size 16: 216.2 samples/sec, 0.0 MB/sample\nBatch size 32: 217.5 samples/sec, 0.0 MB/sample\nBatch size 64: 241.9 samples/sec, 0.0 MB/sample\n\nOptimal batch size: 64"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html#summary",
    "href": "course-materials/extras/cheatsheets/model_inference.html#summary",
    "title": "Model Inference & Feature Extraction",
    "section": "Summary",
    "text": "Summary\nKey inference and feature extraction techniques: - Basic inference: Single image and batch processing - Feature extraction: Layer-wise and multi-scale features\n- Attention visualization: Understanding model focus - Gradient explanations: Grad-CAM for interpretability - Dimensionality reduction: PCA and t-SNE analysis - Optimization: Quantization and batch size tuning - Performance monitoring: Speed and memory profiling"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "",
    "text": "Matplotlib provides powerful tools for visualizing geospatial data, including satellite imagery, raster data, and cartographic projections.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import LinearSegmentedColormap, Normalize, LogNorm\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.collections import PatchCollection\nimport numpy as np\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\nimport rasterio\nfrom rasterio.plot import show as rasterio_show\nimport matplotlib.gridspec as gridspec\n\nprint(f\"Matplotlib version: {plt.matplotlib.__version__}\")\nprint(f\"Cartopy available: {'‚úì' if 'cartopy' in globals() else '‚úó'}\")\n\nMatplotlib version: 3.10.5\nCartopy available: ‚úó"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#introduction-to-geospatial-plotting",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#introduction-to-geospatial-plotting",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "",
    "text": "Matplotlib provides powerful tools for visualizing geospatial data, including satellite imagery, raster data, and cartographic projections.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import LinearSegmentedColormap, Normalize, LogNorm\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.collections import PatchCollection\nimport numpy as np\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\nimport rasterio\nfrom rasterio.plot import show as rasterio_show\nimport matplotlib.gridspec as gridspec\n\nprint(f\"Matplotlib version: {plt.matplotlib.__version__}\")\nprint(f\"Cartopy available: {'‚úì' if 'cartopy' in globals() else '‚úó'}\")\n\nMatplotlib version: 3.10.5\nCartopy available: ‚úó"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#basic-satellite-imagery-visualization",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#basic-satellite-imagery-visualization",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "Basic Satellite Imagery Visualization",
    "text": "Basic Satellite Imagery Visualization\n\nSingle band plotting\n\ndef create_sample_satellite_data():\n    \"\"\"Generate sample multi-band satellite data\"\"\"\n    \n    # Create synthetic satellite data\n    np.random.seed(42)\n    height, width = 512, 512\n    \n    # Simulate different spectral bands\n    bands = {\n        'red': np.random.beta(2, 5, (height, width)) * 0.8,\n        'green': np.random.beta(3, 4, (height, width)) * 0.7, \n        'blue': np.random.beta(4, 3, (height, width)) * 0.6,\n        'nir': np.random.beta(1.5, 3, (height, width)) * 0.9,\n        'swir1': np.random.beta(2, 6, (height, width)) * 0.5,\n        'swir2': np.random.beta(1, 4, (height, width)) * 0.4\n    }\n    \n    # Add some spatial structure (simulate land features)\n    y, x = np.ogrid[:height, :width]\n    center_y, center_x = height // 2, width // 2\n    \n    # Add circular feature (lake/urban area)\n    lake_mask = (x - center_x)**2 + (y - center_y)**2 &lt; (height // 4)**2\n    bands['blue'][lake_mask] *= 1.5\n    bands['green'][lake_mask] *= 0.7\n    bands['red'][lake_mask] *= 0.5\n    \n    # Add linear features (rivers/roads)\n    river_mask = np.abs(y - center_y - 0.3 * (x - center_x)) &lt; 10\n    bands['blue'][river_mask] *= 1.3\n    bands['green'][river_mask] *= 0.8\n    \n    return bands\n\ndef plot_single_band(band_data, band_name, cmap='viridis', figsize=(8, 6)):\n    \"\"\"Plot a single spectral band\"\"\"\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Display the band\n    im = ax.imshow(band_data, cmap=cmap, aspect='equal')\n    \n    # Add colorbar\n    cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n    cbar.set_label(f'{band_name.upper()} Reflectance', fontsize=12)\n    \n    # Styling\n    ax.set_title(f'{band_name.upper()} Band', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Pixel X', fontsize=12)\n    ax.set_ylabel('Pixel Y', fontsize=12)\n    \n    # Remove tick labels for cleaner look\n    ax.tick_params(labelbottom=False, labelleft=False)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig, ax\n\n# Generate sample data\nbands = create_sample_satellite_data()\n\n# Plot individual bands\nplot_single_band(bands['red'], 'Red', cmap='Reds')\nplot_single_band(bands['nir'], 'NIR', cmap='RdYlGn')\nplot_single_band(bands['swir1'], 'SWIR1', cmap='YlOrBr')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(&lt;Figure size 768x576 with 2 Axes&gt;,\n &lt;Axes: title={'center': 'SWIR1 Band'}, xlabel='Pixel X', ylabel='Pixel Y'&gt;)\n\n\n\n\nMulti-band comparison\n\ndef plot_band_comparison(bands, band_names, ncols=3, figsize=(15, 10)):\n    \"\"\"Plot multiple bands for comparison\"\"\"\n    \n    nrows = len(band_names) // ncols + (1 if len(band_names) % ncols else 0)\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    if nrows == 1 and ncols == 1:\n        axes = [axes]\n    elif nrows == 1 or ncols == 1:\n        axes = axes.flatten()\n    else:\n        axes = axes.flatten()\n    \n    # Color maps for different bands\n    cmaps = {\n        'red': 'Reds', 'green': 'Greens', 'blue': 'Blues',\n        'nir': 'RdYlGn', 'swir1': 'YlOrBr', 'swir2': 'copper'\n    }\n    \n    for i, band_name in enumerate(band_names):\n        ax = axes[i]\n        band_data = bands[band_name]\n        cmap = cmaps.get(band_name, 'viridis')\n        \n        im = ax.imshow(band_data, cmap=cmap, aspect='equal')\n        \n        # Add colorbar for each subplot\n        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n        cbar.set_label('Reflectance', fontsize=10)\n        \n        ax.set_title(f'{band_name.upper()}', fontsize=12, fontweight='bold')\n        ax.tick_params(labelbottom=False, labelleft=False)\n    \n    # Hide unused subplots\n    for i in range(len(band_names), len(axes)):\n        axes[i].set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig, axes\n\n# Plot all bands\nband_names = ['red', 'green', 'blue', 'nir', 'swir1', 'swir2']\nplot_band_comparison(bands, band_names, ncols=3)\n\n\n\n\n\n\n\n\n(&lt;Figure size 1440x960 with 12 Axes&gt;,\n array([&lt;Axes: title={'center': 'RED'}&gt;, &lt;Axes: title={'center': 'GREEN'}&gt;,\n        &lt;Axes: title={'center': 'BLUE'}&gt;, &lt;Axes: title={'center': 'NIR'}&gt;,\n        &lt;Axes: title={'center': 'SWIR1'}&gt;,\n        &lt;Axes: title={'center': 'SWIR2'}&gt;], dtype=object))"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#rgb-and-false-color-composites",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#rgb-and-false-color-composites",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "RGB and False Color Composites",
    "text": "RGB and False Color Composites\n\nRGB composite\n\ndef create_rgb_composite(red, green, blue, enhance=True, gamma=1.0):\n    \"\"\"Create RGB composite from individual bands\"\"\"\n    \n    # Stack bands\n    rgb = np.stack([red, green, blue], axis=-1)\n    \n    if enhance:\n        # Contrast stretching\n        for i in range(3):\n            band = rgb[:, :, i]\n            p2, p98 = np.percentile(band, (2, 98))\n            rgb[:, :, i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n    \n    # Gamma correction\n    if gamma != 1.0:\n        rgb = np.power(rgb, gamma)\n    \n    return np.clip(rgb, 0, 1)\n\ndef plot_rgb_composite(rgb_data, title='RGB Composite', figsize=(10, 8)):\n    \"\"\"Plot RGB composite\"\"\"\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    \n    ax.imshow(rgb_data, aspect='equal')\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.tick_params(labelbottom=False, labelleft=False)\n    \n    # Add scale bar (approximate)\n    scale_bar = Rectangle((rgb_data.shape[1] - 100, rgb_data.shape[0] - 30), \n                         80, 10, facecolor='white', edgecolor='black')\n    ax.add_patch(scale_bar)\n    ax.text(rgb_data.shape[1] - 60, rgb_data.shape[0] - 45, '1 km', \n            ha='center', va='top', fontsize=10, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig, ax\n\n# Create RGB composite\nrgb_composite = create_rgb_composite(bands['red'], bands['green'], bands['blue'])\nplot_rgb_composite(rgb_composite, 'True Color RGB')\n\n# Create false color composite (NIR-Red-Green)\nfalse_color = create_rgb_composite(bands['nir'], bands['red'], bands['green'])\nplot_rgb_composite(false_color, 'False Color (NIR-Red-Green)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(&lt;Figure size 960x768 with 1 Axes&gt;,\n &lt;Axes: title={'center': 'False Color (NIR-Red-Green)'}&gt;)\n\n\n\n\nMultiple composite comparison\n\ndef plot_composite_comparison(bands, composite_configs, figsize=(15, 10)):\n    \"\"\"Plot multiple composite configurations\"\"\"\n    \n    n_composites = len(composite_configs)\n    ncols = 2\n    nrows = n_composites // ncols + (1 if n_composites % ncols else 0)\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    if nrows == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i, (name, config) in enumerate(composite_configs.items()):\n        row, col = i // ncols, i % ncols\n        ax = axes[row, col]\n        \n        # Create composite\n        r_band = bands[config['red']]\n        g_band = bands[config['green']]  \n        b_band = bands[config['blue']]\n        \n        composite = create_rgb_composite(r_band, g_band, b_band, enhance=True)\n        \n        ax.imshow(composite, aspect='equal')\n        ax.set_title(f'{name}\\n({config[\"red\"]}-{config[\"green\"]}-{config[\"blue\"]})', \n                    fontsize=12, fontweight='bold')\n        ax.tick_params(labelbottom=False, labelleft=False)\n    \n    # Hide unused subplots\n    for i in range(n_composites, nrows * ncols):\n        row, col = i // ncols, i % ncols\n        axes[row, col].set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig, axes\n\n# Define different composite configurations\ncomposite_configs = {\n    'True Color': {'red': 'red', 'green': 'green', 'blue': 'blue'},\n    'False Color': {'red': 'nir', 'green': 'red', 'blue': 'green'},\n    'Agriculture': {'red': 'swir1', 'green': 'nir', 'blue': 'red'},\n    'Urban': {'red': 'swir2', 'green': 'swir1', 'blue': 'red'}\n}\n\nplot_composite_comparison(bands, composite_configs)\n\n\n\n\n\n\n\n\n(&lt;Figure size 1440x960 with 4 Axes&gt;,\n array([[&lt;Axes: title={'center': 'True Color\\n(red-green-blue)'}&gt;,\n         &lt;Axes: title={'center': 'False Color\\n(nir-red-green)'}&gt;],\n        [&lt;Axes: title={'center': 'Agriculture\\n(swir1-nir-red)'}&gt;,\n         &lt;Axes: title={'center': 'Urban\\n(swir2-swir1-red)'}&gt;]],\n       dtype=object))"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#advanced-visualization-techniques",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#advanced-visualization-techniques",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "Advanced Visualization Techniques",
    "text": "Advanced Visualization Techniques\n\nSpectral indices calculation and plotting\n\ndef calculate_ndvi(nir, red):\n    \"\"\"Calculate Normalized Difference Vegetation Index\"\"\"\n    return (nir - red) / (nir + red + 1e-8)  # Add small value to avoid division by zero\n\ndef calculate_ndwi(green, nir):\n    \"\"\"Calculate Normalized Difference Water Index\"\"\"\n    return (green - nir) / (green + nir + 1e-8)\n\ndef calculate_nbr(nir, swir2):\n    \"\"\"Calculate Normalized Burn Ratio\"\"\"\n    return (nir - swir2) / (nir + swir2 + 1e-8)\n\ndef plot_spectral_indices(bands, figsize=(15, 5)):\n    \"\"\"Plot common spectral indices\"\"\"\n    \n    # Calculate indices\n    ndvi = calculate_ndvi(bands['nir'], bands['red'])\n    ndwi = calculate_ndwi(bands['green'], bands['nir']) \n    nbr = calculate_nbr(bands['nir'], bands['swir2'])\n    \n    # Create subplots\n    fig, axes = plt.subplots(1, 3, figsize=figsize)\n    \n    # NDVI plot\n    im1 = axes[0].imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1, aspect='equal')\n    axes[0].set_title('NDVI\\n(Vegetation Index)', fontsize=12, fontweight='bold')\n    axes[0].tick_params(labelbottom=False, labelleft=False)\n    cbar1 = plt.colorbar(im1, ax=axes[0], shrink=0.8)\n    cbar1.set_label('NDVI', fontsize=10)\n    \n    # NDWI plot\n    im2 = axes[1].imshow(ndwi, cmap='Blues', vmin=-1, vmax=1, aspect='equal')\n    axes[1].set_title('NDWI\\n(Water Index)', fontsize=12, fontweight='bold')\n    axes[1].tick_params(labelbottom=False, labelleft=False)\n    cbar2 = plt.colorbar(im2, ax=axes[1], shrink=0.8)\n    cbar2.set_label('NDWI', fontsize=10)\n    \n    # NBR plot\n    im3 = axes[2].imshow(nbr, cmap='RdYlBu_r', vmin=-1, vmax=1, aspect='equal')\n    axes[2].set_title('NBR\\n(Burn Ratio)', fontsize=12, fontweight='bold')\n    axes[2].tick_params(labelbottom=False, labelleft=False)\n    cbar3 = plt.colorbar(im3, ax=axes[2], shrink=0.8)\n    cbar3.set_label('NBR', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {'ndvi': ndvi, 'ndwi': ndwi, 'nbr': nbr}\n\n# Plot spectral indices\nindices = plot_spectral_indices(bands)\n\n\n\n\n\n\n\n\n\n\nThematic classification visualization\n\ndef create_landcover_classification(indices, rgb_composite):\n    \"\"\"Create simple land cover classification\"\"\"\n    \n    height, width = indices['ndvi'].shape\n    landcover = np.zeros((height, width), dtype=np.uint8)\n    \n    # Classification rules (simplified)\n    # 1 = Water, 2 = Vegetation, 3 = Urban/Built-up, 4 = Bare soil\n    \n    # Water (high NDWI, low NDVI)\n    water_mask = (indices['ndwi'] &gt; 0.3) & (indices['ndvi'] &lt; 0.1)\n    landcover[water_mask] = 1\n    \n    # Vegetation (high NDVI)\n    veg_mask = (indices['ndvi'] &gt; 0.3) & ~water_mask\n    landcover[veg_mask] = 2\n    \n    # Urban/Built-up (low NDVI, moderate brightness)\n    urban_mask = (indices['ndvi'] &lt; 0.1) & (rgb_composite.mean(axis=2) &gt; 0.3) & ~water_mask\n    landcover[urban_mask] = 3\n    \n    # Bare soil (everything else)\n    bare_mask = (landcover == 0)\n    landcover[bare_mask] = 4\n    \n    return landcover\n\ndef plot_classification_results(landcover, rgb_composite, figsize=(15, 6)):\n    \"\"\"Plot classification results alongside RGB\"\"\"\n    \n    # Define colors and labels for classes\n    colors = ['black', 'blue', 'green', 'red', 'brown']\n    labels = ['Background', 'Water', 'Vegetation', 'Urban', 'Bare Soil']\n    \n    # Create custom colormap\n    from matplotlib.colors import ListedColormap\n    cmap = ListedColormap(colors)\n    \n    fig, axes = plt.subplots(1, 2, figsize=figsize)\n    \n    # RGB composite\n    axes[0].imshow(rgb_composite, aspect='equal')\n    axes[0].set_title('RGB Composite', fontsize=14, fontweight='bold')\n    axes[0].tick_params(labelbottom=False, labelleft=False)\n    \n    # Classification\n    im = axes[1].imshow(landcover, cmap=cmap, vmin=0, vmax=4, aspect='equal')\n    axes[1].set_title('Land Cover Classification', fontsize=14, fontweight='bold')\n    axes[1].tick_params(labelbottom=False, labelleft=False)\n    \n    # Create custom legend\n    import matplotlib.patches as mpatches\n    legend_patches = [mpatches.Patch(color=colors[i], label=labels[i]) \n                     for i in range(1, len(colors))]\n    axes[1].legend(handles=legend_patches, loc='upper right', bbox_to_anchor=(1.3, 1))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print class statistics\n    unique, counts = np.unique(landcover, return_counts=True)\n    total_pixels = landcover.size\n    \n    print(\"Land Cover Statistics:\")\n    for class_id, count in zip(unique, counts):\n        if class_id &gt; 0:  # Skip background\n            percentage = (count / total_pixels) * 100\n            print(f\"{labels[class_id]}: {count:,} pixels ({percentage:.1f}%)\")\n    \n    return fig, axes\n\n# Create and plot classification\nindices = plot_spectral_indices(bands)  # Re-calculate for consistency\nlandcover = create_landcover_classification(indices, rgb_composite)\nplot_classification_results(landcover, rgb_composite)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLand Cover Statistics:\nWater: 53,449 pixels (20.4%)\nVegetation: 106,221 pixels (40.5%)\nUrban: 50,542 pixels (19.3%)\nBare Soil: 51,932 pixels (19.8%)\n\n\n(&lt;Figure size 1440x576 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'RGB Composite'}&gt;,\n        &lt;Axes: title={'center': 'Land Cover Classification'}&gt;],\n       dtype=object))"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#cartographic-projections-with-cartopy",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#cartographic-projections-with-cartopy",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "Cartographic Projections with Cartopy",
    "text": "Cartographic Projections with Cartopy\n\nBasic map projections\n\ndef plot_different_projections(figsize=(15, 10)):\n    \"\"\"Demonstrate different map projections\"\"\"\n    \n    # Sample geographic data (simulate satellite coverage)\n    lons = np.linspace(-180, 180, 100)\n    lats = np.linspace(-90, 90, 50)\n    lon_grid, lat_grid = np.meshgrid(lons, lats)\n    \n    # Sample data (e.g., temperature, vegetation)\n    data = np.sin(np.radians(lat_grid)) * np.cos(np.radians(lon_grid * 2)) + \\\n           0.3 * np.random.randn(*lat_grid.shape)\n    \n    # Different projections\n    projections = [\n        ('PlateCarree', ccrs.PlateCarree()),\n        ('Mollweide', ccrs.Mollweide()),\n        ('Robinson', ccrs.Robinson()),\n        ('Orthographic', ccrs.Orthographic(central_longitude=0, central_latitude=45))\n    ]\n    \n    fig = plt.figure(figsize=figsize)\n    \n    for i, (name, proj) in enumerate(projections):\n        ax = fig.add_subplot(2, 2, i + 1, projection=proj)\n        \n        # Add map features\n        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n        ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n        ax.add_feature(cfeature.OCEAN, color='lightblue', alpha=0.5)\n        ax.add_feature(cfeature.LAND, color='lightgray', alpha=0.5)\n        \n        # Plot data using imshow (more stable with projections)\n        im = ax.imshow(data, extent=[-180, 180, -90, 90],\n                      transform=ccrs.PlateCarree(),\n                      cmap='RdYlBu_r', alpha=0.7, origin='lower')\n        \n        # Add gridlines\n        ax.gridlines(draw_labels=True if name == 'PlateCarree' else False,\n                    dms=True, x_inline=False, y_inline=False)\n        \n        ax.set_title(f'{name} Projection', fontsize=12, fontweight='bold')\n        \n        # Add colorbar for the last subplot\n        if i == len(projections) - 1:\n            cbar = plt.colorbar(im, ax=ax, shrink=0.5, orientation='horizontal', pad=0.05)\n            cbar.set_label('Sample Data', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig\n\n# Plot different projections\nplot_different_projections()\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n\n\n\n\n\n\n\n\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n\n\n\n\n\n\n\n\n\n\n\nRegional focus maps\n\ndef plot_regional_satellite_data(center_lon=0, center_lat=45, extent=20, figsize=(12, 8)):\n    \"\"\"Plot regional satellite data with geographic context\"\"\"\n    \n    # Define region bounds\n    west = center_lon - extent/2\n    east = center_lon + extent/2  \n    south = center_lat - extent/2\n    north = center_lat + extent/2\n    \n    # Create synthetic satellite data for the region\n    lons = np.linspace(west, east, 200)\n    lats = np.linspace(south, north, 150)\n    lon_grid, lat_grid = np.meshgrid(lons, lats)\n    \n    # Simulate NDVI data\n    ndvi_data = 0.6 * np.sin(np.radians(lat_grid * 4)) + \\\n                0.3 * np.cos(np.radians(lon_grid * 3)) + \\\n                0.2 * np.random.randn(*lat_grid.shape)\n    ndvi_data = np.clip(ndvi_data, -1, 1)\n    \n    # Create map\n    fig = plt.figure(figsize=figsize)\n    ax = plt.axes(projection=ccrs.PlateCarree())\n    \n    # Set extent\n    ax.set_extent([west, east, south, north], ccrs.PlateCarree())\n    \n    # Add geographic features\n    ax.add_feature(cfeature.COASTLINE, linewidth=1)\n    ax.add_feature(cfeature.BORDERS, linewidth=0.8)\n    ax.add_feature(cfeature.RIVERS, linewidth=0.5, color='blue')\n    ax.add_feature(cfeature.LAKES, color='lightblue')\n    \n    # Plot NDVI data\n    im = ax.imshow(ndvi_data, extent=[west, east, south, north],\n                   transform=ccrs.PlateCarree(), cmap='RdYlGn',\n                   vmin=-1, vmax=1, alpha=0.8)\n    \n    # Add gridlines and labels\n    gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n    gl.top_labels = False\n    gl.right_labels = False\n    \n    # Add colorbar\n    cbar = plt.colorbar(im, ax=ax, shrink=0.7, orientation='vertical')\n    cbar.set_label('NDVI', fontsize=12)\n    \n    # Add title\n    ax.set_title(f'Regional NDVI Data\\n({south:.1f}¬∞-{north:.1f}¬∞N, {west:.1f}¬∞-{east:.1f}¬∞E)', \n                fontsize=14, fontweight='bold', pad=20)\n    \n    # Add scale bar and north arrow\n    # Scale bar (approximate)\n    scale_x = west + (east - west) * 0.7\n    scale_y = south + (north - south) * 0.1\n    ax.plot([scale_x, scale_x + 2], [scale_y, scale_y], \n           'k-', linewidth=3, transform=ccrs.PlateCarree())\n    ax.text(scale_x + 1, scale_y - 0.5, '200 km', \n           ha='center', va='top', fontsize=10, fontweight='bold',\n           transform=ccrs.PlateCarree())\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig, ax\n\n# Plot regional data for different areas\nplot_regional_satellite_data(center_lon=-100, center_lat=40, extent=15)  # US Great Plains\nplot_regional_satellite_data(center_lon=25, center_lat=-15, extent=20)   # Southern Africa\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning:\n\nfacecolor will have no effect as it has been defined as \"never\".\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/io/__init__.py:242: DownloadWarning:\n\nDownloading: https://naturalearth.s3.amazonaws.com/10m_physical/ne_10m_rivers_lake_centerlines.zip\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/io/__init__.py:242: DownloadWarning:\n\nDownloading: https://naturalearth.s3.amazonaws.com/10m_physical/ne_10m_lakes.zip\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/io/__init__.py:242: DownloadWarning:\n\nDownloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/io/__init__.py:242: DownloadWarning:\n\nDownloading: https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_0_boundary_lines_land.zip\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/io/__init__.py:242: DownloadWarning:\n\nDownloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_rivers_lake_centerlines.zip\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/io/__init__.py:242: DownloadWarning:\n\nDownloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_lakes.zip\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n\n\n\n\n\n\n\n\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/cartopy/mpl/feature_artist.py:143: UserWarning:\n\nfacecolor will have no effect as it has been defined as \"never\".\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/shapely/creation.py:730: RuntimeWarning:\n\ninvalid value encountered in create_collection\n\n\n\n\n\n\n\n\n\n\n(&lt;Figure size 1152x768 with 2 Axes&gt;,\n &lt;GeoAxes: title={'center': 'Regional NDVI Data\\n(-25.0¬∞--5.0¬∞N, 15.0¬∞-35.0¬∞E)'}&gt;)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#multi-panel-complex-layouts",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#multi-panel-complex-layouts",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "Multi-panel Complex Layouts",
    "text": "Multi-panel Complex Layouts\n\nDashboard-style visualization\n\ndef create_satellite_dashboard(bands, indices, figsize=(16, 12)):\n    \"\"\"Create a comprehensive satellite data dashboard\"\"\"\n    \n    # Create custom grid layout\n    fig = plt.figure(figsize=figsize)\n    gs = gridspec.GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)\n    \n    # Large RGB composite (top left, 2x2)\n    ax_rgb = fig.add_subplot(gs[0:2, 0:2])\n    rgb_composite = create_rgb_composite(bands['red'], bands['green'], bands['blue'])\n    ax_rgb.imshow(rgb_composite, aspect='equal')\n    ax_rgb.set_title('True Color RGB', fontsize=14, fontweight='bold')\n    ax_rgb.tick_params(labelbottom=False, labelleft=False)\n    \n    # Individual band plots (top right)\n    band_axes = []\n    band_list = ['red', 'green', 'nir']\n    cmaps = ['Reds', 'Greens', 'RdYlGn']\n    \n    for i, (band_name, cmap) in enumerate(zip(band_list, cmaps)):\n        ax = fig.add_subplot(gs[i, 2])\n        im = ax.imshow(bands[band_name], cmap=cmap, aspect='equal')\n        ax.set_title(band_name.upper(), fontsize=12, fontweight='bold')\n        ax.tick_params(labelbottom=False, labelleft=False, labelsize=8)\n        \n        # Small colorbar\n        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n        cbar.ax.tick_params(labelsize=8)\n    \n    # NDVI (top right, bottom)\n    ax_ndvi = fig.add_subplot(gs[0, 3])\n    im_ndvi = ax_ndvi.imshow(indices['ndvi'], cmap='RdYlGn', vmin=-1, vmax=1, aspect='equal')\n    ax_ndvi.set_title('NDVI', fontsize=12, fontweight='bold')\n    ax_ndvi.tick_params(labelbottom=False, labelleft=False, labelsize=8)\n    cbar_ndvi = plt.colorbar(im_ndvi, ax=ax_ndvi, shrink=0.8)\n    cbar_ndvi.ax.tick_params(labelsize=8)\n    \n    # Histogram (middle right)\n    ax_hist = fig.add_subplot(gs[1, 3])\n    ax_hist.hist(indices['ndvi'].flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n    ax_hist.set_title('NDVI Histogram', fontsize=12, fontweight='bold')\n    ax_hist.set_xlabel('NDVI Value', fontsize=10)\n    ax_hist.set_ylabel('Frequency', fontsize=10)\n    ax_hist.grid(True, alpha=0.3)\n    ax_hist.tick_params(labelsize=8)\n    \n    # Scatter plot (bottom left)\n    ax_scatter = fig.add_subplot(gs[2, 0])\n    scatter_data = ax_scatter.scatter(bands['red'].flatten(), bands['nir'].flatten(), \n                                    c=indices['ndvi'].flatten(), cmap='RdYlGn', \n                                    alpha=0.5, s=1)\n    ax_scatter.set_xlabel('Red Reflectance', fontsize=10)\n    ax_scatter.set_ylabel('NIR Reflectance', fontsize=10)\n    ax_scatter.set_title('Red vs NIR\\n(colored by NDVI)', fontsize=12, fontweight='bold')\n    ax_scatter.grid(True, alpha=0.3)\n    ax_scatter.tick_params(labelsize=8)\n    \n    # Classification (bottom center)\n    ax_class = fig.add_subplot(gs[2, 1])\n    landcover = create_landcover_classification(indices, rgb_composite)\n    colors = ['blue', 'green', 'red', 'brown']\n    labels = ['Water', 'Vegetation', 'Urban', 'Bare Soil']\n    from matplotlib.colors import ListedColormap\n    cmap_class = ListedColormap(colors)\n    im_class = ax_class.imshow(landcover, cmap=cmap_class, vmin=1, vmax=4, aspect='equal')\n    ax_class.set_title('Land Cover\\nClassification', fontsize=12, fontweight='bold')\n    ax_class.tick_params(labelbottom=False, labelleft=False)\n    \n    # Statistics table (bottom right)\n    ax_stats = fig.add_subplot(gs[2, 2:4])\n    ax_stats.axis('off')\n    \n    # Calculate statistics\n    stats_data = [\n        ['Band', 'Mean', 'Std', 'Min', 'Max'],\n        ['Red', f'{bands[\"red\"].mean():.3f}', f'{bands[\"red\"].std():.3f}', \n         f'{bands[\"red\"].min():.3f}', f'{bands[\"red\"].max():.3f}'],\n        ['Green', f'{bands[\"green\"].mean():.3f}', f'{bands[\"green\"].std():.3f}', \n         f'{bands[\"green\"].min():.3f}', f'{bands[\"green\"].max():.3f}'],\n        ['NIR', f'{bands[\"nir\"].mean():.3f}', f'{bands[\"nir\"].std():.3f}', \n         f'{bands[\"nir\"].min():.3f}', f'{bands[\"nir\"].max():.3f}'],\n        ['NDVI', f'{indices[\"ndvi\"].mean():.3f}', f'{indices[\"ndvi\"].std():.3f}', \n         f'{indices[\"ndvi\"].min():.3f}', f'{indices[\"ndvi\"].max():.3f}']\n    ]\n    \n    table = ax_stats.table(cellText=stats_data, loc='center', cellLoc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1, 1.5)\n    ax_stats.set_title('Band Statistics', fontsize=12, fontweight='bold', pad=20)\n    \n    # Main title\n    fig.suptitle('Satellite Imagery Analysis Dashboard', fontsize=18, fontweight='bold', y=0.95)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig\n\n# Create dashboard\ndashboard_fig = create_satellite_dashboard(bands, indices)\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_55845/2054560415.py:95: UserWarning:\n\nThis figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n\n\n\n\n\n\n\n\n\n\n\n\nTime series visualization\n\ndef create_time_series_plot(figsize=(15, 10)):\n    \"\"\"Create time series visualization of satellite indices\"\"\"\n    \n    # Generate synthetic time series data\n    dates = pd.date_range('2020-01-01', '2020-12-31', freq='16D')  # Landsat revisit\n    n_dates = len(dates)\n    \n    # Simulate seasonal NDVI pattern\n    day_of_year = np.array([d.timetuple().tm_yday for d in dates])\n    base_ndvi = 0.3 + 0.4 * np.sin(2 * np.pi * (day_of_year - 120) / 365)\n    \n    # Add noise and random events\n    np.random.seed(42)\n    ndvi_series = base_ndvi + 0.1 * np.random.randn(n_dates)\n    \n    # Simulate drought event (reduce NDVI mid-year)\n    drought_mask = (day_of_year &gt; 180) & (day_of_year &lt; 240)\n    ndvi_series[drought_mask] -= 0.2\n    \n    # Create other indices\n    evi_series = ndvi_series * 1.2 + 0.1 * np.random.randn(n_dates)\n    nbr_series = ndvi_series * 0.8 + 0.15 * np.random.randn(n_dates)\n    \n    # Simulate fire event (drop in NBR)\n    fire_date = np.where(day_of_year &gt; 200)[0][0]\n    nbr_series[fire_date:fire_date+3] -= 0.6\n    \n    # Create multi-panel time series plot\n    fig, axes = plt.subplots(3, 1, figsize=figsize, sharex=True)\n    \n    # NDVI plot\n    axes[0].plot(dates, ndvi_series, 'o-', color='green', linewidth=2, markersize=4)\n    axes[0].fill_between(dates, ndvi_series, alpha=0.3, color='green')\n    axes[0].set_ylabel('NDVI', fontsize=12, fontweight='bold')\n    axes[0].set_title('Vegetation Index Time Series', fontsize=14, fontweight='bold')\n    axes[0].grid(True, alpha=0.3)\n    axes[0].set_ylim(-0.2, 0.8)\n    \n    # Mark drought period\n    drought_start = dates[drought_mask][0]\n    drought_end = dates[drought_mask][-1]\n    axes[0].axvspan(drought_start, drought_end, alpha=0.2, color='red', label='Drought Period')\n    axes[0].legend()\n    \n    # EVI plot\n    axes[1].plot(dates, evi_series, 'o-', color='darkgreen', linewidth=2, markersize=4)\n    axes[1].fill_between(dates, evi_series, alpha=0.3, color='darkgreen')\n    axes[1].set_ylabel('EVI', fontsize=12, fontweight='bold')\n    axes[1].grid(True, alpha=0.3)\n    axes[1].set_ylim(-0.2, 1.0)\n    \n    # NBR plot\n    axes[2].plot(dates, nbr_series, 'o-', color='brown', linewidth=2, markersize=4)\n    axes[2].fill_between(dates, nbr_series, alpha=0.3, color='brown')\n    axes[2].set_ylabel('NBR', fontsize=12, fontweight='bold')\n    axes[2].set_xlabel('Date', fontsize=12, fontweight='bold')\n    axes[2].grid(True, alpha=0.3)\n    \n    # Mark fire event\n    fire_date_actual = dates[fire_date]\n    axes[2].axvline(x=fire_date_actual, color='red', linestyle='--', linewidth=2, label='Fire Event')\n    axes[2].legend()\n    \n    # Format x-axis\n    import matplotlib.dates as mdates\n    axes[2].xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n    axes[2].xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n    plt.setp(axes[2].xaxis.get_majorticklabels(), rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig, axes\n\n# Import pandas for date handling\nimport pandas as pd\n\n# Create time series plot\nts_fig, ts_axes = create_time_series_plot()"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#publication-ready-styling",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#publication-ready-styling",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "Publication-Ready Styling",
    "text": "Publication-Ready Styling\n\nProfessional styling\n\ndef create_publication_figure(bands, indices, figsize=(12, 8)):\n    \"\"\"Create publication-ready figure with professional styling\"\"\"\n    \n    # Set publication style\n    plt.rcParams.update({\n        'font.family': 'serif',\n        'font.size': 10,\n        'axes.linewidth': 0.8,\n        'axes.spines.top': False,\n        'axes.spines.right': False,\n        'xtick.direction': 'inout',\n        'ytick.direction': 'inout',\n        'figure.dpi': 300\n    })\n    \n    fig, axes = plt.subplots(2, 3, figsize=figsize)\n    \n    # A) RGB Composite\n    rgb = create_rgb_composite(bands['red'], bands['green'], bands['blue'])\n    axes[0, 0].imshow(rgb, aspect='equal')\n    axes[0, 0].set_title('A) RGB Composite', fontsize=11, fontweight='bold', loc='left')\n    axes[0, 0].tick_params(labelbottom=False, labelleft=False)\n    \n    # Add scale bar\n    scale_bar = Rectangle((rgb.shape[1] - 80, rgb.shape[0] - 25), 60, 8, \n                         facecolor='white', edgecolor='black', linewidth=0.8)\n    axes[0, 0].add_patch(scale_bar)\n    axes[0, 0].text(rgb.shape[1] - 50, rgb.shape[0] - 35, '1 km', \n                   ha='center', va='top', fontsize=8, fontweight='bold')\n    \n    # B) False Color\n    false_color = create_rgb_composite(bands['nir'], bands['red'], bands['green'])\n    axes[0, 1].imshow(false_color, aspect='equal')\n    axes[0, 1].set_title('B) False Color (NIR-R-G)', fontsize=11, fontweight='bold', loc='left')\n    axes[0, 1].tick_params(labelbottom=False, labelleft=False)\n    \n    # C) NDVI\n    im_ndvi = axes[0, 2].imshow(indices['ndvi'], cmap='RdYlGn', vmin=-1, vmax=1, aspect='equal')\n    axes[0, 2].set_title('C) NDVI', fontsize=11, fontweight='bold', loc='left')\n    axes[0, 2].tick_params(labelbottom=False, labelleft=False)\n    \n    # Add NDVI colorbar\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n    divider = make_axes_locatable(axes[0, 2])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    cbar = plt.colorbar(im_ndvi, cax=cax)\n    cbar.set_label('NDVI', fontsize=9)\n    cbar.ax.tick_params(labelsize=8)\n    \n    # D) Land Cover Classification\n    landcover = create_landcover_classification(indices, rgb)\n    colors = ['#0066CC', '#00AA00', '#CC0000', '#996633']  # Professional colors\n    labels = ['Water', 'Vegetation', 'Urban', 'Bare Soil']\n    from matplotlib.colors import ListedColormap\n    cmap_prof = ListedColormap(colors)\n    \n    im_class = axes[1, 0].imshow(landcover, cmap=cmap_prof, vmin=1, vmax=4, aspect='equal')\n    axes[1, 0].set_title('D) Land Cover Classification', fontsize=11, fontweight='bold', loc='left')\n    axes[1, 0].tick_params(labelbottom=False, labelleft=False)\n    \n    # Add classification legend\n    import matplotlib.patches as mpatches\n    legend_patches = [mpatches.Patch(color=colors[i], label=labels[i]) \n                     for i in range(len(colors))]\n    axes[1, 0].legend(handles=legend_patches, loc='upper left', \n                     bbox_to_anchor=(0.02, 0.98), fontsize=8, frameon=True, fancybox=False)\n    \n    # E) Spectral Profiles\n    axes[1, 1].axis('off')  # Remove axes\n    ax_profiles = fig.add_subplot(2, 3, 5)  # Add back with different approach\n    \n    # Sample points from different land cover types\n    water_pts = np.where(landcover == 1)\n    veg_pts = np.where(landcover == 2)\n    urban_pts = np.where(landcover == 3)\n    \n    # Extract spectral profiles\n    band_names = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']\n    wavelengths = [0.48, 0.56, 0.66, 0.83, 1.65, 2.22]  # Approximate wavelengths (¬µm)\n    \n    # Calculate mean reflectance for each class\n    water_profile = [bands['blue'][water_pts].mean(), bands['green'][water_pts].mean(), \n                    bands['red'][water_pts].mean(), bands['nir'][water_pts].mean(),\n                    bands['swir1'][water_pts].mean(), bands['swir2'][water_pts].mean()]\n    \n    veg_profile = [bands['blue'][veg_pts].mean(), bands['green'][veg_pts].mean(),\n                  bands['red'][veg_pts].mean(), bands['nir'][veg_pts].mean(),\n                  bands['swir1'][veg_pts].mean(), bands['swir2'][veg_pts].mean()]\n    \n    urban_profile = [bands['blue'][urban_pts].mean(), bands['green'][urban_pts].mean(),\n                    bands['red'][urban_pts].mean(), bands['nir'][urban_pts].mean(),\n                    bands['swir1'][urban_pts].mean(), bands['swir2'][urban_pts].mean()]\n    \n    ax_profiles.plot(wavelengths, water_profile, 'o-', color='#0066CC', linewidth=2, \n                    label='Water', markersize=6)\n    ax_profiles.plot(wavelengths, veg_profile, 's-', color='#00AA00', linewidth=2, \n                    label='Vegetation', markersize=6)\n    ax_profiles.plot(wavelengths, urban_profile, '^-', color='#CC0000', linewidth=2, \n                    label='Urban', markersize=6)\n    \n    ax_profiles.set_xlabel('Wavelength (Œºm)', fontsize=10)\n    ax_profiles.set_ylabel('Reflectance', fontsize=10)\n    ax_profiles.set_title('E) Spectral Profiles', fontsize=11, fontweight='bold', loc='left')\n    ax_profiles.legend(fontsize=8, frameon=False)\n    ax_profiles.grid(True, alpha=0.3, linewidth=0.5)\n    ax_profiles.tick_params(labelsize=8)\n    \n    # F) Statistics/Summary\n    axes[1, 2].axis('off')\n    \n    # Create summary statistics text\n    stats_text = f\"\"\"F) Summary Statistics\n    \nImage Dimensions: {rgb.shape[0]} √ó {rgb.shape[1]} pixels\nSpatial Resolution: 30 m\n    \nLand Cover Distribution:\nWater: {(landcover == 1).sum() / landcover.size * 100:.1f}%\nVegetation: {(landcover == 2).sum() / landcover.size * 100:.1f}%\nUrban: {(landcover == 3).sum() / landcover.size * 100:.1f}%\nBare Soil: {(landcover == 4).sum() / landcover.size * 100:.1f}%\n\nNDVI Statistics:\nMean: {indices['ndvi'].mean():.3f}\nStd: {indices['ndvi'].std():.3f}\nRange: [{indices['ndvi'].min():.3f}, {indices['ndvi'].max():.3f}]\"\"\"\n    \n    axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes, \n                   fontsize=9, verticalalignment='top', fontfamily='monospace',\n                   bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Reset rcParams\n    plt.rcParams.update(plt.rcParamsDefault)\n    \n    return fig\n\n# Create publication figure\npub_fig = create_publication_figure(bands, indices)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#custom-colormaps-and-advanced-styling",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#custom-colormaps-and-advanced-styling",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "Custom Colormaps and Advanced Styling",
    "text": "Custom Colormaps and Advanced Styling\n\nCustom colormap creation\n\ndef create_custom_colormaps():\n    \"\"\"Create custom colormaps for different geospatial applications\"\"\"\n    \n    # Custom NDVI colormap (brown to green)\n    ndvi_colors = ['#8B4513', '#CD853F', '#F4A460', '#FFFFE0', '#90EE90', '#32CD32', '#006400']\n    ndvi_cmap = LinearSegmentedColormap.from_list('custom_ndvi', ndvi_colors, N=256)\n    \n    # Custom water depth colormap\n    water_colors = ['#000080', '#0066CC', '#00AAFF', '#66CCFF', '#CCE5FF']\n    water_cmap = LinearSegmentedColormap.from_list('water_depth', water_colors, N=256)\n    \n    # Custom elevation colormap\n    elev_colors = ['#2E8B57', '#90EE90', '#FFFFE0', '#CD853F', '#8B4513', '#FFFFFF']\n    elev_cmap = LinearSegmentedColormap.from_list('elevation', elev_colors, N=256)\n    \n    return {\n        'ndvi': ndvi_cmap,\n        'water': water_cmap,\n        'elevation': elev_cmap\n    }\n\ndef demonstrate_custom_colormaps(bands, indices, figsize=(15, 5)):\n    \"\"\"Demonstrate custom colormaps\"\"\"\n    \n    custom_cmaps = create_custom_colormaps()\n    \n    fig, axes = plt.subplots(1, 3, figsize=figsize)\n    \n    # NDVI with custom colormap\n    im1 = axes[0].imshow(indices['ndvi'], cmap=custom_cmaps['ndvi'], \n                        vmin=-1, vmax=1, aspect='equal')\n    axes[0].set_title('NDVI - Custom Colormap', fontsize=12, fontweight='bold')\n    axes[0].tick_params(labelbottom=False, labelleft=False)\n    cbar1 = plt.colorbar(im1, ax=axes[0], shrink=0.8)\n    cbar1.set_label('NDVI', fontsize=10)\n    \n    # Simulated water depth\n    water_depth = indices['ndwi'] * 5  # Scale for visualization\n    water_depth[water_depth &lt; 0] = 0\n    \n    im2 = axes[1].imshow(water_depth, cmap=custom_cmaps['water'], \n                        vmin=0, vmax=water_depth.max(), aspect='equal')\n    axes[1].set_title('Water Depth - Custom Colormap', fontsize=12, fontweight='bold')\n    axes[1].tick_params(labelbottom=False, labelleft=False)\n    cbar2 = plt.colorbar(im2, ax=axes[1], shrink=0.8)\n    cbar2.set_label('Depth (m)', fontsize=10)\n    \n    # Simulated elevation\n    elevation = (bands['nir'] - bands['blue']) * 1000 + 500  # Simulate elevation\n    \n    im3 = axes[2].imshow(elevation, cmap=custom_cmaps['elevation'], aspect='equal')\n    axes[2].set_title('Elevation - Custom Colormap', fontsize=12, fontweight='bold')\n    axes[2].tick_params(labelbottom=False, labelleft=False)\n    cbar3 = plt.colorbar(im3, ax=axes[2], shrink=0.8)\n    cbar3.set_label('Elevation (m)', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fig\n\n# Demonstrate custom colormaps\ncustom_cmap_fig = demonstrate_custom_colormaps(bands, indices)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#summary",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html#summary",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "Summary",
    "text": "Summary\nKey matplotlib techniques for geospatial visualization: - Basic plotting: Single and multi-band satellite imagery display - RGB composites: True color and false color combinations - Advanced visualization: Spectral indices, classification, and thematic mapping - Cartographic projections: Using Cartopy for geographic context - Multi-panel layouts: Dashboard-style and publication-ready figures - Time series: Temporal analysis of satellite data - Custom styling: Professional colormaps and publication formatting - Interactive elements: Annotations, scale bars, and legends"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html",
    "title": "Foundation Model Architectures",
    "section": "",
    "text": "Foundation models represent a paradigm shift in machine learning, where large-scale models are trained on diverse, unlabeled data and then adapted to various downstream tasks. The term was coined by the Stanford HAI Center to describe models like GPT-3, BERT, and CLIP that serve as a ‚Äúfoundation‚Äù for numerous applications.\nThis cheatsheet provides a comprehensive comparison between Language Models (LLMs) and Geospatial Foundation Models (GFMs), examining their architectures, training procedures, and deployment strategies. While LLMs process discrete text tokens, GFMs work with continuous multi-spectral imagery, requiring fundamentally different approaches to tokenization, positional encoding, and objective functions.\nKey Resources:\n\nAttention Is All You Need - Original Transformer paper\nAn Image is Worth 16x16 Words - Vision Transformer (ViT)\nMasked Autoencoders Are Scalable Vision Learners - MAE approach\nPrithvi Foundation Model - IBM/NASA geospatial model\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModel, AutoConfig\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")  # Always False on Mac\n\nmps_available = torch.backends.mps.is_available()\nprint(f\"MPS available: {mps_available}\")\n\nif mps_available:\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n\nPyTorch version: 2.7.1\nCUDA available: False\nMPS available: True\nUsing device: mps"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#introduction-to-foundation-model-architectures",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#introduction-to-foundation-model-architectures",
    "title": "Foundation Model Architectures",
    "section": "",
    "text": "Foundation models represent a paradigm shift in machine learning, where large-scale models are trained on diverse, unlabeled data and then adapted to various downstream tasks. The term was coined by the Stanford HAI Center to describe models like GPT-3, BERT, and CLIP that serve as a ‚Äúfoundation‚Äù for numerous applications.\nThis cheatsheet provides a comprehensive comparison between Language Models (LLMs) and Geospatial Foundation Models (GFMs), examining their architectures, training procedures, and deployment strategies. While LLMs process discrete text tokens, GFMs work with continuous multi-spectral imagery, requiring fundamentally different approaches to tokenization, positional encoding, and objective functions.\nKey Resources:\n\nAttention Is All You Need - Original Transformer paper\nAn Image is Worth 16x16 Words - Vision Transformer (ViT)\nMasked Autoencoders Are Scalable Vision Learners - MAE approach\nPrithvi Foundation Model - IBM/NASA geospatial model\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModel, AutoConfig\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")  # Always False on Mac\n\nmps_available = torch.backends.mps.is_available()\nprint(f\"MPS available: {mps_available}\")\n\nif mps_available:\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n\nPyTorch version: 2.7.1\nCUDA available: False\nMPS available: True\nUsing device: mps"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#evolution-from-ai-to-transformers",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#evolution-from-ai-to-transformers",
    "title": "Foundation Model Architectures",
    "section": "Evolution from AI to Transformers",
    "text": "Evolution from AI to Transformers\nThe development of foundation models represents a convergence of several key technological breakthroughs. The journey from symbolic AI to modern foundation models reflects both advances in computational power and fundamental insights about learning representations from large-scale data.\n\nKey Historical Milestones\nThe transformer architecture emerged from decades of research in neural networks, attention mechanisms, and sequence modeling. The 2017 paper ‚ÄúAttention Is All You Need‚Äù introduced the transformer, which became the foundation for both language and vision models.\nCritical Developments:\n\n1950s-1990s: Symbolic AI dominated, with rule-based systems and early neural networks\n2012: AlexNet‚Äôs ImageNet victory sparked the deep learning revolution\n2017: The Transformer architecture introduced self-attention and parallelizable training\n2018-2020: BERT and GPT families demonstrated the power of pre-trained language models\n2021-2024: Scaling laws, ChatGPT, and multimodal models like CLIP\n\nFoundation Model Evolution Timeline:\n\n2017 - Transformer: Base architecture introduced\n2018 - BERT-Base: 110M parameters\n2019 - GPT-2: 1.5B parameters\n\n2020 - GPT-3: 175B parameters\n2021 - ViT-Large: 307M parameters for vision\n2022 - PaLM: 540B parameters\n2023 - GPT-4: ~1T parameters (estimated)\n2024 - Claude-3: Multi-modal capabilities\n\n\n\nTransformer Architecture Essentials\nThe transformer architecture revolutionized deep learning by introducing the self-attention mechanism, which allows models to process sequences in parallel rather than sequentially. This architecture forms the backbone of both language models (GPT, BERT) and vision models (ViT, DINO).\nCore Components:\n\nMulti-Head Attention: Allows the model to attend to different representation subspaces (see Self-attention)\nFeed-Forward Networks: Point-wise processing with GELU activation (see Multilayer perceptron)\nResidual Connections: Enable training of very deep networks (see Residual neural network)\nLayer Normalization: Stabilizes training and improves convergence (see Layer normalization)\n\nKey terms (quick definitions):\n\nSelf-Attention (SA): A mechanism where each token attends to every other token (including itself) to compute a weighted combination of their representations.\nQueries (Q), Keys (K), Values (V): Learned linear projections of the input. Attention scores are computed from Q¬∑K·µÄ; those scores weight V to produce the output (see Attention (machine learning)).\nMulti-Head Attention (MHA): Runs several SA ‚Äúheads‚Äù in parallel on different subspaces, then concatenates and projects the results. This lets the model capture diverse relationships (see Self-attention).\nFeed-Forward Network (FFN/MLP): A small two-layer MLP (Multi-Layer Perceptron) applied to each token independently; expands the dimension then projects back (often with GELU activation) (see Multilayer perceptron).\nResidual Connection (Skip): Adds the input of a sub-layer to its output, helping gradients flow and preserving information (see Residual neural network).\nLayer Normalization (LayerNorm, LN): Normalizes features per token to stabilize training. The ordering with residuals defines Post-LN vs Pre-LN variants (see Layer normalization).\nGELU: Gaussian Error Linear Unit activation; smoother than ReLU and standard in transformers (see Gaussian error linear unit).\nTensor shape convention: Using batch_first=True, tensors are [batch_size, seq_len, embed_dim].\nAbbreviations used: B = batch size, S = sequence length (or number of tokens/patches), E = embedding dimension, H = number of attention heads.\n\nThe following implementation demonstrates a simplified transformer block following the original architecture:\n\nclass SimpleTransformerBlock(nn.Module):\n    \"\"\"Transformer block with Multi-Head Self-Attention (MHSA) and MLP.\n\n    Inputs/Outputs:\n      - x: [batch_size, seq_len, embed_dim]\n      - returns: same shape as input\n\n    Structure (Post-LN variant):\n      x = LN(x + MHSA(x))\n      x = LN(x + MLP(x))\n    \"\"\"\n\n    def __init__(self, embed_dim=768, num_heads=8, mlp_ratio=4):\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        # Multi-Head Self-Attention (MHSA)\n        # - batch_first=True ‚Üí tensors are [B, S, E]\n        # - Self-attention uses Q=K=V=linear(x) by default\n        self.attention = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            batch_first=True,\n        )\n\n        # Position-wise feed-forward network (applied independently to each token)\n        hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),  # expand\n            nn.GELU(),                         # nonlinearity\n            nn.Linear(hidden_dim, embed_dim),  # project back\n        )\n\n        # LayerNorms used after residual additions (Post-LN)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # x: [B, S, E]\n\n        # Multi-head self-attention\n        # attn_out: [B, S, E]; attn_weights (ignored here) would be [B, num_heads, S, S]\n        attn_out, _ = self.attention(x, x, x)  # Q=K=V=x (self-attention)\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm1(x + attn_out)\n\n        # Position-wise MLP\n        mlp_out = self.mlp(x)  # [B, S, E]\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm2(x + mlp_out)\n\n        return x  # shape preserved: [B, S, E]\n\n\n# Example usage and shape checks\ntransformer_block = SimpleTransformerBlock(embed_dim=768, num_heads=8)\nsample_input = torch.randn(2, 100, 768)  # [batch_size, seq_len, embed_dim]\noutput = transformer_block(sample_input)\n\nprint(f\"Input shape:  {sample_input.shape}\")\nprint(f\"Output shape: {output.shape}\")  # should match input\nprint(f\"Parameters:  {sum(p.numel() for p in transformer_block.parameters()):,}\")\n\nInput shape:  torch.Size([2, 100, 768])\nOutput shape: torch.Size([2, 100, 768])\nParameters:  7,087,872\n\n\nWhat to notice: - Shapes are stable through the block: [B, S, E] ‚Üí [B, S, E]. - Residuals + LayerNorm appear after each sublayer (Post-LN variant). - MLP expands to mlp_ratio √ó embed_dim then projects back."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#step-development-pipeline-comparison",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#step-development-pipeline-comparison",
    "title": "Foundation Model Architectures",
    "section": "9-Step Development Pipeline Comparison",
    "text": "9-Step Development Pipeline Comparison\nBoth LLMs and GFMs follow similar high-level development pipelines, but differ in the details because language and satellite imagery are very different kinds of data.\n\nPlain-language overview of the 9 steps\n\nData Preparation: Gather raw data and clean it up so the model can learn useful patterns.\nTokenization (turning inputs into pieces the model can handle): Decide how to chop inputs into small parts the model can process.\nArchitecture (the model blueprint): Choose how many layers, how wide/tall the model is, and how it connects information.\nPretraining Objective (what the model practices): Pick the learning task the model does before any specific application.\nTraining Loop (how learning happens): Decide optimizers, learning rate, precision, and how to stabilize training.\nEvaluation (how we check learning): Use simple tests to see if the model is improving in the right ways.\nPretrained Weights (starting point): Load existing model parameters to avoid training from scratch.\nFinetuning (adapting the model): Add a small head or nudge the model for a specific task with labeled examples.\nDeployment (using the model in practice): Serve the model efficiently and handle real-world input sizes.\n\n\n\nSide-by-side (LLMs vs GFMs)\n\n\n\n\n\n\n\n\n\nStep\nLLMs (text)\nGFMs (satellite imagery)\n\n\n\n\n1. Data Preparation\nCollect large text sets, remove duplicates and low-quality content\nCollect scenes from sensors, correct for atmosphere/sensor, remove clouds, tile into manageable chips\n\n\n2. Tokenization\nBreak text into subword tokens; build a vocabulary\nCut images into patches; turn each patch into a vector; add 2D (and time) positions\n\n\n3. Architecture\nTransformer layers over token sequences (often decoder-only for GPT, encoder-only for BERT)\nVision Transformer-style encoders over patch sequences; may include temporal attention for time series\n\n\n4. Pretraining Objective\nPredict the next/missing word to learn language patterns\nReconstruct masked image patches or learn similarities across views/time to learn visual patterns\n\n\n5. Training Loop\nAdamW, learning-rate schedule, mixed precision; long sequences can stress memory\nSimilar tools, but memory shaped by patch size, bands, and image tiling; may mask clouds or invalid pixels\n\n\n6. Evaluation\nQuick checks like ‚Äúhow surprised is the model?‚Äù (e.g., next-word loss) and small downstream tasks\nQuick checks like reconstruction quality and small downstream tasks (e.g., linear probes on land cover)\n\n\n7. Pretrained Weights\nDownload weights and matching tokenizer from model hubs\nDownload weights from hubs (e.g., Prithvi, SatMAE); ensure band order and preprocessing match\n\n\n8. Finetuning\nAdd a small head or adapters; few labeled examples can go far\nAdd a task head (classification/segmentation); often freeze encoder and train a light head on small datasets\n\n\n9. Deployment\nServe via APIs; speed up with caching of past context\nRun sliding-window/tiling over large scenes; export results as geospatial rasters/vectors\n\n\n\n\n\n\nDetails (optional depth)\n\nLLM Development Pipeline\nLanguage models like GPT and BERT have established a mature development pipeline refined through years of research. The process emphasizes text quality, vocabulary optimization, and language understanding benchmarks.\nKey References:\n\nLanguage Models are Few-Shot Learners - GPT-3 methodology\nTraining language models to follow instructions - InstructGPT\nPaLM: Scaling Language Modeling - Large-scale training\n\nLLM Development Pipeline Steps (concise):\n\nData Preparation: Text corpora, deduplication, quality filtering, mixing ratios\nTokenization: BPE, vocabulary construction, special tokens\n\nArchitecture: GPT/BERT variants, depth/width scaling, context length\nPretraining Objective: Next-token prediction, masked language modeling\nTraining Loop: Optimizers, LR schedules, mixed precision, gradient clipping\nEvaluation: Next-token loss, downstream task probing, benchmarks\nPretrained Weights: Model hubs, tokenizer alignment, loading utilities\nFinetuning: Task-specific heads, PEFT methods, instruction tuning\nDeployment: API serving, KV caching, inference optimization\n\n\n\nGFM Development Pipeline\nGeospatial foundation models adapt the transformer architecture for Earth observation data, requiring specialized handling of multi-spectral imagery, temporal sequences, and spatial relationships.\nKey References:\n\nPrithvi Foundation Model - IBM/NASA collaboration\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery\nClay Foundation Model - Open-source geospatial model\n\nGFM Development Pipeline Steps (concise):\n\nData Preparation: Multi-spectral data, georegistration, tiling, cloud masking\nTokenization: Patch-based, continuous embeddings, 2D/temporal positions\nArchitecture: ViT encoders, spatial/temporal attention, memory constraints\nPretraining Objective: Masked patch reconstruction, contrastive learning\nTraining Loop: Cloud masks, mixed precision, gradient accumulation\nEvaluation: Reconstruction metrics, linear probing, generalization\nPretrained Weights: Prithvi, SatMAE, adapter loading, band alignment\nFinetuning: Task heads, PEFT, few-shot learning for limited labels\nDeployment: Tiling inference, geospatial APIs, batch processing"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#step-by-step-detailed-comparison",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#step-by-step-detailed-comparison",
    "title": "Foundation Model Architectures",
    "section": "Step-by-Step Detailed Comparison",
    "text": "Step-by-Step Detailed Comparison\nThis section provides detailed comparisons of each development pipeline step, highlighting the fundamental differences between text and geospatial data processing.\n\n1. Data Preparation Differences\nData preparation represents one of the most significant differences between LLMs and GFMs. LLMs work with human-generated text that requires quality filtering and deduplication, while GFMs process sensor data that requires physical corrections and calibration.\nLLM Data Challenges:\n\nScale: Training datasets like CommonCrawl contain hundreds of terabytes\nQuality: Filtering toxic content, spam, and low-quality text\nDeduplication: Removing exact and near-duplicate documents\nLanguage Detection: Identifying and filtering by language\n\nGFM Data Challenges:\n\nSensor Calibration: Converting raw digital numbers to physical units\nAtmospheric Correction: Removing atmospheric effects from satellite imagery\nCloud Masking: Identifying and handling cloudy pixels\nGeoregistration: Aligning images to geographic coordinate systems\n\n\n# LLM text preprocessing example\nsample_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning is transforming many industries.\", \n    \"Climate change requires urgent global action.\"\n]\n\n# Basic tokenization for vocabulary construction\nvocab = set()\nfor text in sample_texts:\n    vocab.update(text.lower().replace('.', '').split())\n\nprint(\"LLM Data Processing:\")\nprint(f\"Sample vocabulary size: {len(vocab)}\")\nprint(f\"Sample tokens: {list(vocab)[:10]}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# GFM satellite data preprocessing example\nnp.random.seed(42)\npatch_size = 64\nnum_bands = 6\n\n# Simulate raw satellite patch (typical 12-bit values)\nsatellite_patch = np.random.randint(0, 4096, (num_bands, patch_size, patch_size))\n\n# Simulate cloud mask (20% cloud coverage)\ncloud_mask = np.random.random((patch_size, patch_size)) &gt; 0.8\n\n# Apply atmospheric correction (normalize to [0,1])\ncorrected_patch = satellite_patch.astype(np.float32) / 4095.0\ncorrected_patch[:, cloud_mask] = np.nan  # Mask cloudy pixels\n\nprint(\"GFM Data Processing:\")\nprint(f\"Satellite patch shape: {satellite_patch.shape} (bands, height, width)\")\nprint(f\"Cloud coverage: {cloud_mask.sum() / cloud_mask.size * 100:.1f}%\")\nprint(f\"Valid pixels per band: {(~np.isnan(corrected_patch[0])).sum():,}\")\n\nLLM Data Processing:\nSample vocabulary size: 20\nSample tokens: ['machine', 'requires', 'over', 'is', 'many', 'change', 'urgent', 'quick', 'fox', 'industries']\n\n==================================================\n\nGFM Data Processing:\nSatellite patch shape: (6, 64, 64) (bands, height, width)\nCloud coverage: 20.3%\nValid pixels per band: 3,265\n\n\n\n\n2. Tokenization Approaches\nTokenization represents a fundamental difference between language and vision models. LLMs use discrete tokenization with learned vocabularies (like BPE), while GFMs use continuous tokenization through patch embeddings inspired by Vision Transformers.\nLLM Tokenization:\n\nByte-Pair Encoding (BPE): Learns subword units to handle out-of-vocabulary words\nVocabulary Size: Typically 30K-100K tokens balancing efficiency and coverage\nSpecial Tokens: [CLS], [SEP], [PAD], [MASK] for different tasks\n\nGFM Tokenization:\n\nPatch Embedding: Divides images into fixed-size patches (e.g., 16√ó16 pixels)\nLinear Projection: Maps high-dimensional patches to embedding space\nPositional Encoding: 2D spatial positions rather than 1D sequence positions\n\n\n# LLM discrete tokenization example\nvocab_size, embed_dim = 50000, 768\ntoken_ids = torch.tensor([1, 15, 234, 5678, 2])  # [CLS, word1, word2, word3, SEP]\n\nembedding_layer = nn.Embedding(vocab_size, embed_dim)\ntoken_embeddings = embedding_layer(token_ids)\n\nprint(\"LLM Tokenization (Discrete):\")\nprint(f\"Token IDs: {token_ids.tolist()}\")\nprint(f\"Token embeddings shape: {token_embeddings.shape}\")\nprint(f\"Vocabulary size: {vocab_size:,}\")\n\nprint(\"\\n\" + \"-\"*40 + \"\\n\")\n\n# GFM continuous patch tokenization\npatch_size = 16\nnum_bands = 6  # Multi-spectral bands\nembed_dim = 768\n\nnum_patches = 4\npatch_dim = patch_size * patch_size * num_bands\npatches = torch.randn(num_patches, patch_dim)\n\n# Linear projection for patch embedding\npatch_projection = nn.Linear(patch_dim, embed_dim)\npatch_embeddings = patch_projection(patches)\n\nprint(\"GFM Tokenization (Continuous Patches):\")\nprint(f\"Patch dimensions: {patch_size}√ó{patch_size}√ó{num_bands} = {patch_dim}\")\nprint(f\"Patch embeddings shape: {patch_embeddings.shape}\")\nprint(\"No discrete vocabulary - continuous projection\")\n\nLLM Tokenization (Discrete):\nToken IDs: [1, 15, 234, 5678, 2]\nToken embeddings shape: torch.Size([5, 768])\nVocabulary size: 50,000\n\n----------------------------------------\n\nGFM Tokenization (Continuous Patches):\nPatch dimensions: 16√ó16√ó6 = 1536\nPatch embeddings shape: torch.Size([4, 768])\nNo discrete vocabulary - continuous projection\n\n\n\n\n3. Architecture Comparison\nWhile both LLMs and GFMs use transformer architectures, they differ in input processing, positional encoding, and output heads. LLMs like GPT use causal attention for autoregressive generation, while GFMs like Prithvi use bidirectional attention for representation learning.\nKey Architectural Differences:\n\nInput Processing: 1D token sequences vs.¬†2D spatial patches\nPositional Encoding: 1D learned positions vs.¬†2D spatial coordinates\nAttention Patterns: Causal masking vs.¬†full bidirectional attention\nOutput Heads: Language modeling head vs.¬†reconstruction/classification heads\n\n\nclass LLMArchitecture(nn.Module):\n    \"\"\"Simplified LLM architecture (GPT-style)\"\"\"\n    \n    def __init__(self, vocab_size=50000, embed_dim=768, num_layers=12, num_heads=12):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.positional_encoding = nn.Embedding(2048, embed_dim)  # Max sequence length\n        \n        self.layers = nn.ModuleList([\n            SimpleTransformerBlock(embed_dim, num_heads) \n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n        self.output_head = nn.Linear(embed_dim, vocab_size)\n    \n    def forward(self, input_ids):\n        seq_len = input_ids.shape[1]\n        positions = torch.arange(seq_len, device=input_ids.device)\n        \n        # Token + positional embeddings\n        x = self.embedding(input_ids) + self.positional_encoding(positions)\n        \n        # Transformer layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.ln_final(x)\n        logits = self.output_head(x)\n        \n        return logits\n\nclass GFMArchitecture(nn.Module):\n    \"\"\"Simplified GFM architecture (ViT-style)\"\"\"\n    \n    def __init__(self, patch_size=16, num_bands=6, embed_dim=768, num_layers=12, num_heads=12):\n        super().__init__()\n        self.patch_size = patch_size\n        self.num_bands = num_bands\n        \n        # Patch embedding\n        patch_dim = patch_size * patch_size * num_bands\n        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n        \n        # 2D positional embedding\n        self.pos_embed_h = nn.Embedding(100, embed_dim // 2)  # Height positions\n        self.pos_embed_w = nn.Embedding(100, embed_dim // 2)  # Width positions\n        \n        self.layers = nn.ModuleList([\n            SimpleTransformerBlock(embed_dim, num_heads) \n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n    \n    def forward(self, patches, patch_positions):\n        batch_size, num_patches, patch_dim = patches.shape\n        \n        # Patch embeddings\n        x = self.patch_embedding(patches)\n        \n        # 2D positional embeddings\n        pos_h, pos_w = patch_positions[:, :, 0], patch_positions[:, :, 1]\n        pos_emb = torch.cat([\n            self.pos_embed_h(pos_h),\n            self.pos_embed_w(pos_w)\n        ], dim=-1)\n        \n        x = x + pos_emb\n        \n        # Transformer layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.ln_final(x)\n        return x\n\n# Compare architectures\nllm_model = LLMArchitecture(vocab_size=10000, embed_dim=384, num_layers=6, num_heads=6)\ngfm_model = GFMArchitecture(patch_size=16, num_bands=6, embed_dim=384, num_layers=6, num_heads=6)\n\nllm_params = sum(p.numel() for p in llm_model.parameters())\ngfm_params = sum(p.numel() for p in gfm_model.parameters())\n\nprint(\"Architecture Comparison:\")\nprint(f\"LLM parameters: {llm_params:,}\")\nprint(f\"GFM parameters: {gfm_params:,}\")\n\n# Test forward passes\nsample_tokens = torch.randint(0, 10000, (2, 50))  # [batch_size, seq_len]\nsample_patches = torch.randn(2, 16, 16*16*6)  # [batch_size, num_patches, patch_dim]\nsample_positions = torch.randint(0, 10, (2, 16, 2))  # [batch_size, num_patches, 2]\n\nllm_output = llm_model(sample_tokens)\ngfm_output = gfm_model(sample_patches, sample_positions)\n\nprint(f\"\\nLLM output shape: {llm_output.shape}\")\nprint(f\"GFM output shape: {gfm_output.shape}\")\n\nArchitecture Comparison:\nLLM parameters: 19,123,984\nGFM parameters: 11,276,160\n\nLLM output shape: torch.Size([2, 50, 10000])\nGFM output shape: torch.Size([2, 16, 384])\n\n\n\n\n4. Pretraining Objectives\nThe pretraining objectives differ fundamentally between text and visual domains. LLMs excel at predictive modeling (predicting the next token), while GFMs focus on reconstructive modeling (rebuilding masked image patches).\nLLM Objectives:\n\nNext-Token Prediction: GPT-style autoregressive modeling for text generation\nMasked Language Modeling: BERT-style bidirectional understanding\nInstruction Following: Learning to follow human instructions (InstructGPT)\n\nGFM Objectives:\n\nMasked Patch Reconstruction: MAE-style learning of visual representations\nContrastive Learning: Learning invariances across time and space (SimCLR, CLIP)\nMulti-task Pretraining: Combining reconstruction with auxiliary tasks\n\nKey References:\n\nMasked Autoencoders Are Scalable Vision Learners - MAE methodology\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery\n\n\n# LLM next-token prediction objective\nsequence = torch.tensor([[1, 2, 3, 4, 5]])\ntargets = torch.tensor([[2, 3, 4, 5, 6]])  # Shifted by one position\n\nvocab_size = 1000\nlogits = torch.randn(1, 5, vocab_size)  # Model predictions\n\nce_loss = nn.CrossEntropyLoss()\nnext_token_loss = ce_loss(logits.view(-1, vocab_size), targets.view(-1))\n\nprint(\"LLM Pretraining Objectives:\")\nprint(f\"Next-token prediction loss: {next_token_loss.item():.4f}\")\n\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# GFM masked patch reconstruction objective\nbatch_size, num_patches, patch_dim = 2, 64, 768\noriginal_patches = torch.randn(batch_size, num_patches, patch_dim)\n\n# Random masking (75% typical for MAE)\nmask_ratio = 0.75\nnum_masked = int(num_patches * mask_ratio)\n\nmask = torch.zeros(batch_size, num_patches, dtype=torch.bool)\nfor i in range(batch_size):\n    masked_indices = torch.randperm(num_patches)[:num_masked]\n    mask[i, masked_indices] = True\n\n# Reconstruction loss on masked patches only\nreconstructed_patches = torch.randn_like(original_patches)\nreconstruction_loss = nn.MSELoss()(\n    reconstructed_patches[mask], \n    original_patches[mask]\n)\n\nprint(\"GFM Pretraining Objectives:\")\nprint(f\"Mask ratio: {mask_ratio:.1%}\")\nprint(f\"Masked patches per sample: {num_masked}\")\nprint(f\"Reconstruction loss: {reconstruction_loss.item():.4f}\")\n\nLLM Pretraining Objectives:\nNext-token prediction loss: 8.0442\n\n--------------------------------------------------\n\nGFM Pretraining Objectives:\nMask ratio: 75.0%\nMasked patches per sample: 48\nReconstruction loss: 2.0040"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#scaling-and-evolution",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#scaling-and-evolution",
    "title": "Foundation Model Architectures",
    "section": "Scaling and Evolution",
    "text": "Scaling and Evolution\nThe scaling trends between LLMs and GFMs reveal different optimization strategies. LLMs focus on parameter scaling (billions of parameters) while GFMs emphasize data modality scaling (spectral, spatial, and temporal dimensions).\n\nParameter Scaling Comparison\nLLM Scaling Milestones:\n\nGPT-1 (2018): 117M parameters - Demonstrated unsupervised pretraining potential\nBERT-Base (2018): 110M parameters - Bidirectional language understanding\nGPT-2 (2019): 1.5B parameters - First signs of emergent capabilities\nGPT-3 (2020): 175B parameters - Few-shot learning breakthrough\nPaLM (2022): 540B parameters - Advanced reasoning capabilities\nGPT-4 (2023): ~1T parameters - Multimodal understanding\n\nGFM Scaling Examples:\n\nSatMAE-Base: 86M parameters - Satellite imagery foundation\nPrithvi-100M: 100M parameters - IBM/NASA Earth observation model\nClay-v0.1: 139M parameters - Open-source geospatial foundation model\nScale-MAE: 600M parameters - Largest published geospatial transformer\n\nContext/Input Scaling Differences:\nLLMs:\n\nContext length: 512 ‚Üí 2K ‚Üí 8K ‚Üí 128K+ tokens\nTraining data: Web text, books, code (curated datasets)\nFocus: Language understanding and generation\n\nGFMs:\n\nInput bands: 3 (RGB) ‚Üí 6+ (multispectral) ‚Üí hyperspectral\nSpatial resolution: Various (10m to 0.3m pixel sizes)\nTemporal dimension: Single ‚Üí time series ‚Üí multi-temporal\nFocus: Earth observation and environmental monitoring\n\n\n# Visualize parameter scaling comparison\nllm_milestones = {\n    'GPT-1': 117e6,\n    'BERT-Base': 110e6,\n    'GPT-2': 1.5e9,\n    'GPT-3': 175e9,\n    'PaLM': 540e9,\n    'GPT-4': 1000e9  # Estimated\n}\n\ngfm_milestones = {\n    'SatMAE-Base': 86e6,\n    'Prithvi-100M': 100e6,\n    'Clay-v0.1': 139e6,\n    'SatLas-Base': 300e6,\n    'Scale-MAE': 600e6\n}\n\n# Create visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# LLM scaling\nmodels = list(llm_milestones.keys())\nparams = [llm_milestones[m]/1e9 for m in models]\n\nax1.bar(models, params, color='skyblue', alpha=0.7)\nax1.set_yscale('log')\nax1.set_ylabel('Parameters (Billions)')\nax1.set_title('LLM Parameter Scaling')\nax1.tick_params(axis='x', rotation=45)\n\n# GFM scaling\nmodels = list(gfm_milestones.keys())\nparams = [gfm_milestones[m]/1e6 for m in models]\n\nax2.bar(models, params, color='lightcoral', alpha=0.7)\nax2.set_ylabel('Parameters (Millions)')\nax2.set_title('GFM Parameter Scaling')\nax2.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nData Requirements and Constraints\nThe data infrastructure requirements for LLMs and GFMs differ dramatically due to the nature of text versus imagery data. Understanding these constraints is crucial for development planning.\n\n\n\nAspect\nLLMs\nGFMs\n\n\n\n\nData Volume\nTerabytes of text data (web crawls, books, code repositories)\nPetabytes of satellite imagery (constrained by storage/IO bandwidth)\n\n\nData Quality Challenges\nDeduplication algorithms, toxicity filtering, language detection\nCloud masking, atmospheric correction, sensor calibration\n\n\nPreprocessing Requirements\nTokenization, sequence packing, attention mask generation\nPatch extraction, normalization, spatial/temporal alignment\n\n\nStorage Format Optimization\nCompressed text files, pre-tokenized sequences\nCloud-optimized formats (COG, Zarr), tiled storage\n\n\nAccess Pattern Differences\nSequential text processing, random document sampling\nSpatial/temporal queries, patch-based sampling, geographic tiling"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#implementation-examples",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#implementation-examples",
    "title": "Foundation Model Architectures",
    "section": "Implementation Examples",
    "text": "Implementation Examples\n\nEmbedding Creation\nEmbeddings are the foundation of both LLMs and GFMs, but they differ in how raw inputs are converted to dense vector representations. This section demonstrates practical implementation patterns for both domains.\n\n# Text embedding creation (LLM approach)\ntext = \"The forest shows signs of deforestation.\"\ntokens = text.lower().replace('.', '').split()\n\n# Create simple vocabulary mapping\nvocab = {word: i for i, word in enumerate(set(tokens))}\nvocab['&lt;PAD&gt;'] = len(vocab)\nvocab['&lt;UNK&gt;'] = len(vocab)\n\n# Convert text to token IDs\ntoken_ids = [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\ntoken_tensor = torch.tensor(token_ids).unsqueeze(0)\n\n# Embedding layer lookup\nembed_layer = nn.Embedding(len(vocab), 256)\ntext_embeddings = embed_layer(token_tensor)\n\nprint(\"Text Embeddings (LLM):\")\nprint(f\"Original text: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Token IDs: {token_ids}\")\nprint(f\"Embeddings shape: {text_embeddings.shape}\")\n\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# Patch embedding creation (GFM approach)\npatch_size = 16\nnum_bands = 6\n\n# Create sample multi-spectral satellite patch\nsatellite_patch = torch.randn(1, num_bands, patch_size, patch_size)\n\n# Flatten patch for linear projection\npatch_flat = satellite_patch.view(1, num_bands * patch_size * patch_size)\n\n# Linear projection to embedding space\npatch_projection = nn.Linear(num_bands * patch_size * patch_size, 256)\npatch_embeddings = patch_projection(patch_flat)\n\nprint(\"Patch Embeddings (GFM):\")\nprint(f\"Original patch shape: {satellite_patch.shape}\")\nprint(f\"Flattened patch shape: {patch_flat.shape}\")\nprint(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n\nText Embeddings (LLM):\nOriginal text: The forest shows signs of deforestation.\nTokens: ['the', 'forest', 'shows', 'signs', 'of', 'deforestation']\nToken IDs: [2, 5, 4, 3, 0, 1]\nEmbeddings shape: torch.Size([1, 6, 256])\n\n--------------------------------------------------\n\nPatch Embeddings (GFM):\nOriginal patch shape: torch.Size([1, 6, 16, 16])\nFlattened patch shape: torch.Size([1, 1536])\nPatch embeddings shape: torch.Size([1, 256])\n\n\n\n\nPositional Encoding Comparison\nPositional encodings enable transformers to understand sequence order (LLMs) or spatial relationships (GFMs). The fundamental difference lies in 1D sequential positions versus 2D spatial coordinates.\nKey Differences:\n\nLLM: 1D sinusoidal encoding for sequence positions\nGFM: 2D spatial encoding combining height/width coordinates\nLearned vs Fixed: Both approaches can use learned or fixed encodings\n\n\n# 1D positional encoding for language models\ndef sinusoidal_positional_encoding(seq_len, embed_dim):\n    \"\"\"Create sinusoidal positional encodings for sequence data\"\"\"\n    pe = torch.zeros(seq_len, embed_dim)\n    position = torch.arange(0, seq_len).unsqueeze(1).float()\n    \n    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * \n                       -(np.log(10000.0) / embed_dim))\n    \n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    return pe\n\n# 2D positional encoding for geospatial models  \ndef create_2d_positional_encoding(height, width, embed_dim):\n    \"\"\"Create 2D positional encodings for spatial data\"\"\"\n    pe = torch.zeros(height, width, embed_dim)\n    \n    # Create position grids\n    y_pos = torch.arange(height).unsqueeze(1).repeat(1, width).float()\n    x_pos = torch.arange(width).unsqueeze(0).repeat(height, 1).float()\n    \n    # Encode Y positions in first half of embedding\n    for i in range(embed_dim // 2):\n        if i % 2 == 0:\n            pe[:, :, i] = torch.sin(y_pos / (10000 ** (i / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(y_pos / (10000 ** (i / embed_dim)))\n    \n    # Encode X positions in second half of embedding\n    for i in range(embed_dim // 2, embed_dim):\n        j = i - embed_dim // 2\n        if j % 2 == 0:\n            pe[:, :, i] = torch.sin(x_pos / (10000 ** (j / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(x_pos / (10000 ** (j / embed_dim)))\n    \n    return pe\n\n# Generate and visualize both encodings\nseq_len, embed_dim = 100, 256\npos_encoding_1d = sinusoidal_positional_encoding(seq_len, embed_dim)\n\nheight, width = 8, 8\npos_encoding_2d = create_2d_positional_encoding(height, width, embed_dim)\n\nprint(f\"1D positional encoding shape: {pos_encoding_1d.shape}\")\nprint(f\"2D positional encoding shape: {pos_encoding_2d.shape}\")\n\n# Visualize both encodings\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(pos_encoding_1d[:50, :50].T, cmap='RdBu', aspect='auto')\nplt.title('1D Positional Encoding (LLM)')\nplt.xlabel('Sequence Position')\nplt.ylabel('Embedding Dimension')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\npos_2d_viz = pos_encoding_2d[:, :, :64].mean(dim=-1)\nplt.imshow(pos_2d_viz, cmap='RdBu', aspect='equal')\nplt.title('2D Positional Encoding (GFM)')\nplt.xlabel('Width')\nplt.ylabel('Height')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n1D positional encoding shape: torch.Size([100, 256])\n2D positional encoding shape: torch.Size([8, 8, 256])"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#course-mapping-and-applications",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#course-mapping-and-applications",
    "title": "Foundation Model Architectures",
    "section": "Course Mapping and Applications",
    "text": "Course Mapping and Applications\nThis cheatsheet directly supports the GEOG 288KC curriculum by providing comparative understanding between established LLM techniques and emerging GFM approaches.\n\nWeekly Course Structure\nWeeks 1-3: Foundation Building - Focus: Data ‚Üí Attention ‚Üí Architecture - LLM Topics: Text preprocessing, tokenization, transformer blocks - GFM Topics: Satellite data, patch embedding, spatial attention\nWeeks 4-7: Model Development\n- Focus: Pretraining ‚Üí Training ‚Üí Evaluation ‚Üí Integration - LLM Topics: Language modeling, training loops, perplexity evaluation - GFM Topics: Masked reconstruction, cloud handling, linear probing\nWeeks 8-10: Deployment & Applications - Focus: Finetuning ‚Üí Deployment ‚Üí Synthesis - LLM Topics: Instruction tuning, PEFT methods, API deployment - GFM Topics: Task-specific heads, few-shot learning, geospatial inference\n\n\nKey Architectural Differences Summary\nData Nature:\n\nLLMs: Discrete text tokens with semantic consistency\nGFMs: Continuous pixel values requiring contextual interpretation\n\nTokenization Approach:\n\nLLMs: Vocabulary-based discrete mapping (BPE, WordPiece)\nGFMs: Patch-based continuous projection (linear embedding)\n\nPositional Information:\n\nLLMs: 1D sequence positions for temporal understanding\nGFMs: 2D spatial + temporal positions for spatiotemporal context\n\nTraining Objectives:\n\nLLMs: Next token prediction or masked language modeling\nGFMs: Masked patch reconstruction or contrastive learning\n\nEvaluation Metrics:\n\nLLMs: Perplexity, BLEU, downstream language task performance\nGFMs: Reconstruction quality, linear probing, spatial generalization\n\nDeployment Patterns:\n\nLLMs: Text generation with streaming and KV caching\nGFMs: Spatial inference with geographic tiling and batch processing"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#further-reading-and-references",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#further-reading-and-references",
    "title": "Foundation Model Architectures",
    "section": "Further Reading and References",
    "text": "Further Reading and References\n\nEssential Foundation Papers\nCore Transformer Architecture:\n\nAttention Is All You Need - Vaswani et al., 2017\nBERT: Pre-training of Deep Bidirectional Transformers - Devlin et al., 2018\nLanguage Models are Few-Shot Learners - Brown et al., 2020 (GPT-3)\n\nVision Transformers:\n\nAn Image is Worth 16x16 Words - Dosovitskiy et al., 2020 (ViT)\nMasked Autoencoders Are Scalable Vision Learners - He et al., 2021\nScaling Vision Transformers - Zhai et al., 2021\n\nGeospatial Foundation Models:\n\nPrithvi Foundation Model - IBM/NASA collaboration, 2023\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery - Cong et al., 2022\nClay Foundation Model - Made with Clay, 2024"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/foundation_model_architectures.html#summary",
    "href": "course-materials/extras/cheatsheets/foundation_model_architectures.html#summary",
    "title": "Foundation Model Architectures",
    "section": "Summary",
    "text": "Summary\nKey concepts for foundation model architectures:\n\nHistorical Evolution: From symbolic AI to transformer-based foundation models\nArchitecture Comparison: LLMs use discrete tokenization, GFMs use continuous patch embeddings\nDevelopment Pipeline: 9-step process with domain-specific adaptations\nScaling Trends: LLMs scale in parameters/context, GFMs scale in spectral/spatial/temporal dimensions\nTraining Objectives: Next-token prediction vs.¬†masked patch reconstruction\nDeployment Considerations: Text streaming vs.¬†spatial tiling and batch inference\nCourse Integration: Weekly progression from data processing to deployment"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html",
    "href": "course-materials/extras/cheatsheets/stac_apis.html",
    "title": "STAC APIs & Planetary Computer",
    "section": "",
    "text": "STAC (SpatioTemporal Asset Catalog) is a specification for describing geospatial information that makes it easier to work with satellite imagery and other earth observation data at scale.\n\nimport pystac_client\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pystac.extensions.eo import EOExtension\nfrom pystac.extensions.sat import SatExtension\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nprint(\"STAC libraries loaded successfully\")\n\nSTAC libraries loaded successfully"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#introduction-to-stac",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#introduction-to-stac",
    "title": "STAC APIs & Planetary Computer",
    "section": "",
    "text": "STAC (SpatioTemporal Asset Catalog) is a specification for describing geospatial information that makes it easier to work with satellite imagery and other earth observation data at scale.\n\nimport pystac_client\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pystac.extensions.eo import EOExtension\nfrom pystac.extensions.sat import SatExtension\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nprint(\"STAC libraries loaded successfully\")\n\nSTAC libraries loaded successfully"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#microsoft-planetary-computer",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#microsoft-planetary-computer",
    "title": "STAC APIs & Planetary Computer",
    "section": "Microsoft Planetary Computer",
    "text": "Microsoft Planetary Computer\nMicrosoft‚Äôs Planetary Computer provides free access to petabytes of earth observation data through STAC APIs.\n\n# Connect to Planetary Computer STAC API\n# Note: Planetary Computer requires authentication for data access, but catalog browsing is public\npc_catalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n)\n\nprint(f\"Planetary Computer STAC API: {pc_catalog.title}\")\nprint(f\"Description: {pc_catalog.description}\")\n\n# List available collections\ncollections = list(pc_catalog.get_collections())\nprint(f\"\\nNumber of collections: {len(collections)}\")\n\n# Show first few collection IDs and titles\nfor i, collection in enumerate(collections[:10]):\n    print(f\"{i+1:2d}. {collection.id}: {collection.title}\")\n\nPlanetary Computer STAC API: Microsoft Planetary Computer STAC API\nDescription: Searchable spatiotemporal metadata describing Earth science datasets hosted by the Microsoft Planetary Computer\n\nNumber of collections: 127\n 1. daymet-annual-pr: Daymet Annual Puerto Rico\n 2. daymet-daily-hi: Daymet Daily Hawaii\n 3. 3dep-seamless: USGS 3DEP Seamless DEMs\n 4. 3dep-lidar-dsm: USGS 3DEP Lidar Digital Surface Model\n 5. fia: Forest Inventory and Analysis\n 6. gridmet: gridMET\n 7. daymet-annual-na: Daymet Annual North America\n 8. daymet-monthly-na: Daymet Monthly North America\n 9. daymet-annual-hi: Daymet Annual Hawaii\n10. daymet-monthly-hi: Daymet Monthly Hawaii\n\n\n\nKey Planetary Computer Collections\n\n# Key collections for GFM training\nkey_collections = [\n    \"sentinel-2-l2a\",      # Sentinel-2 Level 2A (surface reflectance)\n    \"landsat-c2-l2\",       # Landsat Collection 2 Level 2\n    \"modis-13A1-061\",      # MODIS Vegetation Indices\n    \"naip\",                # National Agriculture Imagery Program\n    \"aster-l1t\",           # ASTER Level 1T\n    \"cop-dem-glo-30\"       # Copernicus DEM Global 30m\n]\n\nprint(\"Key Collections for GFM Training:\")\nprint(\"=\" * 50)\n\nfor collection_id in key_collections:\n    try:\n        collection = pc_catalog.get_collection(collection_id)\n        print(f\"\\n{collection.id}\")\n        print(f\"  Title: {collection.title}\")\n        print(f\"  Extent: {collection.extent.temporal.intervals[0][0]} to {collection.extent.temporal.intervals[0][1]}\")\n        \n        # Show available bands if it's an EO collection\n        if 'eo:bands' in collection.summaries:\n            bands = collection.summaries['eo:bands']\n            print(f\"  Bands: {len(bands)} bands available\")\n            \n    except Exception as e:\n        print(f\"  Error accessing {collection_id}: {e}\")\n\nKey Collections for GFM Training:\n==================================================\n\nsentinel-2-l2a\n  Title: Sentinel-2 Level-2A\n  Extent: 2015-06-27 10:25:31+00:00 to None\n  Error accessing sentinel-2-l2a: argument of type 'Summaries' is not iterable\n\nlandsat-c2-l2\n  Title: Landsat Collection 2 Level-2\n  Extent: 1982-08-22 00:00:00+00:00 to None\n  Error accessing landsat-c2-l2: argument of type 'Summaries' is not iterable\n\nmodis-13A1-061\n  Title: MODIS Vegetation Indices 16-Day (500m)\n  Extent: 2000-02-18 00:00:00+00:00 to None\n  Error accessing modis-13A1-061: argument of type 'Summaries' is not iterable\n\nnaip\n  Title: NAIP: National Agriculture Imagery Program\n  Extent: 2010-01-01 00:00:00+00:00 to 2023-12-31 00:00:00+00:00\n  Error accessing naip: argument of type 'Summaries' is not iterable\n\naster-l1t\n  Title: ASTER L1T\n  Extent: 2000-03-04 12:00:00+00:00 to 2006-12-31 12:00:00+00:00\n  Error accessing aster-l1t: argument of type 'Summaries' is not iterable\n\ncop-dem-glo-30\n  Title: Copernicus DEM GLO-30\n  Extent: 2021-04-22 00:00:00+00:00 to 2021-04-22 00:00:00+00:00\n  Error accessing cop-dem-glo-30: argument of type 'Summaries' is not iterable"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#searching-for-data",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#searching-for-data",
    "title": "STAC APIs & Planetary Computer",
    "section": "Searching for Data",
    "text": "Searching for Data\n\nBasic Search Parameters\n\n# Define area of interest (AOI) - California Central Valley\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[\n        [-121.5, 37.0],  # Southwest corner\n        [-121.0, 37.0],  # Southeast corner  \n        [-121.0, 37.5],  # Northeast corner\n        [-121.5, 37.5],  # Northwest corner\n        [-121.5, 37.0]   # Close polygon\n    ]]\n}\n\n# Define time range\nstart_date = \"2023-06-01\"\nend_date = \"2023-08-31\"\n\nprint(f\"Search parameters:\")\nprint(f\"  AOI: Central Valley, California\")\nprint(f\"  Time range: {start_date} to {end_date}\")\nprint(f\"  Collections: Sentinel-2 L2A\")\n\nSearch parameters:\n  AOI: Central Valley, California\n  Time range: 2023-06-01 to 2023-08-31\n  Collections: Sentinel-2 L2A\n\n\n\n\nSentinel-2 Search Example\n\n# Search for Sentinel-2 data\nsearch = pc_catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    intersects=aoi,\n    datetime=f\"{start_date}/{end_date}\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}  # Less than 20% cloud cover\n)\n\n# Get search results\nitems = list(search.items())\nprint(f\"Found {len(items)} Sentinel-2 scenes\")\n\n# Display first few results\nfor i, item in enumerate(items[:5]):\n    # Get cloud cover from properties\n    cloud_cover = item.properties.get('eo:cloud_cover', 'N/A')\n    date = item.datetime.strftime('%Y-%m-%d')\n    \n    print(f\"{i+1}. {item.id}\")\n    print(f\"   Date: {date}\")\n    print(f\"   Cloud cover: {cloud_cover}%\")\n    print(f\"   Assets: {list(item.assets.keys())}\")\n\nFound 111 Sentinel-2 scenes\n1. S2B_MSIL2A_20230829T183929_R070_T10SFG_20240821T160323\n   Date: 2023-08-29\n   Cloud cover: 0.040323%\n   Assets: ['AOT', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A', 'SCL', 'WVP', 'visual', 'safe-manifest', 'granule-metadata', 'inspire-metadata', 'product-metadata', 'datastrip-metadata', 'tilejson', 'rendered_preview']\n2. S2B_MSIL2A_20230829T183929_R070_T10SFG_20230830T002629\n   Date: 2023-08-29\n   Cloud cover: 4.674362%\n   Assets: ['AOT', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A', 'SCL', 'WVP', 'visual', 'preview', 'safe-manifest', 'granule-metadata', 'inspire-metadata', 'product-metadata', 'datastrip-metadata', 'tilejson', 'rendered_preview']\n3. S2B_MSIL2A_20230829T183929_R070_T10SFF_20240821T160323\n   Date: 2023-08-29\n   Cloud cover: 1.465987%\n   Assets: ['AOT', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A', 'SCL', 'WVP', 'visual', 'safe-manifest', 'granule-metadata', 'inspire-metadata', 'product-metadata', 'datastrip-metadata', 'tilejson', 'rendered_preview']\n4. S2B_MSIL2A_20230829T183929_R070_T10SFF_20230830T002657\n   Date: 2023-08-29\n   Cloud cover: 2.389342%\n   Assets: ['AOT', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A', 'SCL', 'WVP', 'visual', 'preview', 'safe-manifest', 'granule-metadata', 'inspire-metadata', 'product-metadata', 'datastrip-metadata', 'tilejson', 'rendered_preview']\n5. S2A_MSIL2A_20230827T184921_R113_T10SFG_20241024T140931\n   Date: 2023-08-27\n   Cloud cover: 0.003351%\n   Assets: ['AOT', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A', 'SCL', 'WVP', 'visual', 'safe-manifest', 'granule-metadata', 'inspire-metadata', 'product-metadata', 'datastrip-metadata', 'tilejson', 'rendered_preview']\n\n\n\n\nMulti-Collection Search\n\n# Search across multiple collections\nmulti_search = pc_catalog.search(\n    collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"],\n    intersects=aoi,\n    datetime=\"2023-07-01/2023-07-31\",\n    limit=10\n)\n\nmulti_items = list(multi_search.items())\nprint(f\"Found {len(multi_items)} items across collections\")\n\n# Group by collection\nby_collection = {}\nfor item in multi_items:\n    collection = item.collection_id\n    if collection not in by_collection:\n        by_collection[collection] = []\n    by_collection[collection].append(item)\n\nfor collection, items in by_collection.items():\n    print(f\"\\n{collection}: {len(items)} items\")\n    for item in items[:3]:  # Show first 3\n        date = item.datetime.strftime('%Y-%m-%d')\n        print(f\"  - {item.id} ({date})\")\n\nFound 65 items across collections\n\nsentinel-2-l2a: 50 items\n  - S2B_MSIL2A_20230730T183929_R070_T10SFG_20241019T130401 (2023-07-30)\n  - S2B_MSIL2A_20230730T183929_R070_T10SFG_20240820T035115 (2023-07-30)\n  - S2B_MSIL2A_20230730T183929_R070_T10SFG_20230731T011717 (2023-07-30)\n\nlandsat-c2-l2: 15 items\n  - LC08_L2SP_043035_20230726_02_T1 (2023-07-26)\n  - LC08_L2SP_043034_20230726_02_T1 (2023-07-26)\n  - LC09_L2SP_044034_20230725_02_T1 (2023-07-25)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#working-with-stac-items",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#working-with-stac-items",
    "title": "STAC APIs & Planetary Computer",
    "section": "Working with STAC Items",
    "text": "Working with STAC Items\n\nExamining Item Metadata\n\n# Take a closer look at a single item\nif items:\n    sample_item = items[0]\n    \n    print(f\"Item ID: {sample_item.id}\")\n    print(f\"Collection: {sample_item.collection_id}\")\n    print(f\"Datetime: {sample_item.datetime}\")\n    print(f\"Geometry: {sample_item.geometry['type']}\")\n    print(f\"Bbox: {sample_item.bbox}\")\n    \n    # Properties\n    print(f\"\\nKey Properties:\")\n    key_props = ['eo:cloud_cover', 'proj:epsg', 'gsd']\n    for prop in key_props:\n        if prop in sample_item.properties:\n            print(f\"  {prop}: {sample_item.properties[prop]}\")\n    \n    # Available assets (bands/files)\n    print(f\"\\nAvailable Assets:\")\n    for asset_key, asset in sample_item.assets.items():\n        print(f\"  {asset_key}: {asset.title}\")\n\nItem ID: LC08_L2SP_043035_20230726_02_T1\nCollection: landsat-c2-l2\nDatetime: 2023-07-26 18:40:05.170226+00:00\nGeometry: Polygon\nBbox: [-122.26260425677917, 34.964494820144395, -119.67909134681727, 37.098925179855605]\n\nKey Properties:\n  eo:cloud_cover: 25.26\n  gsd: 30\n\nAvailable Assets:\n  qa: Surface Temperature Quality Assessment Band\n  ang: Angle Coefficients File\n  red: Red Band\n  blue: Blue Band\n  drad: Downwelled Radiance Band\n  emis: Emissivity Band\n  emsd: Emissivity Standard Deviation Band\n  trad: Thermal Radiance Band\n  urad: Upwelled Radiance Band\n  atran: Atmospheric Transmittance Band\n  cdist: Cloud Distance Band\n  green: Green Band\n  nir08: Near Infrared Band 0.8\n  lwir11: Surface Temperature Band\n  swir16: Short-wave Infrared Band 1.6\n  swir22: Short-wave Infrared Band 2.2\n  coastal: Coastal/Aerosol Band\n  mtl.txt: Product Metadata File (txt)\n  mtl.xml: Product Metadata File (xml)\n  mtl.json: Product Metadata File (json)\n  qa_pixel: Pixel Quality Assessment Band\n  qa_radsat: Radiometric Saturation and Terrain Occlusion Quality Assessment Band\n  qa_aerosol: Aerosol Quality Assessment Band\n  tilejson: TileJSON with default rendering\n  rendered_preview: Rendered preview\n\n\n\n\nAccessing Asset URLs\n\n# Get asset URLs (authentication required for actual data download)\n# For production use, install: pip install planetary-computer\nif items:\n    sample_item = items[0]\n    \n    # Key Sentinel-2 bands for ML applications\n    key_bands = ['B02', 'B03', 'B04', 'B08']  # Blue, Green, Red, NIR\n    \n    print(\"Asset URLs for key bands:\")\n    for band in key_bands:\n        if band in sample_item.assets:\n            # Get the asset URL (would need signing for actual data access)\n            asset = sample_item.assets[band]\n            asset_href = asset.href\n            print(f\"  {band}: {asset_href[:80]}...\")\n            print(f\"    Title: {asset.title}\")\n        else:\n            print(f\"  {band}: Not available\")\n\n# Note: For actual data access, use planetary-computer package:\n# import planetary_computer as pc\n# signed_item = pc.sign(sample_item)\n# Then use signed_item.assets[band].href for downloading\nprint(f\"\\nNote: URLs above require authentication for actual data access\")\nprint(f\"Install 'planetary-computer' package and use pc.sign() for data downloads\")\n\nAsset URLs for key bands:\n  B02: Not available\n  B03: Not available\n  B04: Not available\n  B08: Not available\n\nNote: URLs above require authentication for actual data access\nInstall 'planetary-computer' package and use pc.sign() for data downloads"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#other-stac-providers",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#other-stac-providers",
    "title": "STAC APIs & Planetary Computer",
    "section": "Other STAC Providers",
    "text": "Other STAC Providers\n\nEarth Search (Element84)\n\n# Connect to Earth Search STAC API\ntry:\n    earth_search = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\n    print(f\"Earth Search API: {earth_search.title}\")\n    \n    # List collections\n    earth_collections = list(earth_search.get_collections())\n    print(f\"Available collections: {len(earth_collections)}\")\n    \n    for collection in earth_collections[:5]:\n        print(f\"  - {collection.id}: {collection.title}\")\n        \nexcept Exception as e:\n    print(f\"Error connecting to Earth Search: {e}\")\n\nEarth Search API: Earth Search by Element 84\nAvailable collections: 9\n  - sentinel-2-pre-c1-l2a: Sentinel-2 Pre-Collection 1 Level-2A \n  - cop-dem-glo-30: Copernicus DEM GLO-30\n  - naip: NAIP: National Agriculture Imagery Program\n  - cop-dem-glo-90: Copernicus DEM GLO-90\n  - landsat-c2-l2: Landsat Collection 2 Level-2\n\n\n\n\nGoogle Earth Engine Data Catalog\n\n# Example of other STAC endpoints\nother_endpoints = {\n    \"USGS STAC\": \"https://landsatlook.usgs.gov/stac-server\",\n    \"CBERS STAC\": \"https://cbers-stac.s3.amazonaws.com\",\n    \"Digital Earth Australia\": \"https://explorer.sandbox.dea.ga.gov.au/stac\"\n}\n\nprint(\"Other STAC Endpoints:\")\nfor name, url in other_endpoints.items():\n    print(f\"  {name}: {url}\")\n    \n# Note: Some endpoints may require authentication or have different access patterns\n\nOther STAC Endpoints:\n  USGS STAC: https://landsatlook.usgs.gov/stac-server\n  CBERS STAC: https://cbers-stac.s3.amazonaws.com\n  Digital Earth Australia: https://explorer.sandbox.dea.ga.gov.au/stac"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#loading-data-for-ml-applications",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#loading-data-for-ml-applications",
    "title": "STAC APIs & Planetary Computer",
    "section": "Loading Data for ML Applications",
    "text": "Loading Data for ML Applications\n\nCreating Datacubes\n\ndef create_datacube_info(items, bands=['B02', 'B03', 'B04', 'B08']):\n    \"\"\"Create information for a datacube from STAC items\"\"\"\n    \n    datacube_info = []\n    \n    for item in items:\n        item_info = {\n            'id': item.id,\n            'datetime': item.datetime,\n            'cloud_cover': item.properties.get('eo:cloud_cover', None),\n            'epsg': item.properties.get('proj:epsg', None),\n            'bands': {}\n        }\n        \n        for band in bands:\n            if band in item.assets:\n                item_info['bands'][band] = item.assets[band].href\n            else:\n                item_info['bands'][band] = None\n                \n        datacube_info.append(item_info)\n    \n    return datacube_info\n\n# Create datacube information\nif items:\n    datacube = create_datacube_info(items[:5])\n    \n    print(\"Datacube Information:\")\n    for i, scene in enumerate(datacube):\n        print(f\"\\nScene {i+1}:\")\n        print(f\"  ID: {scene['id']}\")\n        print(f\"  Date: {scene['datetime'].strftime('%Y-%m-%d')}\")\n        print(f\"  Cloud cover: {scene['cloud_cover']}%\")\n        print(f\"  Available bands: {list(scene['bands'].keys())}\")\n\nDatacube Information:\n\nScene 1:\n  ID: LC08_L2SP_043035_20230726_02_T1\n  Date: 2023-07-26\n  Cloud cover: 25.26%\n  Available bands: ['B02', 'B03', 'B04', 'B08']\n\nScene 2:\n  ID: LC08_L2SP_043034_20230726_02_T1\n  Date: 2023-07-26\n  Cloud cover: 0.75%\n  Available bands: ['B02', 'B03', 'B04', 'B08']\n\nScene 3:\n  ID: LC09_L2SP_044034_20230725_02_T1\n  Date: 2023-07-25\n  Cloud cover: 32.5%\n  Available bands: ['B02', 'B03', 'B04', 'B08']\n\nScene 4:\n  ID: LE07_L2SP_044034_20230721_02_T1\n  Date: 2023-07-21\n  Cloud cover: 54.0%\n  Available bands: ['B02', 'B03', 'B04', 'B08']\n\nScene 5:\n  ID: LC09_L2SP_043035_20230718_02_T1\n  Date: 2023-07-18\n  Cloud cover: 33.19%\n  Available bands: ['B02', 'B03', 'B04', 'B08']\n\n\n\n\nIntegration with Rasterio and Xarray\n\n# Example of loading data with rasterio (conceptual)\ndef load_stac_band_info(item, band_name):\n    \"\"\"Get information needed to load a band with rasterio\"\"\"\n    \n    if band_name in item.assets:\n        asset = item.assets[band_name]\n        \n        band_info = {\n            'url': asset.href,\n            'title': asset.title,\n            'description': asset.description,\n            'eo_bands': []\n        }\n        \n        # Get EO band information if available\n        if hasattr(asset, 'extra_fields') and 'eo:bands' in asset.extra_fields:\n            band_info['eo_bands'] = asset.extra_fields['eo:bands']\n            \n        return band_info\n    else:\n        return None\n\n# Example usage\nif items:\n    sample_item = items[0]\n    red_band_info = load_stac_band_info(sample_item, 'B04')\n    \n    if red_band_info:\n        print(\"Red Band Information:\")\n        for key, value in red_band_info.items():\n            print(f\"  {key}: {value}\")"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#stac-for-gfm-training-workflows",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#stac-for-gfm-training-workflows",
    "title": "STAC APIs & Planetary Computer",
    "section": "STAC for GFM Training Workflows",
    "text": "STAC for GFM Training Workflows\n\nMulti-Temporal Data Collection\n\ndef plan_multitemporal_collection(aoi, date_ranges, collections):\n    \"\"\"Plan a multi-temporal data collection for GFM training\"\"\"\n    \n    collection_plan = {\n        'total_scenes': 0,\n        'by_date_range': {},\n        'by_collection': {}\n    }\n    \n    for date_range in date_ranges:\n        start, end = date_range\n        range_key = f\"{start}_to_{end}\"\n        collection_plan['by_date_range'][range_key] = {}\n        \n        for collection in collections:\n            # Simulate search (would normally use actual search)\n            estimated_scenes = np.random.randint(10, 50)  # Mock data\n            \n            collection_plan['by_date_range'][range_key][collection] = estimated_scenes\n            \n            if collection not in collection_plan['by_collection']:\n                collection_plan['by_collection'][collection] = 0\n            collection_plan['by_collection'][collection] += estimated_scenes\n            \n            collection_plan['total_scenes'] += estimated_scenes\n    \n    return collection_plan\n\n# Plan multi-temporal collection\ndate_ranges = [\n    (\"2023-03-01\", \"2023-05-31\"),  # Spring\n    (\"2023-06-01\", \"2023-08-31\"),  # Summer  \n    (\"2023-09-01\", \"2023-11-30\")   # Fall\n]\n\ncollections = [\"sentinel-2-l2a\", \"landsat-c2-l2\"]\n\nplan = plan_multitemporal_collection(aoi, date_ranges, collections)\n\nprint(\"Multi-temporal Collection Plan:\")\nprint(f\"Total estimated scenes: {plan['total_scenes']}\")\n\nprint(\"\\nBy Date Range:\")\nfor date_range, collections in plan['by_date_range'].items():\n    print(f\"  {date_range}:\")\n    for collection, count in collections.items():\n        print(f\"    {collection}: {count} scenes\")\n\nprint(\"\\nBy Collection:\")\nfor collection, count in plan['by_collection'].items():\n    print(f\"  {collection}: {count} scenes\")\n\nMulti-temporal Collection Plan:\nTotal estimated scenes: 192\n\nBy Date Range:\n  2023-03-01_to_2023-05-31:\n    sentinel-2-l2a: 25 scenes\n    landsat-c2-l2: 27 scenes\n  2023-06-01_to_2023-08-31:\n    sentinel-2-l2a: 49 scenes\n    landsat-c2-l2: 37 scenes\n  2023-09-01_to_2023-11-30:\n    sentinel-2-l2a: 20 scenes\n    landsat-c2-l2: 34 scenes\n\nBy Collection:\n  sentinel-2-l2a: 94 scenes\n  landsat-c2-l2: 98 scenes\n\n\n\n\nQuality Filtering for ML\n\ndef filter_scenes_for_ml(items, max_cloud_cover=10, min_data_coverage=80):\n    \"\"\"Filter STAC items for ML training quality\"\"\"\n    \n    filtered_items = []\n    filter_stats = {\n        'total_input': len(items),\n        'passed_cloud_filter': 0,\n        'passed_data_filter': 0,\n        'final_count': 0\n    }\n    \n    for item in items:\n        # Check cloud cover\n        cloud_cover = item.properties.get('eo:cloud_cover', 100)\n        if cloud_cover &gt; max_cloud_cover:\n            continue\n        filter_stats['passed_cloud_filter'] += 1\n        \n        # Check data coverage (if available)\n        data_coverage = item.properties.get('s2:data_coverage_percentage', 100)\n        if data_coverage &lt; min_data_coverage:\n            continue\n        filter_stats['passed_data_filter'] += 1\n        \n        filtered_items.append(item)\n        filter_stats['final_count'] += 1\n    \n    return filtered_items, filter_stats\n\n# Apply quality filtering\nif items:\n    filtered_items, stats = filter_scenes_for_ml(items, max_cloud_cover=15)\n    \n    print(\"Quality Filtering Results:\")\n    print(f\"  Input scenes: {stats['total_input']}\")\n    print(f\"  Passed cloud filter (&lt;15%): {stats['passed_cloud_filter']}\")\n    print(f\"  Passed data filter (&gt;80%): {stats['passed_data_filter']}\")\n    print(f\"  Final count: {stats['final_count']}\")\n    print(f\"  Retention rate: {stats['final_count']/stats['total_input']*100:.1f}%\")\n\nQuality Filtering Results:\n  Input scenes: 15\n  Passed cloud filter (&lt;15%): 7\n  Passed data filter (&gt;80%): 7\n  Final count: 7\n  Retention rate: 46.7%\n\n\n\n\nMetadata Extraction for Training\n\ndef extract_training_metadata(items):\n    \"\"\"Extract metadata useful for ML training\"\"\"\n    \n    metadata_df = []\n    \n    for item in items:\n        metadata = {\n            'scene_id': item.id,\n            'collection': item.collection_id,\n            'datetime': item.datetime,\n            'cloud_cover': item.properties.get('eo:cloud_cover'),\n            'sun_azimuth': item.properties.get('s2:mean_solar_azimuth'),\n            'sun_elevation': item.properties.get('s2:mean_solar_zenith'),\n            'data_coverage': item.properties.get('s2:data_coverage_percentage'),\n            'processing_level': item.properties.get('processing:level'),\n            'spatial_resolution': item.properties.get('gsd'),\n            'epsg_code': item.properties.get('proj:epsg'),\n            'bbox': item.bbox,\n            'geometry_type': item.geometry['type']\n        }\n        \n        # Count available bands\n        band_count = len([k for k in item.assets.keys() if k.startswith('B')])\n        metadata['band_count'] = band_count\n        \n        metadata_df.append(metadata)\n    \n    return pd.DataFrame(metadata_df)\n\n# Extract metadata\nif items:\n    metadata_df = extract_training_metadata(items[:10])\n    \n    print(\"Training Metadata Summary:\")\n    print(f\"  Scenes: {len(metadata_df)}\")\n    print(f\"  Date range: {metadata_df['datetime'].min()} to {metadata_df['datetime'].max()}\")\n    print(f\"  Cloud cover range: {metadata_df['cloud_cover'].min()}% to {metadata_df['cloud_cover'].max()}%\")\n    print(f\"  Average bands per scene: {metadata_df['band_count'].mean():.1f}\")\n    \n    # Show first few rows\n    print(\"\\nFirst 3 scenes:\")\n    print(metadata_df[['scene_id', 'datetime', 'cloud_cover', 'band_count']].head(3).to_string(index=False))\n\nTraining Metadata Summary:\n  Scenes: 10\n  Date range: 2023-07-10 18:39:59.445083+00:00 to 2023-07-26 18:40:05.170226+00:00\n  Cloud cover range: 0.75% to 54.0%\n  Average bands per scene: 0.0\n\nFirst 3 scenes:\n                       scene_id                         datetime  cloud_cover  band_count\nLC08_L2SP_043035_20230726_02_T1 2023-07-26 18:40:05.170226+00:00        25.26           0\nLC08_L2SP_043034_20230726_02_T1 2023-07-26 18:39:41.283422+00:00         0.75           0\nLC09_L2SP_044034_20230725_02_T1 2023-07-25 18:45:40.240352+00:00        32.50           0"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#advanced-stac-operations",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#advanced-stac-operations",
    "title": "STAC APIs & Planetary Computer",
    "section": "Advanced STAC Operations",
    "text": "Advanced STAC Operations\n\nAggregating Collections\n\ndef compare_collections(catalog, collection_ids, aoi, date_range):\n    \"\"\"Compare multiple collections for the same area and time\"\"\"\n    \n    comparison = {}\n    \n    for collection_id in collection_ids:\n        try:\n            search = catalog.search(\n                collections=[collection_id],\n                intersects=aoi,\n                datetime=date_range,\n                limit=100\n            )\n            \n            items = list(search.items())\n            \n            if items:\n                # Calculate statistics\n                cloud_covers = [item.properties.get('eo:cloud_cover', 0) for item in items if item.properties.get('eo:cloud_cover') is not None]\n                \n                comparison[collection_id] = {\n                    'item_count': len(items),\n                    'avg_cloud_cover': np.mean(cloud_covers) if cloud_covers else None,\n                    'min_cloud_cover': np.min(cloud_covers) if cloud_covers else None,\n                    'temporal_coverage': (items[0].datetime, items[-1].datetime),\n                    'sample_bands': list(items[0].assets.keys())[:5]\n                }\n            else:\n                comparison[collection_id] = {'item_count': 0}\n                \n        except Exception as e:\n            comparison[collection_id] = {'error': str(e)}\n    \n    return comparison\n\n# Compare collections\ncomparison = compare_collections(\n    pc_catalog,\n    [\"sentinel-2-l2a\", \"landsat-c2-l2\"],\n    aoi,\n    \"2023-07-01/2023-07-31\"\n)\n\nprint(\"Collection Comparison:\")\nfor collection_id, stats in comparison.items():\n    print(f\"\\n{collection_id}:\")\n    if 'error' in stats:\n        print(f\"  Error: {stats['error']}\")\n    elif stats['item_count'] == 0:\n        print(\"  No items found\")\n    else:\n        print(f\"  Items found: {stats['item_count']}\")\n        if stats['avg_cloud_cover'] is not None:\n            print(f\"  Avg cloud cover: {stats['avg_cloud_cover']:.1f}%\")\n            print(f\"  Min cloud cover: {stats['min_cloud_cover']:.1f}%\")\n        print(f\"  Sample bands: {stats['sample_bands']}\")\n\nCollection Comparison:\n\nsentinel-2-l2a:\n  Items found: 50\n  Avg cloud cover: 8.1%\n  Min cloud cover: 0.0%\n  Sample bands: ['AOT', 'B01', 'B02', 'B03', 'B04']\n\nlandsat-c2-l2:\n  Items found: 15\n  Avg cloud cover: 20.3%\n  Min cloud cover: 0.8%\n  Sample bands: ['qa', 'ang', 'red', 'blue', 'drad']\n\n\n\n\nCreating Training Datasets\n\ndef create_training_manifest(filtered_items, output_bands=['B02', 'B03', 'B04', 'B08']):\n    \"\"\"Create a manifest file for ML training\"\"\"\n    \n    training_manifest = {\n        'dataset_info': {\n            'created_at': datetime.now().isoformat(),\n            'total_scenes': len(filtered_items),\n            'bands': output_bands,\n            'description': 'STAC-derived training dataset manifest'\n        },\n        'scenes': []\n    }\n    \n    for item in filtered_items:\n        scene_data = {\n            'scene_id': item.id,\n            'collection': item.collection_id,\n            'datetime': item.datetime.isoformat(),\n            'bbox': item.bbox,\n            'properties': {\n                'cloud_cover': item.properties.get('eo:cloud_cover'),\n                'sun_elevation': item.properties.get('s2:mean_solar_zenith'),\n                'epsg': item.properties.get('proj:epsg')\n            },\n            'band_urls': {}\n        }\n        \n        for band in output_bands:\n            if band in item.assets:\n                scene_data['band_urls'][band] = item.assets[band].href\n        \n        training_manifest['scenes'].append(scene_data)\n    \n    return training_manifest\n\n# Create training manifest\nif items:\n    filtered_items, _ = filter_scenes_for_ml(items[:5], max_cloud_cover=15)\n    manifest = create_training_manifest(filtered_items)\n    \n    print(\"Training Manifest Created:\")\n    print(f\"  Total scenes: {manifest['dataset_info']['total_scenes']}\")\n    print(f\"  Bands: {manifest['dataset_info']['bands']}\")\n    print(f\"  Created: {manifest['dataset_info']['created_at']}\")\n    \n    # Show first scene structure\n    if manifest['scenes']:\n        print(f\"\\nFirst scene structure:\")\n        first_scene = manifest['scenes'][0]\n        for key, value in first_scene.items():\n            if key == 'band_urls':\n                print(f\"  {key}: {list(value.keys())}\")\n            else:\n                print(f\"  {key}: {value}\")\n\nTraining Manifest Created:\n  Total scenes: 1\n  Bands: ['B02', 'B03', 'B04', 'B08']\n  Created: 2025-08-10T12:06:47.466015\n\nFirst scene structure:\n  scene_id: LC08_L2SP_043034_20230726_02_T1\n  collection: landsat-c2-l2\n  datetime: 2023-07-26T18:39:41.283422+00:00\n  bbox: [-121.86762720531041, 36.39286485893108, -119.22622808725025, 38.534055141068926]\n  properties: {'cloud_cover': 0.75, 'sun_elevation': None, 'epsg': None}\n  band_urls: []"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/stac_apis.html#summary",
    "href": "course-materials/extras/cheatsheets/stac_apis.html#summary",
    "title": "STAC APIs & Planetary Computer",
    "section": "Summary",
    "text": "Summary\nKey STAC concepts for GFM development:\n\nSTAC APIs provide standardized access to petabytes of earth observation data\nMicrosoft Planetary Computer offers free access to major satellite datasets\nQuality filtering is essential for ML training data preparation\nMulti-temporal collections enable time-series and change detection models\nMetadata extraction supports dataset organization and model training\nCross-collection searches maximize data availability and diversity\n\nEssential workflows: - Search and filter scenes by quality metrics - Extract and organize metadata for training - Create manifests linking STAC items to training pipelines - Compare collections to optimize data selection - Plan multi-temporal acquisitions for comprehensive datasets\nThese patterns enable scalable, reproducible access to satellite imagery for geospatial foundation model development."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html",
    "title": "TorchGeo Datasets & Transforms",
    "section": "",
    "text": "TorchGeo is a PyTorch domain library for geospatial data, providing datasets, samplers, transforms, and pre-trained models for satellite imagery and geospatial applications.\n\nimport torch\nimport torchgeo\nfrom torchgeo.datasets import RasterDataset, stack_samples\nfrom torchgeo.transforms import AugmentationSequential\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(f\"TorchGeo version: {torchgeo.__version__}\")\nprint(f\"PyTorch version: {torch.__version__}\")\n\nTorchGeo version: 0.7.1\nPyTorch version: 2.7.1"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#introduction-to-torchgeo",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#introduction-to-torchgeo",
    "title": "TorchGeo Datasets & Transforms",
    "section": "",
    "text": "TorchGeo is a PyTorch domain library for geospatial data, providing datasets, samplers, transforms, and pre-trained models for satellite imagery and geospatial applications.\n\nimport torch\nimport torchgeo\nfrom torchgeo.datasets import RasterDataset, stack_samples\nfrom torchgeo.transforms import AugmentationSequential\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(f\"TorchGeo version: {torchgeo.__version__}\")\nprint(f\"PyTorch version: {torch.__version__}\")\n\nTorchGeo version: 0.7.1\nPyTorch version: 2.7.1"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#core-dataset-classes",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#core-dataset-classes",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Core Dataset Classes",
    "text": "Core Dataset Classes\n\nRasterDataset basics\n\nfrom torchgeo.datasets import RasterDataset\nfrom torchgeo.samplers import RandomGeoSampler\nimport tempfile\nimport os\nfrom pathlib import Path\n\n# Create a simple custom dataset (not inheriting from RasterDataset for demo)\nfrom torchgeo.datasets import BoundingBox\nfrom rtree.index import Index, Property\n\nclass SampleGeoDataset:\n    \"\"\"Sample geospatial dataset for demonstration\"\"\"\n    \n    def __init__(self, transforms=None):\n        self.transforms = transforms\n        # Define dataset bounds\n        self.bounds = BoundingBox(-10.0, 10.0, -10.0, 10.0, 0, 100)\n        # Define resolution (meters per pixel)\n        self.res = 10.0  # 10 meter resolution\n        \n        # Create spatial index required by TorchGeo samplers\n        self.index = Index(interleaved=False, properties=Property(dimension=3))\n        # Add the dataset bounds to the index\n        self.index.insert(0, tuple(self.bounds))\n        \n    def __getitem__(self, query):\n        # Create synthetic data for demonstration\n        sample = {\n            'image': torch.rand(3, 256, 256),  # RGB image\n            'bbox': query,\n            'crs': 'EPSG:4326'\n        }\n        \n        if self.transforms:\n            sample = self.transforms(sample)\n            \n        return sample\n    \n    def __len__(self):\n        return 1000  # Arbitrary length for sampling\n\n# Initialize dataset\ndataset = SampleGeoDataset()\nprint(f\"Dataset created: {type(dataset).__name__}\")\nprint(f\"Dataset bounds: {dataset.bounds}\")\n\nDataset created: SampleGeoDataset\nDataset bounds: BoundingBox(minx=-10.0, maxx=10.0, miny=-10.0, maxy=10.0, mint=0, maxt=100)\n\n\n\n\nVisionDataset examples\n\nfrom torchgeo.datasets import RESISC45, EuroSAT\n\n# Note: These require downloaded data files\n# For demonstration, we show the usage patterns\n\n# RESISC45 - Remote sensing image scene classification\n# resisc45 = RESISC45(root='data/resisc45', download=True)\n# print(f\"RESISC45 classes: {len(resisc45.classes)}\")\n\n# EuroSAT - Sentinel-2 image classification  \n# eurosat = EuroSAT(root='data/eurosat', download=True)\n# print(f\"EuroSAT classes: {len(eurosat.classes)}\")\n\nprint(\"Vision dataset classes ready for use with downloaded data\")\n\nVision dataset classes ready for use with downloaded data"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#geospatial-sampling",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#geospatial-sampling",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Geospatial Sampling",
    "text": "Geospatial Sampling\n\nRandomGeoSampler\n\nfrom torchgeo.samplers import RandomGeoSampler, GridGeoSampler\nfrom torchgeo.datasets import BoundingBox\n\n# Define a region of interest\nroi = BoundingBox(\n    minx=-10.0, maxx=10.0,\n    miny=-10.0, maxy=10.0,\n    mint=0, maxt=100\n)\n\n# For demonstration, show sampler concepts without full implementation\nprint(\"TorchGeo Samplers:\")\nprint(\"- RandomGeoSampler: Randomly samples patches from spatial regions\")\nprint(\"- GridGeoSampler: Systematically samples patches in a grid pattern\") \nprint(\"- Units can be PIXELS or CRS (coordinate reference system)\")\nprint(f\"Sample ROI: {roi}\")\n\n# Note: Actual usage requires proper GeoDataset implementation\n# random_sampler = RandomGeoSampler(dataset=dataset, size=256, length=100, roi=roi)\n\nTorchGeo Samplers:\n- RandomGeoSampler: Randomly samples patches from spatial regions\n- GridGeoSampler: Systematically samples patches in a grid pattern\n- Units can be PIXELS or CRS (coordinate reference system)\nSample ROI: BoundingBox(minx=-10.0, maxx=10.0, miny=-10.0, maxy=10.0, mint=0, maxt=100)\n\n\n\n\nGridGeoSampler\n\n# Grid-based systematic sampling concept\nprint(\"GridGeoSampler Usage Pattern:\")\nprint(\"- size: Patch size in pixels (e.g., 256)\")\nprint(\"- stride: Step size between patches (e.g., 128 for overlap)\")\nprint(\"- roi: Region of interest as BoundingBox\")\nprint(\"- Provides systematic spatial coverage\")\n\n# Example conceptual usage:\n# grid_sampler = GridGeoSampler(dataset=dataset, size=256, stride=128, roi=roi)\n\nGridGeoSampler Usage Pattern:\n- size: Patch size in pixels (e.g., 256)\n- stride: Step size between patches (e.g., 128 for overlap)\n- roi: Region of interest as BoundingBox\n- Provides systematic spatial coverage"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#data-transforms",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#data-transforms",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Data Transforms",
    "text": "Data Transforms\n\nBasic transforms\n\nimport torchvision.transforms as T\nfrom torchgeo.transforms import AugmentationSequential\n\n# Standard computer vision transforms for preprocessing\nnormalization_transform = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n# Basic geometric augmentations\nbasic_augments = T.Compose([\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomVerticalFlip(p=0.5),\n])\n\nprint(\"Transform sequences created:\")\nprint(\"- Normalization transform for pretrained models\") \nprint(\"- Basic augmentations for training\")\nprint(\"- TorchGeo's AugmentationSequential preserves spatial relationships\")\n\nTransform sequences created:\n- Normalization transform for pretrained models\n- Basic augmentations for training\n- TorchGeo's AugmentationSequential preserves spatial relationships\n\n\n\n\nGeospatial-aware transforms\n\n# Create sample data for demonstration\nsample_image = torch.rand(3, 256, 256)\nsample_mask = torch.randint(0, 5, (256, 256))\n\n# Apply basic transforms\naugmented_image = basic_augments(sample_image)\nnormalized_image = normalization_transform(sample_image)\n\nprint(f\"Original image shape: {sample_image.shape}\")\nprint(f\"Augmented image shape: {augmented_image.shape}\")\nprint(f\"Normalized image range: [{normalized_image.min():.3f}, {normalized_image.max():.3f}]\")\n\n# TorchGeo's AugmentationSequential provides spatial awareness\nprint(\"\\nTorchGeo AugmentationSequential benefits:\")\nprint(\"- Preserves spatial relationships between image and mask\")\nprint(\"- Handles coordinate transformations\")\nprint(\"- Supports multi-modal data (image + labels + metadata)\")\n\nOriginal image shape: torch.Size([3, 256, 256])\nAugmented image shape: torch.Size([3, 256, 256])\nNormalized image range: [-2.118, 2.640]\n\nTorchGeo AugmentationSequential benefits:\n- Preserves spatial relationships between image and mask\n- Handles coordinate transformations\n- Supports multi-modal data (image + labels + metadata)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#working-with-real-satellite-data",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#working-with-real-satellite-data",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Working with Real Satellite Data",
    "text": "Working with Real Satellite Data\n\nLandsat dataset example\n\nfrom torchgeo.datasets import Landsat8\n\n# Note: Requires actual Landsat data\n# landsat = Landsat8(root='data/landsat8')\n\n# Define query for specific area and time\nquery = BoundingBox(\n    minx=-100.0, maxx=-99.0,  # Longitude\n    miny=40.0, maxy=41.0,     # Latitude  \n    mint=637110000,           # Time (Unix timestamp)\n    maxt=637196400\n)\n\n# Sample usage pattern:\n# sample = landsat[query]\n# print(f\"Landsat sample keys: {sample.keys()}\")\n\nprint(\"Landsat dataset pattern demonstrated\")\n\nLandsat dataset pattern demonstrated\n\n\n\n\nSentinel-2 dataset example\n\nfrom torchgeo.datasets import Sentinel2\n\n# Sentinel-2 usage pattern\n# sentinel = Sentinel2(root='data/sentinel2')\n# s2_sample = sentinel[query]\n\nprint(\"Sentinel-2 dataset pattern demonstrated\")\n\nSentinel-2 dataset pattern demonstrated"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#multi-modal-data-fusion",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#multi-modal-data-fusion",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Multi-modal Data Fusion",
    "text": "Multi-modal Data Fusion\n\nCombining datasets\n\nfrom torchgeo.datasets import IntersectionDataset, UnionDataset\n\n# Multi-modal data fusion concept\nprint(\"TorchGeo Dataset Fusion:\")\nprint(\"- IntersectionDataset: Combines data that exists in ALL datasets\")\nprint(\"- UnionDataset: Combines data that exists in ANY dataset\")\nprint(\"- Useful for multi-modal analysis (optical + SAR + DEM)\")\n\n# Example fusion workflow:\nprint(\"\\nTypical fusion workflow:\")\nprint(\"1. Load optical imagery dataset (Sentinel-2)\")\nprint(\"2. Load elevation dataset (DEM)\")\nprint(\"3. Load land cover dataset (labels)\")\nprint(\"4. Use IntersectionDataset to ensure spatial-temporal alignment\")\nprint(\"5. Sample consistent patches across all modalities\")\n\n# Note: Requires proper GeoDataset implementations\n# fused_ds = IntersectionDataset(optical_ds, dem_ds, landcover_ds)\n\nTorchGeo Dataset Fusion:\n- IntersectionDataset: Combines data that exists in ALL datasets\n- UnionDataset: Combines data that exists in ANY dataset\n- Useful for multi-modal analysis (optical + SAR + DEM)\n\nTypical fusion workflow:\n1. Load optical imagery dataset (Sentinel-2)\n2. Load elevation dataset (DEM)\n3. Load land cover dataset (labels)\n4. Use IntersectionDataset to ensure spatial-temporal alignment\n5. Sample consistent patches across all modalities\n\n\n\n\nStack samples utility\n\n# Create multiple samples to stack\nsamples = []\nfor i in range(4):\n    sample = {\n        'image': torch.rand(3, 64, 64),\n        'mask': torch.randint(0, 2, (64, 64)),\n        'elevation': torch.rand(1, 64, 64)\n    }\n    samples.append(sample)\n\n# Stack into batch\nbatch = stack_samples(samples)\n\nprint(f\"Batch image shape: {batch['image'].shape}\")\nprint(f\"Batch mask shape: {batch['mask'].shape}\")\nprint(f\"Batch elevation shape: {batch['elevation'].shape}\")\n\nBatch image shape: torch.Size([4, 3, 64, 64])\nBatch mask shape: torch.Size([4, 64, 64])\nBatch elevation shape: torch.Size([4, 1, 64, 64])"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#datamodule-for-training",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#datamodule-for-training",
    "title": "TorchGeo Datasets & Transforms",
    "section": "DataModule for Training",
    "text": "DataModule for Training\n\nLightning DataModule\n\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\n\nclass GeospatialDataModule(pl.LightningDataModule):\n    \"\"\"Data module for geospatial training\"\"\"\n    \n    def __init__(self, batch_size=32, num_workers=4):\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        \n    def setup(self, stage=None):\n        print(\"Setting up geospatial data module:\")\n        print(\"- Train/val split: 80/20\")\n        print(\"- Spatial sampling strategy\")\n        print(\"- Multi-worker data loading\")\n        \n    def train_dataloader(self):\n        print(\"Creating train dataloader with TorchGeo samplers\")\n        return None  # Would return actual DataLoader with GeoSampler\n    \n    def val_dataloader(self):\n        print(\"Creating validation dataloader\")\n        return None  # Would return actual DataLoader\n\n# Example usage pattern\nprint(\"PyTorch Lightning + TorchGeo Integration:\")\nprint(\"- Use GeoDataModule for spatial-aware data loading\")\nprint(\"- Combine with GeoSamplers for patch-based training\")\nprint(\"- Stack samples for batch processing\")\nprint(\"- Supports multi-modal geospatial data\")\n\ndatamodule = GeospatialDataModule(batch_size=8)\ndatamodule.setup()\n\nPyTorch Lightning + TorchGeo Integration:\n- Use GeoDataModule for spatial-aware data loading\n- Combine with GeoSamplers for patch-based training\n- Stack samples for batch processing\n- Supports multi-modal geospatial data\nSetting up geospatial data module:\n- Train/val split: 80/20\n- Spatial sampling strategy\n- Multi-worker data loading"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#pre-trained-models",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#pre-trained-models",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Pre-trained Models",
    "text": "Pre-trained Models\n\nUsing TorchGeo models\n\nfrom torchgeo.models import ResNet18_Weights\nimport torchvision.models as models\n\n# Load pre-trained weights for satellite imagery\n# weights = ResNet18_Weights.SENTINEL2_ALL_MOCO\n# model = models.resnet18(weights=weights)\n\n# For demonstration without actual weights:\nmodel = models.resnet18(pretrained=False)\nmodel.conv1 = torch.nn.Conv2d(\n    in_channels=12,  # Sentinel-2 has 12 bands\n    out_channels=64,\n    kernel_size=7,\n    stride=2,\n    padding=3,\n    bias=False\n)\n\nprint(f\"Model adapted for {model.conv1.in_channels} input channels\")\n\nModel adapted for 12 input channels\n\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n\n\n\n\n\nFine-tuning for classification\n\nimport torch.nn as nn\n\nclass GeospatialClassifier(nn.Module):\n    \"\"\"Classifier for geospatial data\"\"\"\n    \n    def __init__(self, backbone, num_classes=10):\n        super().__init__()\n        self.backbone = backbone\n        \n        # Replace classifier head\n        if hasattr(backbone, 'fc'):\n            in_features = backbone.fc.in_features\n            backbone.fc = nn.Linear(in_features, num_classes)\n        \n    def forward(self, x):\n        return self.backbone(x)\n\n# Create classifier\nclassifier = GeospatialClassifier(model, num_classes=10)\nprint(f\"Classifier created for {classifier.backbone.fc.out_features} classes\")\n\nClassifier created for 10 classes"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#visualization-and-inspection",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#visualization-and-inspection",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Visualization and Inspection",
    "text": "Visualization and Inspection\n\nPlotting samples\n\ndef plot_sample(sample, figsize=(12, 4)):\n    \"\"\"Plot a geospatial sample\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=figsize)\n    \n    # RGB image (first 3 channels)\n    if 'image' in sample:\n        image = sample['image']\n        if image.shape[0] &gt;= 3:\n            rgb = image[:3].permute(1, 2, 0)\n            # Normalize for display\n            rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n            axes[0].imshow(rgb)\n            axes[0].set_title('RGB Composite')\n            axes[0].axis('off')\n    \n    # Mask/labels\n    if 'mask' in sample:\n        mask = sample['mask']\n        axes[1].imshow(mask, cmap='tab10')\n        axes[1].set_title('Mask/Labels')\n        axes[1].axis('off')\n    \n    # Additional data (e.g., elevation)\n    if 'elevation' in sample:\n        elev = sample['elevation'].squeeze()\n        im = axes[2].imshow(elev, cmap='terrain')\n        axes[2].set_title('Elevation')\n        axes[2].axis('off')\n        plt.colorbar(im, ax=axes[2], shrink=0.8)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Create and plot a sample\ndemo_sample = {\n    'image': torch.rand(3, 128, 128),\n    'mask': torch.randint(0, 5, (128, 128)),\n    'elevation': torch.rand(1, 128, 128) * 1000\n}\n\nplot_sample(demo_sample)\n\n\n\n\n\n\n\n\n\n\nDataset statistics\n\ndef compute_dataset_stats(dataloader, num_samples=100):\n    \"\"\"Compute dataset statistics for normalization\"\"\"\n    \n    pixel_sum = torch.zeros(3)\n    pixel_squared_sum = torch.zeros(3)\n    num_pixels = 0\n    \n    for i, batch in enumerate(dataloader):\n        if i &gt;= num_samples:\n            break\n            \n        images = batch['image']\n        batch_size, channels, height, width = images.shape\n        num_pixels += batch_size * height * width\n        \n        pixel_sum += images.sum(dim=[0, 2, 3])\n        pixel_squared_sum += (images ** 2).sum(dim=[0, 2, 3])\n    \n    mean = pixel_sum / num_pixels\n    var = (pixel_squared_sum / num_pixels) - (mean ** 2)\n    std = torch.sqrt(var)\n    \n    return mean, std\n\n# Example usage (would require actual dataloader)\n# mean, std = compute_dataset_stats(train_loader)\n# print(f\"Dataset mean: {mean}\")\n# print(f\"Dataset std: {std}\")\n\nprint(\"Dataset statistics computation function ready\")\n\nDataset statistics computation function ready"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#advanced-features",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#advanced-features",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nCustom indices and bands\n\nclass SpectralIndices:\n    \"\"\"Common spectral indices for satellite imagery\"\"\"\n    \n    @staticmethod\n    def ndvi(red, nir):\n        \"\"\"Normalized Difference Vegetation Index\"\"\"\n        return (nir - red) / (nir + red + 1e-8)\n    \n    @staticmethod\n    def ndwi(green, nir):\n        \"\"\"Normalized Difference Water Index\"\"\"\n        return (green - nir) / (green + nir + 1e-8)\n    \n    @staticmethod\n    def evi(blue, red, nir, g=2.5, c1=6.0, c2=7.5, l=1.0):\n        \"\"\"Enhanced Vegetation Index\"\"\"\n        return g * (nir - red) / (nir + c1 * red - c2 * blue + l)\n\n# Example with Sentinel-2 bands (simulated)\ns2_image = torch.rand(12, 256, 256)  # 12 Sentinel-2 bands\n\n# Extract specific bands (0-indexed)\nblue = s2_image[1]    # B2\ngreen = s2_image[2]   # B3  \nred = s2_image[3]     # B4\nnir = s2_image[7]     # B8\n\n# Calculate indices\nndvi = SpectralIndices.ndvi(red, nir)\nndwi = SpectralIndices.ndwi(green, nir)\nevi = SpectralIndices.evi(blue, red, nir)\n\nprint(f\"NDVI shape: {ndvi.shape}, range: [{ndvi.min():.3f}, {ndvi.max():.3f}]\")\nprint(f\"NDWI shape: {ndwi.shape}, range: [{ndwi.min():.3f}, {ndwi.max():.3f}]\")\n\nNDVI shape: torch.Size([256, 256]), range: [-1.000, 1.000]\nNDWI shape: torch.Size([256, 256]), range: [-1.000, 1.000]\n\n\n\n\nTemporal data handling\n\nclass TemporalDataset:\n    \"\"\"Dataset for temporal satellite imagery\"\"\"\n    \n    def __init__(self, time_steps=5):\n        self.time_steps = time_steps\n        self.bounds = BoundingBox(-10.0, 10.0, -10.0, 10.0, 0, 100)\n        # Create spatial index\n        self.index = Index(interleaved=False, properties=Property(dimension=3))\n        self.index.insert(0, tuple(self.bounds))\n    \n    def __getitem__(self, query):\n        # Simulate temporal data\n        temporal_images = []\n        \n        for t in range(self.time_steps):\n            # Each time step has slightly different data\n            image = torch.rand(3, 256, 256) + t * 0.1\n            temporal_images.append(image)\n        \n        return {\n            'image': torch.stack(temporal_images, dim=0),  # [T, C, H, W]\n            'bbox': query,\n            'timestamps': torch.arange(self.time_steps)\n        }\n\n# Create temporal dataset\ntemporal_ds = TemporalDataset(time_steps=5)\nprint(\"Temporal dataset created for time series analysis\")\n\nTemporal dataset created for time series analysis"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#performance-optimization",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#performance-optimization",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Performance Optimization",
    "text": "Performance Optimization\n\nCaching and preprocessing\n\nclass CachedDataset:\n    \"\"\"Dataset with caching for repeated access\"\"\"\n    \n    def __init__(self, cache_size=1000):\n        self.cache = {}\n        self.cache_size = cache_size\n        self.bounds = BoundingBox(-10.0, 10.0, -10.0, 10.0, 0, 100)\n        # Create spatial index\n        self.index = Index(interleaved=False, properties=Property(dimension=3))\n        self.index.insert(0, tuple(self.bounds))\n    \n    def __getitem__(self, query):\n        query_key = str(query)\n        \n        if query_key in self.cache:\n            return self.cache[query_key]\n        \n        # Generate/load data\n        sample = {\n            'image': torch.rand(3, 256, 256),\n            'bbox': query\n        }\n        \n        # Cache if space available\n        if len(self.cache) &lt; self.cache_size:\n            self.cache[query_key] = sample\n        \n        return sample\n\nprint(\"Cached dataset implementation ready\")\n\nCached dataset implementation ready\n\n\n\n\nMemory-efficient loading\n\ndef create_efficient_dataloader(dataset, batch_size=32, num_workers=4):\n    \"\"\"Create memory-efficient dataloader\"\"\"\n    \n    sampler = RandomGeoSampler(dataset, size=256, length=1000)\n    \n    return DataLoader(\n        dataset,\n        sampler=sampler,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        collate_fn=stack_samples,\n        pin_memory=True,  # Faster GPU transfer\n        persistent_workers=True,  # Keep workers alive\n        prefetch_factor=2  # Prefetch batches\n    )\n\nprint(\"Efficient dataloader configuration ready\")\n\nEfficient dataloader configuration ready"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html#summary",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html#summary",
    "title": "TorchGeo Datasets & Transforms",
    "section": "Summary",
    "text": "Summary\nKey TorchGeo concepts: - RasterDataset: Base class for raster data - VisionDataset: Classification datasets (RESISC45, EuroSAT) - GeoSampler: Spatial sampling strategies - Transforms: Geospatial-aware data augmentation - DataModule: PyTorch Lightning integration - Multi-modal: Combining different data sources - Pre-trained models: Domain-specific model weights - Spectral indices: Vegetation, water, soil indices"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html",
    "title": "Xarray for Multi-dimensional Data",
    "section": "",
    "text": "Xarray is a powerful Python library for working with labeled, multi-dimensional arrays, particularly useful for climate and geospatial data.\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nprint(f\"Xarray version: {xr.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n\nXarray version: 2025.7.1\nNumPy version: 1.26.4"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#introduction-to-xarray",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#introduction-to-xarray",
    "title": "Xarray for Multi-dimensional Data",
    "section": "",
    "text": "Xarray is a powerful Python library for working with labeled, multi-dimensional arrays, particularly useful for climate and geospatial data.\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nprint(f\"Xarray version: {xr.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n\nXarray version: 2025.7.1\nNumPy version: 1.26.4"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#creating-sample-data",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#creating-sample-data",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Creating Sample Data",
    "text": "Creating Sample Data\n\nCreating DataArrays\n\n# Create sample temperature data\nnp.random.seed(42)\n\n# Coordinates\ntime = pd.date_range('2020-01-01', periods=365, freq='D')\nlat = np.linspace(25, 50, 25)  # Latitude\nlon = np.linspace(-125, -65, 60)  # Longitude\n\n# Create temperature data with seasonal pattern\ntemp_data = np.random.randn(365, 25, 60) * 5 + 20\nfor i, t in enumerate(time):\n    seasonal = 10 * np.sin(2 * np.pi * (t.dayofyear - 80) / 365)\n    temp_data[i] += seasonal\n\n# Create DataArray\ntemperature = xr.DataArray(\n    temp_data,\n    coords={\n        'time': time,\n        'lat': lat, \n        'lon': lon\n    },\n    dims=['time', 'lat', 'lon'],\n    attrs={\n        'units': 'degrees_Celsius',\n        'description': 'Daily temperature',\n        'source': 'Simulated data'\n    }\n)\n\nprint(f\"Temperature DataArray shape: {temperature.shape}\")\nprint(f\"Coordinates: {list(temperature.coords.keys())}\")\n\nTemperature DataArray shape: (365, 25, 60)\nCoordinates: ['time', 'lat', 'lon']\n\n\n\n\nCreating Datasets\n\n# Create precipitation data\nprecip_data = np.maximum(0, np.random.randn(365, 25, 60) * 2 + 1)\nprecipitation = xr.DataArray(\n    precip_data,\n    coords=temperature.coords,\n    dims=temperature.dims,\n    attrs={'units': 'mm', 'description': 'Daily precipitation'}\n)\n\n# Create humidity data\nhumidity_data = np.random.beta(0.7, 0.3, (365, 25, 60)) * 100\nhumidity = xr.DataArray(\n    humidity_data,\n    coords=temperature.coords,\n    dims=temperature.dims,\n    attrs={'units': 'percent', 'description': 'Relative humidity'}\n)\n\n# Combine into Dataset\nweather_ds = xr.Dataset({\n    'temperature': temperature,\n    'precipitation': precipitation,\n    'humidity': humidity\n})\n\nprint(f\"Dataset variables: {list(weather_ds.data_vars)}\")\nprint(f\"Dataset dimensions: {weather_ds.sizes}\")\n\nDataset variables: ['temperature', 'precipitation', 'humidity']\nDataset dimensions: Frozen({'time': 365, 'lat': 25, 'lon': 60})"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#basic-data-inspection",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#basic-data-inspection",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Basic Data Inspection",
    "text": "Basic Data Inspection\n\nDataset overview\n\n# Dataset info\nprint(\"Dataset structure:\")\nprint(weather_ds)\n\nprint(f\"\\nDataset size in memory: {weather_ds.nbytes / 1e6:.1f} MB\")\n\n# Variable info\nprint(f\"\\nTemperature statistics:\")\nprint(f\"  Mean: {weather_ds.temperature.mean().values:.2f}¬∞C\")\nprint(f\"  Min:  {weather_ds.temperature.min().values:.2f}¬∞C\") \nprint(f\"  Max:  {weather_ds.temperature.max().values:.2f}¬∞C\")\n\nDataset structure:\n&lt;xarray.Dataset&gt; Size: 13MB\nDimensions:        (time: 365, lat: 25, lon: 60)\nCoordinates:\n  * time           (time) datetime64[ns] 3kB 2020-01-01 ... 2020-12-30\n  * lat            (lat) float64 200B 25.0 26.04 27.08 ... 47.92 48.96 50.0\n  * lon            (lon) float64 480B -125.0 -124.0 -123.0 ... -66.02 -65.0\nData variables:\n    temperature    (time, lat, lon) float64 4MB 12.71 9.53 13.46 ... 9.767 15.39\n    precipitation  (time, lat, lon) float64 4MB 5.153 0.0 0.0 ... 1.206 0.0 0.0\n    humidity       (time, lat, lon) float64 4MB 23.86 14.75 ... 35.48 69.34\n\nDataset size in memory: 13.1 MB\n\nTemperature statistics:\n  Mean: 19.99¬∞C\n  Min:  -10.97¬∞C\n  Max:  51.09¬∞C\n\n\n\n\nCoordinate inspection\n\n# Examine coordinates\nprint(\"Time coordinate:\")\nprint(f\"  Start: {weather_ds.time.values[0]}\")\nprint(f\"  End: {weather_ds.time.values[-1]}\")\nprint(f\"  Frequency: daily\")\n\nprint(f\"\\nSpatial extent:\")\nprint(f\"  Latitude: {weather_ds.lat.min().values:.1f}¬∞ to {weather_ds.lat.max().values:.1f}¬∞\")\nprint(f\"  Longitude: {weather_ds.lon.min().values:.1f}¬∞ to {weather_ds.lon.max().values:.1f}¬∞\")\n\n# Check for missing values\nprint(f\"\\nMissing values:\")\nprint(f\"  Temperature: {weather_ds.temperature.isnull().sum().values}\")\nprint(f\"  Precipitation: {weather_ds.precipitation.isnull().sum().values}\")\n\nTime coordinate:\n  Start: 2020-01-01T00:00:00.000000000\n  End: 2020-12-30T00:00:00.000000000\n  Frequency: daily\n\nSpatial extent:\n  Latitude: 25.0¬∞ to 50.0¬∞\n  Longitude: -125.0¬∞ to -65.0¬∞\n\nMissing values:\n  Temperature: 0\n  Precipitation: 0"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#data-selection-and-indexing",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#data-selection-and-indexing",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Data Selection and Indexing",
    "text": "Data Selection and Indexing\n\nLabel-based selection\n\n# Select by coordinate values\nsummer_data = weather_ds.sel(time=slice('2020-06-01', '2020-08-31'))\nprint(f\"Summer data shape: {summer_data.temperature.shape}\")\n\n# Select specific coordinates\npoint_data = weather_ds.sel(lat=40, lon=-100, method='nearest')\nprint(f\"Point time series shape: {point_data.temperature.shape}\")\n\n# Select multiple points\nregion_data = weather_ds.sel(\n    lat=slice(30, 45),\n    lon=slice(-120, -90)\n)\nprint(f\"Regional data shape: {region_data.temperature.shape}\")\n\nSummer data shape: (92, 25, 60)\nPoint time series shape: (365,)\nRegional data shape: (365, 15, 30)\n\n\n\n\nInteger-based indexing\n\n# Index-based selection\nfirst_week = weather_ds.isel(time=slice(0, 7))\nprint(f\"First week shape: {first_week.temperature.shape}\")\n\n# Select every 10th day\nmonthly_subset = weather_ds.isel(time=slice(None, None, 10))\nprint(f\"Monthly subset shape: {monthly_subset.temperature.shape}\")\n\n# Select specific grid cells\ncorner_data = weather_ds.isel(lat=[0, -1], lon=[0, -1])\nprint(f\"Corner data shape: {corner_data.temperature.shape}\")\n\nFirst week shape: (7, 25, 60)\nMonthly subset shape: (37, 25, 60)\nCorner data shape: (365, 2, 2)\n\n\n\n\nBoolean masking\n\n# Temperature-based mask\nhot_days = weather_ds.where(weather_ds.temperature &gt; 25, drop=True)\nprint(f\"Hot days data points: {hot_days.temperature.count().values}\")\n\n# Multiple conditions\nsummer_hot = weather_ds.where(\n    (weather_ds.temperature &gt; 25) & \n    (weather_ds.time.dt.season == 'JJA'), \n    drop=True\n)\nprint(f\"Summer hot days: {summer_hot.temperature.count().values}\")\n\nHot days data points: 171081\nSummer hot days: 98382"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#statistical-operations",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#statistical-operations",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Statistical Operations",
    "text": "Statistical Operations\n\nBasic statistics\n\n# Global statistics\nglobal_stats = weather_ds.mean()\nprint(\"Global mean values:\")\nfor var in global_stats.data_vars:\n    print(f\"  {var}: {global_stats[var].values:.2f}\")\n\n# Temporal statistics\nmonthly_means = weather_ds.groupby('time.month').mean()\nprint(f\"Monthly means shape: {monthly_means.temperature.shape}\")\n\n# Spatial statistics\nspatial_mean = weather_ds.mean(['lat', 'lon'])\nprint(f\"Time series of spatial means: {spatial_mean.temperature.shape}\")\n\nGlobal mean values:\n  temperature: 19.99\n  precipitation: 1.39\n  humidity: 69.99\nMonthly means shape: (12, 25, 60)\nTime series of spatial means: (365,)\n\n\n\n\nAdvanced aggregations\n\n# Standard deviation\ntemp_std = weather_ds.temperature.std('time')\nprint(f\"Temperature variability shape: {temp_std.shape}\")\n\n# Percentiles\ntemp_p90 = weather_ds.temperature.quantile(0.9, 'time')\nprint(f\"90th percentile temperature shape: {temp_p90.shape}\")\n\n# Cumulative operations\ncumulative_precip = weather_ds.precipitation.cumsum('time')\nprint(f\"Cumulative precipitation shape: {cumulative_precip.shape}\")\n\nTemperature variability shape: (25, 60)\n90th percentile temperature shape: (25, 60)\nCumulative precipitation shape: (365, 25, 60)\n\n\n\n\nGroupby operations\n\n# Group by season\nseasonal_stats = weather_ds.groupby('time.season').mean()\nprint(f\"Seasonal statistics dimensions: {seasonal_stats.dims}\")\n\n# Group by month\nmonthly_stats = weather_ds.groupby('time.month').std()\nprint(f\"Monthly variability shape: {monthly_stats.temperature.shape}\")\n\n# Custom grouping\ndef get_decade(time):\n    return (time.dt.day - 1) // 10\n\ndecade_stats = weather_ds.groupby(get_decade(weather_ds.time)).mean()\nprint(\"Decade-based statistics created\")\n\nSeasonal statistics dimensions: FrozenMappingWarningOnValuesAccess({'season': 4, 'lat': 25, 'lon': 60})\nMonthly variability shape: (12, 25, 60)\nDecade-based statistics created"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#data-visualization",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#data-visualization",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nSimple plots\n\n# Time series plot at a specific location\nlocation_ts = weather_ds.sel(lat=40, lon=-100, method='nearest')\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Temperature time series\nlocation_ts.temperature.plot(ax=axes[0], color='red')\naxes[0].set_title('Temperature Time Series')\naxes[0].set_ylabel('Temperature (¬∞C)')\n\n# Precipitation time series  \nlocation_ts.precipitation.plot(ax=axes[1], color='blue')\naxes[1].set_title('Precipitation Time Series')\naxes[1].set_ylabel('Precipitation (mm)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSpatial maps\n\n# Plot spatial maps for specific dates\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Summer and winter temperature maps\nsummer_temp = weather_ds.temperature.sel(time='2020-07-15')\nwinter_temp = weather_ds.temperature.sel(time='2020-01-15')\n\nsummer_temp.plot(ax=axes[0,0], cmap='Reds', add_colorbar=True)\naxes[0,0].set_title('Summer Temperature (July 15)')\n\nwinter_temp.plot(ax=axes[0,1], cmap='Blues', add_colorbar=True)\naxes[0,1].set_title('Winter Temperature (January 15)')\n\n# Annual mean temperature and precipitation\nannual_temp_mean = weather_ds.temperature.mean('time')\nannual_precip_sum = weather_ds.precipitation.sum('time')\n\nannual_temp_mean.plot(ax=axes[1,0], cmap='RdYlBu_r', add_colorbar=True)\naxes[1,0].set_title('Annual Mean Temperature')\n\nannual_precip_sum.plot(ax=axes[1,1], cmap='BuPu', add_colorbar=True)\naxes[1,1].set_title('Annual Total Precipitation')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#data-manipulation-and-processing",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#data-manipulation-and-processing",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Data Manipulation and Processing",
    "text": "Data Manipulation and Processing\n\nMathematical operations\n\n# Convert Celsius to Fahrenheit\ntemp_fahrenheit = weather_ds.temperature * 9/5 + 32\ntemp_fahrenheit.attrs['units'] = 'degrees_Fahrenheit'\n\nprint(f\"Temperature in F: {temp_fahrenheit.mean().values:.1f}¬∞F\")\n\n# Calculate derived variables\n# Heat index approximation (simplified)\nheat_index = (weather_ds.temperature + weather_ds.humidity * 0.1)\nheat_index.attrs['description'] = 'Simplified heat index'\n\n# Daily temperature range\ndaily_temp_range = weather_ds.temperature.max(['lat', 'lon']) - weather_ds.temperature.min(['lat', 'lon'])\nprint(f\"Daily temperature range shape: {daily_temp_range.shape}\")\n\nTemperature in F: 68.0¬∞F\nDaily temperature range shape: (365,)\n\n\n\n\nResampling and interpolation\n\n# Temporal resampling\nweekly_data = weather_ds.resample(time='W').mean()\nprint(f\"Weekly data shape: {weekly_data.temperature.shape}\")\n\nmonthly_data = weather_ds.resample(time='M').mean()\nprint(f\"Monthly data shape: {monthly_data.temperature.shape}\")\n\n# Interpolation\n# Create higher resolution coordinates\nhigh_res_lat = np.linspace(25, 50, 50)  # Double resolution\nhigh_res_lon = np.linspace(-125, -65, 120)\n\n# Interpolate to higher resolution\nhigh_res_data = weather_ds.interp(lat=high_res_lat, lon=high_res_lon)\nprint(f\"High resolution shape: {high_res_data.temperature.shape}\")\n\nWeekly data shape: (53, 25, 60)\nMonthly data shape: (12, 25, 60)\n\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/xarray/groupers.py:509: FutureWarning:\n\n'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n\n\n\nHigh resolution shape: (365, 50, 120)\n\n\n\n\nRolling operations\n\n# Rolling mean (7-day moving average)\nrolling_temp = weather_ds.temperature.rolling(time=7, center=True).mean()\nprint(f\"7-day rolling mean shape: {rolling_temp.shape}\")\n\n# Rolling sum for precipitation (weekly totals)\nweekly_precip = weather_ds.precipitation.rolling(time=7).sum()\nprint(f\"Weekly precipitation totals shape: {weekly_precip.shape}\")\n\n7-day rolling mean shape: (365, 25, 60)\nWeekly precipitation totals shape: (365, 25, 60)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#working-with-real-netcdf-files",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#working-with-real-netcdf-files",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Working with Real NetCDF Files",
    "text": "Working with Real NetCDF Files\n\nFile I/O operations\n\n# Save dataset to NetCDF\nweather_ds.to_netcdf('sample_weather_data.nc')\nprint(\"Dataset saved to NetCDF file\")\n\n# Load dataset from file\nloaded_ds = xr.open_dataset('sample_weather_data.nc')\nprint(f\"Loaded dataset variables: {list(loaded_ds.data_vars)}\")\n\n# Open multiple files (example pattern)\n# multi_file_ds = xr.open_mfdataset('weather_*.nc', combine='by_coords')\n\nDataset saved to NetCDF file\nLoaded dataset variables: ['temperature', 'precipitation', 'humidity']\n\n\n\n\nChunking and Dask integration\n\n# Create chunked dataset for large data\nchunked_ds = weather_ds.chunk({'time': 30, 'lat': 10, 'lon': 20})\nprint(f\"Chunked dataset: {chunked_ds.temperature}\")\n\n# Lazy operations with chunked data\nlazy_mean = chunked_ds.temperature.mean()\nprint(f\"Lazy computation created: {type(lazy_mean.data)}\")\n\n# Compute result\nactual_mean = lazy_mean.compute()\nprint(f\"Computed mean: {actual_mean.values:.2f}\")\n\nChunked dataset: &lt;xarray.DataArray 'temperature' (time: 365, lat: 25, lon: 60)&gt; Size: 4MB\ndask.array&lt;xarray-temperature, shape=(365, 25, 60), dtype=float64, chunksize=(30, 10, 20), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-30\n  * lat      (lat) float64 200B 25.0 26.04 27.08 28.12 ... 47.92 48.96 50.0\n  * lon      (lon) float64 480B -125.0 -124.0 -123.0 ... -67.03 -66.02 -65.0\nAttributes:\n    units:        degrees_Celsius\n    description:  Daily temperature\n    source:       Simulated data\nLazy computation created: &lt;class 'dask.array.core.Array'&gt;\nComputed mean: 19.99"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#advanced-operations",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#advanced-operations",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Advanced Operations",
    "text": "Advanced Operations\n\nCoordinate operations\n\n# Add new coordinates\nweather_ds_with_doy = weather_ds.assign_coords(\n    day_of_year=weather_ds.time.dt.dayofyear\n)\n\n# Stack/unstack dimensions\nstacked = weather_ds.stack(location=['lat', 'lon'])\nprint(f\"Stacked dimensions: {stacked.temperature.dims}\")\n\nunstacked = stacked.unstack('location')\nprint(f\"Unstacked back to: {unstacked.temperature.dims}\")\n\nStacked dimensions: ('time', 'location')\nUnstacked back to: ('time', 'lat', 'lon')\n\n\n\n\nApply functions\n\n# Apply custom function along dimension\ndef temp_category(temp_array):\n    \"\"\"Categorize temperature\"\"\"\n    return xr.where(temp_array &lt; 0, 'cold',\n                   xr.where(temp_array &lt; 20, 'mild', 'warm'))\n\ntemp_categories = xr.apply_ufunc(\n    temp_category,\n    weather_ds.temperature,\n    dask='allowed',\n    output_dtypes=[object]\n)\n\nprint(\"Temperature categorization applied\")\n\nTemperature categorization applied\n\n\n\n\nMerge and concatenate\n\n# Split dataset by time\nfirst_half = weather_ds.isel(time=slice(0, 182))\nsecond_half = weather_ds.isel(time=slice(182, None))\n\n# Concatenate back together\nfull_dataset = xr.concat([first_half, second_half], dim='time')\nprint(f\"Concatenated dataset shape: {full_dataset.temperature.shape}\")\n\n# Merge different datasets\nelevation_data = xr.DataArray(\n    np.random.randint(0, 3000, (25, 60)),\n    coords={'lat': lat, 'lon': lon},\n    dims=['lat', 'lon'],\n    attrs={'units': 'meters', 'description': 'Elevation'}\n)\n\nmerged_ds = weather_ds.merge({'elevation': elevation_data})\nprint(f\"Merged dataset variables: {list(merged_ds.data_vars)}\")\n\nConcatenated dataset shape: (365, 25, 60)\nMerged dataset variables: ['temperature', 'precipitation', 'humidity', 'elevation']"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#performance-tips-and-best-practices",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#performance-tips-and-best-practices",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Performance Tips and Best Practices",
    "text": "Performance Tips and Best Practices\n\nMemory management\n\n# Check memory usage\nprint(f\"Dataset memory usage: {weather_ds.nbytes / 1e6:.1f} MB\")\n\n# Use lazy loading for large files\n# lazy_ds = xr.open_dataset('large_file.nc', chunks={'time': 100})\n\n# Close files when done\nloaded_ds.close()\nprint(\"File closed to free memory\")\n\nDataset memory usage: 13.1 MB\nFile closed to free memory\n\n\n\n\nEfficient operations\n\n# Use vectorized operations\nefficient_calc = weather_ds.temperature - weather_ds.temperature.mean('time')\nprint(\"Efficient anomaly calculation completed\")\n\n# Avoid loops when possible - use built-in functions\nmonthly_anomalies = weather_ds.groupby('time.month') - weather_ds.groupby('time.month').mean()\nprint(\"Monthly anomalies calculated efficiently\")\n\nEfficient anomaly calculation completed\nMonthly anomalies calculated efficiently"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html#summary",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html#summary",
    "title": "Xarray for Multi-dimensional Data",
    "section": "Summary",
    "text": "Summary\nKey Xarray concepts: - DataArrays: Labeled, multi-dimensional arrays - Datasets: Collections of DataArrays with shared coordinates\n- Coordinates: Labels for array dimensions - Selection: Label-based (.sel) and integer-based (.isel) - GroupBy: Split-apply-combine operations - Resampling: Temporal aggregation and frequency conversion - I/O: Reading/writing NetCDF and other formats - Dask integration: Lazy evaluation for large datasets"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html",
    "title": "GFM Architecture Cheatsheet",
    "section": "",
    "text": "Quick, student-facing reference for how our Geospatial Foundation Model (GFM) is organized, what each part does, and where to look as we go from MVP to more capable systems."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#why-this-cheatsheet",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#why-this-cheatsheet",
    "title": "GFM Architecture Cheatsheet",
    "section": "",
    "text": "Quick, student-facing reference for how our Geospatial Foundation Model (GFM) is organized, what each part does, and where to look as we go from MVP to more capable systems."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#roadmap-at-a-glance",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#roadmap-at-a-glance",
    "title": "GFM Architecture Cheatsheet",
    "section": "Roadmap at a glance",
    "text": "Roadmap at a glance\n\n\n\n\n\ngraph TD\n  A[Week 1: Data Foundations] --&gt; B[Week 2: Attention & Blocks]\n  B --&gt; C[Week 3: Full Encoder]\n  C --&gt; D[Week 4: MAE Pretraining]\n  D --&gt; E[Week 5: Training Loop]\n  E --&gt; F[Week 6: Eval & Viz]\n  F --&gt; G[Week 7+: Interop & Scale]"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#minimal-structure-youll-use",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#minimal-structure-youll-use",
    "title": "GFM Architecture Cheatsheet",
    "section": "Minimal structure you‚Äôll use",
    "text": "Minimal structure you‚Äôll use\ngeogfm/\n  core/\n    config.py                # Minimal typed configs for model, data, training\n  data/\n    loaders.py               # build_dataloader(...)\n    datasets/\n      stac_dataset.py        # Simple STAC-backed dataset\n    transforms/\n      normalization.py       # Per-channel normalization\n      patchify.py            # Extract fixed-size patches\n  modules/\n    attention/\n      multihead_attention.py     # Standard MHA (from scratch)\n    embeddings/\n      patch_embedding.py         # Conv patch embedding\n      positional_encoding.py     # Simple positional encoding\n    blocks/\n      transformer_block.py       # PreNorm block (MHA + MLP)\n    heads/\n      reconstruction_head.py     # Lightweight decoder/readout\n    losses/\n      mae_loss.py                # Masked reconstruction loss\n  models/\n    gfm_vit.py                   # GeoViT-style encoder\n    gfm_mae.py                   # MAE wrapper (masking + encoder + head)\n  training/\n    optimizer.py                 # AdamW builder\n    loop.py                      # fit/train_step/eval_step with basic checkpointing\n  evaluation/\n    visualization.py             # Visualize inputs vs reconstructions\n\n# Outside the package (repo root)\nconfigs/   # Small YAML/JSON run configs\ntests/     # Unit tests\ndata/      # Datasets, splits, stats, build scripts"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#what-each-part-does-one-liners",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#what-each-part-does-one-liners",
    "title": "GFM Architecture Cheatsheet",
    "section": "What each part does (one-liners)",
    "text": "What each part does (one-liners)\n\ncore/config.py: Typed configs for model/data/training; keeps parameters organized.\ndata/datasets/stac_dataset.py: Reads imagery + metadata (e.g., STAC), returns tensors.\ndata/transforms/normalization.py: Normalizes channels using precomputed stats.\ndata/transforms/patchify.py: Turns large images into uniform patches for ViT.\ndata/loaders.py: Builds PyTorch DataLoaders for train/val.\nmodules/embeddings/patch_embedding.py: Projects image patches into token vectors.\nmodules/embeddings/positional_encoding.py: Adds position info to tokens.\nmodules/attention/multihead_attention.py: Lets tokens attend to each other.\nmodules/blocks/transformer_block.py: Core transformer layer (attention + MLP).\nmodules/heads/reconstruction_head.py: Reconstructs pixels from encoded tokens.\nmodules/losses/mae_loss.py: Computes masked reconstruction loss for MAE.\nmodels/gfm_vit.py: Assembles the encoder backbone from blocks.\nmodels/gfm_mae.py: Wraps encoder with masking + reconstruction for pretraining.\ntraining/optimizer.py: Creates AdamW with common defaults.\ntraining/loop.py: Runs epochs, backprop, validation, and simple checkpoints.\nevaluation/visualization.py: Plots sample inputs and reconstructions."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#from-scratch-vs-library-backed",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#from-scratch-vs-library-backed",
    "title": "GFM Architecture Cheatsheet",
    "section": "From-scratch vs library-backed",
    "text": "From-scratch vs library-backed\n\nUse PyTorch for Dataset/DataLoader, AdamW, schedulers, AMP, checkpointing.\nBuild core blocks from scratch first: PatchEmbedding, MHA, TransformerBlock, MAE loss/head.\nLater, swap in optimized options when needed:\n\ntorch.nn.MultiheadAttention, timm ViT blocks, FlashAttention\nTorchGeo datasets/transforms, torchvision/kornia/albumentations\ntorchmetrics for metrics; accelerate/lightning for training scale-up"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#quick-start-conceptual",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#quick-start-conceptual",
    "title": "GFM Architecture Cheatsheet",
    "section": "Quick start (conceptual)",
    "text": "Quick start (conceptual)\nfrom geogfm.core.config import ModelConfig, DataConfig, TrainConfig\nfrom geogfm.models.gfm_vit import GeoViTBackbone\nfrom geogfm.models.gfm_mae import MaskedAutoencoder\nfrom geogfm.data.loaders import build_dataloader\nfrom geogfm.training.loop import fit\n\nmodel_cfg = ModelConfig(architecture=\"gfm_vit\", embed_dim=768, depth=12, image_size=224)\ndata_cfg = DataConfig(dataset=\"stac\", patch_size=16, num_workers=8)\ntrain_cfg = TrainConfig(epochs=1, batch_size=8, optimizer={\"name\": \"adamw\", \"lr\": 2e-4})\n\nencoder = GeoViTBackbone(model_cfg)\nmodel = MaskedAutoencoder(model_cfg, encoder)\ntrain_dl, val_dl = build_dataloader(data_cfg)\nfit(model, (train_dl, val_dl), train_cfg)\nWhat to notice: - The encoder and MAE wrapper are separate so the encoder can be reused for other tasks. - Data transforms (normalize/patchify) are decoupled from the model and driven by config."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#where-data-lives-vs-dataset-code",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#where-data-lives-vs-dataset-code",
    "title": "GFM Architecture Cheatsheet",
    "section": "Where data lives vs dataset code",
    "text": "Where data lives vs dataset code\n\ndata/ (repo root): datasets, splits, stats, caches, and build scripts (e.g., STAC builders). No Python package imports here.\ngeogfm/data/datasets/: pure Python classes (subclass torch.utils.data.Dataset) that read from paths provided via configs. No real data inside the package.\n\nWhy: separates large mutable artifacts (datasets) from installable, testable code."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#mvp-vs-later-phases",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#mvp-vs-later-phases",
    "title": "GFM Architecture Cheatsheet",
    "section": "MVP vs later phases",
    "text": "MVP vs later phases\n\nMVP (Weeks 1‚Äì6): files shown above; single-node training; basic logging/viz.\nPhase 2 (Weeks 5‚Äì7+): AMP, scheduler, simple metrics (PSNR/SSIM), samplers, light registry.\nPhase 3 (Weeks 7‚Äì10): interop (HF/timm/TorchGeo), task heads, inference tiling, model hub/compat."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#extended-reference-structure-for-context",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#extended-reference-structure-for-context",
    "title": "GFM Architecture Cheatsheet",
    "section": "Extended reference structure (for context)",
    "text": "Extended reference structure (for context)\ngeogfm/\n  core/{registry.py, config.py, types.py, utils.py}\n  data/{loaders.py, samplers.py, datasets/*, transforms/*, tokenizers/*}\n  modules/{attention/*, embeddings/*, blocks/*, losses/*, heads/*, adapters/*}\n  models/{gfm_vit.py, gfm_mae.py, prithvi_compat.py, hub/*}\n  tasks/{pretraining_mae.py, classification.py, segmentation.py, change_detection.py, retrieval.py}\n  training/{loop.py, optimizer.py, scheduler.py, mixed_precision.py, callbacks.py, ema.py, checkpointing.py}\n  evaluation/{metrics.py, probes.py, visualization.py, nearest_neighbors.py}\n  inference/{serving.py, tiling.py, sliding_window.py, postprocess.py}\n  interoperability/{huggingface.py, timm.py, torchgeo.py}\n  utils/{logging.py, distributed.py, io.py, profiling.py, seed.py}"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/gfm_architecture.html#week-mapping-quick-reference",
    "href": "course-materials/extras/cheatsheets/gfm_architecture.html#week-mapping-quick-reference",
    "title": "GFM Architecture Cheatsheet",
    "section": "Week mapping (quick reference)",
    "text": "Week mapping (quick reference)\n\nWeek 1: data (data/datasets, data/transforms, data/loaders, core/config.py)\nWeek 2: attention/embeddings/blocks (modules/)\nWeek 3: architecture (models/gfm_vit.py, modules/heads/...)\nWeek 4: MAE (models/gfm_mae.py, modules/losses/mae_loss.py)\nWeek 5: training (training/optimizer.py, training/loop.py)\nWeek 6: viz/metrics (evaluation/visualization.py)\nWeeks 7‚Äì10: interop, tasks, inference, larger models (e.g., Prithvi)"
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html",
    "href": "course-materials/c00a-foundation_model_architectures.html",
    "title": "Foundation Model Architectures",
    "section": "",
    "text": "Foundation models represent a paradigm shift in machine learning, where large-scale models are trained on diverse, unlabeled data and then adapted to various downstream tasks. The term was coined by the Stanford HAI Center to describe models like GPT-3, BERT, and CLIP that serve as a ‚Äúfoundation‚Äù for numerous applications.\nThis cheatsheet provides a comprehensive comparison between Language Models (LLMs) and Geospatial Foundation Models (GFMs), examining their architectures, training procedures, and deployment strategies. While LLMs process discrete text tokens, GFMs work with continuous multi-spectral imagery, requiring fundamentally different approaches to tokenization, positional encoding, and objective functions.\nKey Resources:\n\nAttention Is All You Need - Original Transformer paper\nAn Image is Worth 16x16 Words - Vision Transformer (ViT)\nMasked Autoencoders Are Scalable Vision Learners - MAE approach\nPrithvi Foundation Model - IBM/NASA geospatial model\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModel, AutoConfig\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")  # Always False on Mac\n\nmps_available = torch.backends.mps.is_available()\nprint(f\"MPS available: {mps_available}\")\n\nif mps_available:\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n\nPyTorch version: 2.7.1\nCUDA available: False\nMPS available: True\nUsing device: mps\n\n\n\n\n\nThe development of foundation models represents a convergence of several key technological breakthroughs. The journey from symbolic AI to modern foundation models reflects both advances in computational power and fundamental insights about learning representations from large-scale data.\n\n\nThe transformer architecture emerged from decades of research in neural networks, attention mechanisms, and sequence modeling. The 2017 paper ‚ÄúAttention Is All You Need‚Äù introduced the transformer, which became the foundation for both language and vision models.\nCritical Developments:\n\n1950s-1990s: Symbolic AI dominated, with rule-based systems and early neural networks\n2012: AlexNet‚Äôs ImageNet victory sparked the deep learning revolution\n2017: The Transformer architecture introduced self-attention and parallelizable training\n2018-2020: BERT and GPT families demonstrated the power of pre-trained language models\n2021-2024: Scaling laws, ChatGPT, and multimodal models like CLIP\n\nFoundation Model Evolution Timeline:\n\n2017 - Transformer: Base architecture introduced\n2018 - BERT-Base: 110M parameters\n2019 - GPT-2: 1.5B parameters\n\n2020 - GPT-3: 175B parameters\n2021 - ViT-Large: 307M parameters for vision\n2022 - PaLM: 540B parameters\n2023 - GPT-4: ~1T parameters (estimated)\n2024 - Claude-3: Multi-modal capabilities\n\n\n\n\n\nThe transformer architecture revolutionized deep learning by introducing the self-attention mechanism, which allows models to process sequences in parallel rather than sequentially. This architecture forms the backbone of both language models (GPT, BERT) and vision models (ViT, DINO).\nCore Components:\n\nMulti-Head Attention: Allows the model to attend to different representation subspaces (see Self-attention)\nFeed-Forward Networks: Point-wise processing with GELU activation (see Multilayer perceptron)\nResidual Connections: Enable training of very deep networks (see Residual neural network)\nLayer Normalization: Stabilizes training and improves convergence (see Layer normalization)\n\nKey terms (quick definitions):\n\nSelf-Attention (SA): A mechanism where each token attends to every other token (including itself) to compute a weighted combination of their representations.\nQueries (Q), Keys (K), Values (V): Learned linear projections of the input. Attention scores are computed from Q¬∑K·µÄ; those scores weight V to produce the output (see Attention (machine learning)).\nMulti-Head Attention (MHA): Runs several SA ‚Äúheads‚Äù in parallel on different subspaces, then concatenates and projects the results. This lets the model capture diverse relationships (see Self-attention).\nFeed-Forward Network (FFN/MLP): A small two-layer MLP (Multi-Layer Perceptron) applied to each token independently; expands the dimension then projects back (often with GELU activation) (see Multilayer perceptron).\nResidual Connection (Skip): Adds the input of a sub-layer to its output, helping gradients flow and preserving information (see Residual neural network).\nLayer Normalization (LayerNorm, LN): Normalizes features per token to stabilize training. The ordering with residuals defines Post-LN vs Pre-LN variants (see Layer normalization).\nGELU: Gaussian Error Linear Unit activation; smoother than ReLU and standard in transformers (see Gaussian error linear unit).\nTensor shape convention: Using batch_first=True, tensors are [batch_size, seq_len, embed_dim].\nAbbreviations used: B = batch size, S = sequence length (or number of tokens/patches), E = embedding dimension, H = number of attention heads.\n\nThe following implementation demonstrates a simplified transformer block following the original architecture:\n\nclass SimpleTransformerBlock(nn.Module):\n    \"\"\"Transformer block with Multi-Head Self-Attention (MHSA) and MLP.\n\n    Inputs/Outputs:\n      - x: [batch_size, seq_len, embed_dim]\n      - returns: same shape as input\n\n    Structure (Post-LN variant):\n      x = LN(x + MHSA(x))\n      x = LN(x + MLP(x))\n    \"\"\"\n\n    def __init__(self, embed_dim=768, num_heads=8, mlp_ratio=4):\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        # Multi-Head Self-Attention (MHSA)\n        # - batch_first=True ‚Üí tensors are [B, S, E]\n        # - Self-attention uses Q=K=V=linear(x) by default\n        self.attention = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            batch_first=True,\n        )\n\n        # Position-wise feed-forward network (applied independently to each token)\n        hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),  # expand\n            nn.GELU(),                         # nonlinearity\n            nn.Linear(hidden_dim, embed_dim),  # project back\n        )\n\n        # LayerNorms used after residual additions (Post-LN)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # x: [B, S, E]\n\n        # Multi-head self-attention\n        # attn_out: [B, S, E]; attn_weights (ignored here) would be [B, num_heads, S, S]\n        attn_out, _ = self.attention(x, x, x)  # Q=K=V=x (self-attention)\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm1(x + attn_out)\n\n        # Position-wise MLP\n        mlp_out = self.mlp(x)  # [B, S, E]\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm2(x + mlp_out)\n\n        return x  # shape preserved: [B, S, E]\n\n\n# Example usage and shape checks\ntransformer_block = SimpleTransformerBlock(embed_dim=768, num_heads=8)\nsample_input = torch.randn(2, 100, 768)  # [batch_size, seq_len, embed_dim]\noutput = transformer_block(sample_input)\n\nprint(f\"Input shape:  {sample_input.shape}\")\nprint(f\"Output shape: {output.shape}\")  # should match input\nprint(f\"Parameters:  {sum(p.numel() for p in transformer_block.parameters()):,}\")\n\nInput shape:  torch.Size([2, 100, 768])\nOutput shape: torch.Size([2, 100, 768])\nParameters:  7,087,872\n\n\nWhat to notice: - Shapes are stable through the block: [B, S, E] ‚Üí [B, S, E]. - Residuals + LayerNorm appear after each sublayer (Post-LN variant). - MLP expands to mlp_ratio √ó embed_dim then projects back."
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#introduction-to-foundation-model-architectures",
    "href": "course-materials/c00a-foundation_model_architectures.html#introduction-to-foundation-model-architectures",
    "title": "Foundation Model Architectures",
    "section": "",
    "text": "Foundation models represent a paradigm shift in machine learning, where large-scale models are trained on diverse, unlabeled data and then adapted to various downstream tasks. The term was coined by the Stanford HAI Center to describe models like GPT-3, BERT, and CLIP that serve as a ‚Äúfoundation‚Äù for numerous applications.\nThis cheatsheet provides a comprehensive comparison between Language Models (LLMs) and Geospatial Foundation Models (GFMs), examining their architectures, training procedures, and deployment strategies. While LLMs process discrete text tokens, GFMs work with continuous multi-spectral imagery, requiring fundamentally different approaches to tokenization, positional encoding, and objective functions.\nKey Resources:\n\nAttention Is All You Need - Original Transformer paper\nAn Image is Worth 16x16 Words - Vision Transformer (ViT)\nMasked Autoencoders Are Scalable Vision Learners - MAE approach\nPrithvi Foundation Model - IBM/NASA geospatial model\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModel, AutoConfig\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")  # Always False on Mac\n\nmps_available = torch.backends.mps.is_available()\nprint(f\"MPS available: {mps_available}\")\n\nif mps_available:\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n\nPyTorch version: 2.7.1\nCUDA available: False\nMPS available: True\nUsing device: mps\n\n\n\n\n\nThe development of foundation models represents a convergence of several key technological breakthroughs. The journey from symbolic AI to modern foundation models reflects both advances in computational power and fundamental insights about learning representations from large-scale data.\n\n\nThe transformer architecture emerged from decades of research in neural networks, attention mechanisms, and sequence modeling. The 2017 paper ‚ÄúAttention Is All You Need‚Äù introduced the transformer, which became the foundation for both language and vision models.\nCritical Developments:\n\n1950s-1990s: Symbolic AI dominated, with rule-based systems and early neural networks\n2012: AlexNet‚Äôs ImageNet victory sparked the deep learning revolution\n2017: The Transformer architecture introduced self-attention and parallelizable training\n2018-2020: BERT and GPT families demonstrated the power of pre-trained language models\n2021-2024: Scaling laws, ChatGPT, and multimodal models like CLIP\n\nFoundation Model Evolution Timeline:\n\n2017 - Transformer: Base architecture introduced\n2018 - BERT-Base: 110M parameters\n2019 - GPT-2: 1.5B parameters\n\n2020 - GPT-3: 175B parameters\n2021 - ViT-Large: 307M parameters for vision\n2022 - PaLM: 540B parameters\n2023 - GPT-4: ~1T parameters (estimated)\n2024 - Claude-3: Multi-modal capabilities\n\n\n\n\n\nThe transformer architecture revolutionized deep learning by introducing the self-attention mechanism, which allows models to process sequences in parallel rather than sequentially. This architecture forms the backbone of both language models (GPT, BERT) and vision models (ViT, DINO).\nCore Components:\n\nMulti-Head Attention: Allows the model to attend to different representation subspaces (see Self-attention)\nFeed-Forward Networks: Point-wise processing with GELU activation (see Multilayer perceptron)\nResidual Connections: Enable training of very deep networks (see Residual neural network)\nLayer Normalization: Stabilizes training and improves convergence (see Layer normalization)\n\nKey terms (quick definitions):\n\nSelf-Attention (SA): A mechanism where each token attends to every other token (including itself) to compute a weighted combination of their representations.\nQueries (Q), Keys (K), Values (V): Learned linear projections of the input. Attention scores are computed from Q¬∑K·µÄ; those scores weight V to produce the output (see Attention (machine learning)).\nMulti-Head Attention (MHA): Runs several SA ‚Äúheads‚Äù in parallel on different subspaces, then concatenates and projects the results. This lets the model capture diverse relationships (see Self-attention).\nFeed-Forward Network (FFN/MLP): A small two-layer MLP (Multi-Layer Perceptron) applied to each token independently; expands the dimension then projects back (often with GELU activation) (see Multilayer perceptron).\nResidual Connection (Skip): Adds the input of a sub-layer to its output, helping gradients flow and preserving information (see Residual neural network).\nLayer Normalization (LayerNorm, LN): Normalizes features per token to stabilize training. The ordering with residuals defines Post-LN vs Pre-LN variants (see Layer normalization).\nGELU: Gaussian Error Linear Unit activation; smoother than ReLU and standard in transformers (see Gaussian error linear unit).\nTensor shape convention: Using batch_first=True, tensors are [batch_size, seq_len, embed_dim].\nAbbreviations used: B = batch size, S = sequence length (or number of tokens/patches), E = embedding dimension, H = number of attention heads.\n\nThe following implementation demonstrates a simplified transformer block following the original architecture:\n\nclass SimpleTransformerBlock(nn.Module):\n    \"\"\"Transformer block with Multi-Head Self-Attention (MHSA) and MLP.\n\n    Inputs/Outputs:\n      - x: [batch_size, seq_len, embed_dim]\n      - returns: same shape as input\n\n    Structure (Post-LN variant):\n      x = LN(x + MHSA(x))\n      x = LN(x + MLP(x))\n    \"\"\"\n\n    def __init__(self, embed_dim=768, num_heads=8, mlp_ratio=4):\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        # Multi-Head Self-Attention (MHSA)\n        # - batch_first=True ‚Üí tensors are [B, S, E]\n        # - Self-attention uses Q=K=V=linear(x) by default\n        self.attention = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            batch_first=True,\n        )\n\n        # Position-wise feed-forward network (applied independently to each token)\n        hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),  # expand\n            nn.GELU(),                         # nonlinearity\n            nn.Linear(hidden_dim, embed_dim),  # project back\n        )\n\n        # LayerNorms used after residual additions (Post-LN)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # x: [B, S, E]\n\n        # Multi-head self-attention\n        # attn_out: [B, S, E]; attn_weights (ignored here) would be [B, num_heads, S, S]\n        attn_out, _ = self.attention(x, x, x)  # Q=K=V=x (self-attention)\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm1(x + attn_out)\n\n        # Position-wise MLP\n        mlp_out = self.mlp(x)  # [B, S, E]\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm2(x + mlp_out)\n\n        return x  # shape preserved: [B, S, E]\n\n\n# Example usage and shape checks\ntransformer_block = SimpleTransformerBlock(embed_dim=768, num_heads=8)\nsample_input = torch.randn(2, 100, 768)  # [batch_size, seq_len, embed_dim]\noutput = transformer_block(sample_input)\n\nprint(f\"Input shape:  {sample_input.shape}\")\nprint(f\"Output shape: {output.shape}\")  # should match input\nprint(f\"Parameters:  {sum(p.numel() for p in transformer_block.parameters()):,}\")\n\nInput shape:  torch.Size([2, 100, 768])\nOutput shape: torch.Size([2, 100, 768])\nParameters:  7,087,872\n\n\nWhat to notice: - Shapes are stable through the block: [B, S, E] ‚Üí [B, S, E]. - Residuals + LayerNorm appear after each sublayer (Post-LN variant). - MLP expands to mlp_ratio √ó embed_dim then projects back."
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#evolution-from-ai-to-transformers",
    "href": "course-materials/c00a-foundation_model_architectures.html#evolution-from-ai-to-transformers",
    "title": "Foundation Model Architectures",
    "section": "Evolution from AI to Transformers",
    "text": "Evolution from AI to Transformers\nThe development of foundation models represents a convergence of several key technological breakthroughs. The journey from symbolic AI to modern foundation models reflects both advances in computational power and fundamental insights about learning representations from large-scale data.\n\nKey Historical Milestones\nThe transformer architecture emerged from decades of research in neural networks, attention mechanisms, and sequence modeling. The 2017 paper ‚ÄúAttention Is All You Need‚Äù introduced the transformer, which became the foundation for both language and vision models.\nCritical Developments:\n\n1950s-1990s: Symbolic AI dominated, with rule-based systems and early neural networks\n2012: AlexNet‚Äôs ImageNet victory sparked the deep learning revolution\n2017: The Transformer architecture introduced self-attention and parallelizable training\n2018-2020: BERT and GPT families demonstrated the power of pre-trained language models\n2021-2024: Scaling laws, ChatGPT, and multimodal models like CLIP\n\nFoundation Model Evolution Timeline:\n\n2017 - Transformer: Base architecture introduced\n2018 - BERT-Base: 110M parameters\n2019 - GPT-2: 1.5B parameters\n\n2020 - GPT-3: 175B parameters\n2021 - ViT-Large: 307M parameters for vision\n2022 - PaLM: 540B parameters\n2023 - GPT-4: ~1T parameters (estimated)\n2024 - Claude-3: Multi-modal capabilities\n\n\n\nTransformer Architecture Essentials\nThe transformer architecture revolutionized deep learning by introducing the self-attention mechanism, which allows models to process sequences in parallel rather than sequentially. This architecture forms the backbone of both language models (GPT, BERT) and vision models (ViT, DINO).\nCore Components:\n\nMulti-Head Attention: Allows the model to attend to different representation subspaces (see Self-attention)\nFeed-Forward Networks: Point-wise processing with GELU activation (see Multilayer perceptron)\nResidual Connections: Enable training of very deep networks (see Residual neural network)\nLayer Normalization: Stabilizes training and improves convergence (see Layer normalization)\n\nKey terms (quick definitions):\n\nSelf-Attention (SA): A mechanism where each token attends to every other token (including itself) to compute a weighted combination of their representations.\nQueries (Q), Keys (K), Values (V): Learned linear projections of the input. Attention scores are computed from Q¬∑K·µÄ; those scores weight V to produce the output (see Attention (machine learning)).\nMulti-Head Attention (MHA): Runs several SA ‚Äúheads‚Äù in parallel on different subspaces, then concatenates and projects the results. This lets the model capture diverse relationships (see Self-attention).\nFeed-Forward Network (FFN/MLP): A small two-layer MLP (Multi-Layer Perceptron) applied to each token independently; expands the dimension then projects back (often with GELU activation) (see Multilayer perceptron).\nResidual Connection (Skip): Adds the input of a sub-layer to its output, helping gradients flow and preserving information (see Residual neural network).\nLayer Normalization (LayerNorm, LN): Normalizes features per token to stabilize training. The ordering with residuals defines Post-LN vs Pre-LN variants (see Layer normalization).\nGELU: Gaussian Error Linear Unit activation; smoother than ReLU and standard in transformers (see Gaussian error linear unit).\nTensor shape convention: Using batch_first=True, tensors are [batch_size, seq_len, embed_dim].\nAbbreviations used: B = batch size, S = sequence length (or number of tokens/patches), E = embedding dimension, H = number of attention heads.\n\nThe following implementation demonstrates a simplified transformer block following the original architecture:\n\nclass SimpleTransformerBlock(nn.Module):\n    \"\"\"Transformer block with Multi-Head Self-Attention (MHSA) and MLP.\n\n    Inputs/Outputs:\n      - x: [batch_size, seq_len, embed_dim]\n      - returns: same shape as input\n\n    Structure (Post-LN variant):\n      x = LN(x + MHSA(x))\n      x = LN(x + MLP(x))\n    \"\"\"\n\n    def __init__(self, embed_dim=768, num_heads=8, mlp_ratio=4):\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        # Multi-Head Self-Attention (MHSA)\n        # - batch_first=True ‚Üí tensors are [B, S, E]\n        # - Self-attention uses Q=K=V=linear(x) by default\n        self.attention = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            batch_first=True,\n        )\n\n        # Position-wise feed-forward network (applied independently to each token)\n        hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),  # expand\n            nn.GELU(),                         # nonlinearity\n            nn.Linear(hidden_dim, embed_dim),  # project back\n        )\n\n        # LayerNorms used after residual additions (Post-LN)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # x: [B, S, E]\n\n        # Multi-head self-attention\n        # attn_out: [B, S, E]; attn_weights (ignored here) would be [B, num_heads, S, S]\n        attn_out, _ = self.attention(x, x, x)  # Q=K=V=x (self-attention)\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm1(x + attn_out)\n\n        # Position-wise MLP\n        mlp_out = self.mlp(x)  # [B, S, E]\n\n        # Residual connection + LayerNorm (Post-LN)\n        x = self.norm2(x + mlp_out)\n\n        return x  # shape preserved: [B, S, E]\n\n\n# Example usage and shape checks\ntransformer_block = SimpleTransformerBlock(embed_dim=768, num_heads=8)\nsample_input = torch.randn(2, 100, 768)  # [batch_size, seq_len, embed_dim]\noutput = transformer_block(sample_input)\n\nprint(f\"Input shape:  {sample_input.shape}\")\nprint(f\"Output shape: {output.shape}\")  # should match input\nprint(f\"Parameters:  {sum(p.numel() for p in transformer_block.parameters()):,}\")\n\nInput shape:  torch.Size([2, 100, 768])\nOutput shape: torch.Size([2, 100, 768])\nParameters:  7,087,872\n\n\nWhat to notice: - Shapes are stable through the block: [B, S, E] ‚Üí [B, S, E]. - Residuals + LayerNorm appear after each sublayer (Post-LN variant). - MLP expands to mlp_ratio √ó embed_dim then projects back."
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#step-development-pipeline-comparison",
    "href": "course-materials/c00a-foundation_model_architectures.html#step-development-pipeline-comparison",
    "title": "Foundation Model Architectures",
    "section": "9-Step Development Pipeline Comparison",
    "text": "9-Step Development Pipeline Comparison\nBoth LLMs and GFMs follow similar high-level development pipelines, but differ in the details because language and satellite imagery are very different kinds of data.\n\nPlain-language overview of the 9 steps\n\nData Preparation: Gather raw data and clean it up so the model can learn useful patterns.\nTokenization (turning inputs into pieces the model can handle): Decide how to chop inputs into small parts the model can process.\nArchitecture (the model blueprint): Choose how many layers, how wide/tall the model is, and how it connects information.\nPretraining Objective (what the model practices): Pick the learning task the model does before any specific application.\nTraining Loop (how learning happens): Decide optimizers, learning rate, precision, and how to stabilize training.\nEvaluation (how we check learning): Use simple tests to see if the model is improving in the right ways.\nPretrained Weights (starting point): Load existing model parameters to avoid training from scratch.\nFinetuning (adapting the model): Add a small head or nudge the model for a specific task with labeled examples.\nDeployment (using the model in practice): Serve the model efficiently and handle real-world input sizes.\n\n\n\nSide-by-side (LLMs vs GFMs)\n\n\n\n\n\n\n\n\n\nStep\nLLMs (text)\nGFMs (satellite imagery)\n\n\n\n\n1. Data Preparation\nCollect large text sets, remove duplicates and low-quality content\nCollect scenes from sensors, correct for atmosphere/sensor, remove clouds, tile into manageable chips\n\n\n2. Tokenization\nBreak text into subword tokens; build a vocabulary\nCut images into patches; turn each patch into a vector; add 2D (and time) positions\n\n\n3. Architecture\nTransformer layers over token sequences (often decoder-only for GPT, encoder-only for BERT)\nVision Transformer-style encoders over patch sequences; may include temporal attention for time series\n\n\n4. Pretraining Objective\nPredict the next/missing word to learn language patterns\nReconstruct masked image patches or learn similarities across views/time to learn visual patterns\n\n\n5. Training Loop\nAdamW, learning-rate schedule, mixed precision; long sequences can stress memory\nSimilar tools, but memory shaped by patch size, bands, and image tiling; may mask clouds or invalid pixels\n\n\n6. Evaluation\nQuick checks like ‚Äúhow surprised is the model?‚Äù (e.g., next-word loss) and small downstream tasks\nQuick checks like reconstruction quality and small downstream tasks (e.g., linear probes on land cover)\n\n\n7. Pretrained Weights\nDownload weights and matching tokenizer from model hubs\nDownload weights from hubs (e.g., Prithvi, SatMAE); ensure band order and preprocessing match\n\n\n8. Finetuning\nAdd a small head or adapters; few labeled examples can go far\nAdd a task head (classification/segmentation); often freeze encoder and train a light head on small datasets\n\n\n9. Deployment\nServe via APIs; speed up with caching of past context\nRun sliding-window/tiling over large scenes; export results as geospatial rasters/vectors\n\n\n\n\n\n\nDetails (optional depth)\n\nLLM Development Pipeline\nLanguage models like GPT and BERT have established a mature development pipeline refined through years of research. The process emphasizes text quality, vocabulary optimization, and language understanding benchmarks.\nKey References:\n\nLanguage Models are Few-Shot Learners - GPT-3 methodology\nTraining language models to follow instructions - InstructGPT\nPaLM: Scaling Language Modeling - Large-scale training\n\nLLM Development Pipeline Steps (concise):\n\nData Preparation: Text corpora, deduplication, quality filtering, mixing ratios\nTokenization: BPE, vocabulary construction, special tokens\n\nArchitecture: GPT/BERT variants, depth/width scaling, context length\nPretraining Objective: Next-token prediction, masked language modeling\nTraining Loop: Optimizers, LR schedules, mixed precision, gradient clipping\nEvaluation: Next-token loss, downstream task probing, benchmarks\nPretrained Weights: Model hubs, tokenizer alignment, loading utilities\nFinetuning: Task-specific heads, PEFT methods, instruction tuning\nDeployment: API serving, KV caching, inference optimization\n\n\n\nGFM Development Pipeline\nGeospatial foundation models adapt the transformer architecture for Earth observation data, requiring specialized handling of multi-spectral imagery, temporal sequences, and spatial relationships.\nKey References:\n\nPrithvi Foundation Model - IBM/NASA collaboration\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery\nClay Foundation Model - Open-source geospatial model\n\nGFM Development Pipeline Steps (concise):\n\nData Preparation: Multi-spectral data, georegistration, tiling, cloud masking\nTokenization: Patch-based, continuous embeddings, 2D/temporal positions\nArchitecture: ViT encoders, spatial/temporal attention, memory constraints\nPretraining Objective: Masked patch reconstruction, contrastive learning\nTraining Loop: Cloud masks, mixed precision, gradient accumulation\nEvaluation: Reconstruction metrics, linear probing, generalization\nPretrained Weights: Prithvi, SatMAE, adapter loading, band alignment\nFinetuning: Task heads, PEFT, few-shot learning for limited labels\nDeployment: Tiling inference, geospatial APIs, batch processing"
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#step-by-step-detailed-comparison",
    "href": "course-materials/c00a-foundation_model_architectures.html#step-by-step-detailed-comparison",
    "title": "Foundation Model Architectures",
    "section": "Step-by-Step Detailed Comparison",
    "text": "Step-by-Step Detailed Comparison\nThis section provides detailed comparisons of each development pipeline step, highlighting the fundamental differences between text and geospatial data processing.\n\n1. Data Preparation Differences\nData preparation represents one of the most significant differences between LLMs and GFMs. LLMs work with human-generated text that requires quality filtering and deduplication, while GFMs process sensor data that requires physical corrections and calibration.\nLLM Data Challenges:\n\nScale: Training datasets like CommonCrawl contain hundreds of terabytes\nQuality: Filtering toxic content, spam, and low-quality text\nDeduplication: Removing exact and near-duplicate documents\nLanguage Detection: Identifying and filtering by language\n\nGFM Data Challenges:\n\nSensor Calibration: Converting raw digital numbers to physical units\nAtmospheric Correction: Removing atmospheric effects from satellite imagery\nCloud Masking: Identifying and handling cloudy pixels\nGeoregistration: Aligning images to geographic coordinate systems\n\n\n# LLM text preprocessing example\nsample_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning is transforming many industries.\", \n    \"Climate change requires urgent global action.\"\n]\n\n# Basic tokenization for vocabulary construction\nvocab = set()\nfor text in sample_texts:\n    vocab.update(text.lower().replace('.', '').split())\n\nprint(\"LLM Data Processing:\")\nprint(f\"Sample vocabulary size: {len(vocab)}\")\nprint(f\"Sample tokens: {list(vocab)[:10]}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# GFM satellite data preprocessing example\nnp.random.seed(42)\npatch_size = 64\nnum_bands = 6\n\n# Simulate raw satellite patch (typical 12-bit values)\nsatellite_patch = np.random.randint(0, 4096, (num_bands, patch_size, patch_size))\n\n# Simulate cloud mask (20% cloud coverage)\ncloud_mask = np.random.random((patch_size, patch_size)) &gt; 0.8\n\n# Apply atmospheric correction (normalize to [0,1])\ncorrected_patch = satellite_patch.astype(np.float32) / 4095.0\ncorrected_patch[:, cloud_mask] = np.nan  # Mask cloudy pixels\n\nprint(\"GFM Data Processing:\")\nprint(f\"Satellite patch shape: {satellite_patch.shape} (bands, height, width)\")\nprint(f\"Cloud coverage: {cloud_mask.sum() / cloud_mask.size * 100:.1f}%\")\nprint(f\"Valid pixels per band: {(~np.isnan(corrected_patch[0])).sum():,}\")\n\nLLM Data Processing:\nSample vocabulary size: 20\nSample tokens: ['fox', 'is', 'machine', 'climate', 'industries', 'lazy', 'jumps', 'over', 'change', 'many']\n\n==================================================\n\nGFM Data Processing:\nSatellite patch shape: (6, 64, 64) (bands, height, width)\nCloud coverage: 20.3%\nValid pixels per band: 3,265\n\n\n\n\n2. Tokenization Approaches\nTokenization represents a fundamental difference between language and vision models. LLMs use discrete tokenization with learned vocabularies (like BPE), while GFMs use continuous tokenization through patch embeddings inspired by Vision Transformers.\nLLM Tokenization:\n\nByte-Pair Encoding (BPE): Learns subword units to handle out-of-vocabulary words\nVocabulary Size: Typically 30K-100K tokens balancing efficiency and coverage\nSpecial Tokens: [CLS], [SEP], [PAD], [MASK] for different tasks\n\nGFM Tokenization:\n\nPatch Embedding: Divides images into fixed-size patches (e.g., 16√ó16 pixels)\nLinear Projection: Maps high-dimensional patches to embedding space\nPositional Encoding: 2D spatial positions rather than 1D sequence positions\n\n\n# LLM discrete tokenization example\nvocab_size, embed_dim = 50000, 768\ntoken_ids = torch.tensor([1, 15, 234, 5678, 2])  # [CLS, word1, word2, word3, SEP]\n\nembedding_layer = nn.Embedding(vocab_size, embed_dim)\ntoken_embeddings = embedding_layer(token_ids)\n\nprint(\"LLM Tokenization (Discrete):\")\nprint(f\"Token IDs: {token_ids.tolist()}\")\nprint(f\"Token embeddings shape: {token_embeddings.shape}\")\nprint(f\"Vocabulary size: {vocab_size:,}\")\n\nprint(\"\\n\" + \"-\"*40 + \"\\n\")\n\n# GFM continuous patch tokenization\npatch_size = 16\nnum_bands = 6  # Multi-spectral bands\nembed_dim = 768\n\nnum_patches = 4\npatch_dim = patch_size * patch_size * num_bands\npatches = torch.randn(num_patches, patch_dim)\n\n# Linear projection for patch embedding\npatch_projection = nn.Linear(patch_dim, embed_dim)\npatch_embeddings = patch_projection(patches)\n\nprint(\"GFM Tokenization (Continuous Patches):\")\nprint(f\"Patch dimensions: {patch_size}√ó{patch_size}√ó{num_bands} = {patch_dim}\")\nprint(f\"Patch embeddings shape: {patch_embeddings.shape}\")\nprint(\"No discrete vocabulary - continuous projection\")\n\nLLM Tokenization (Discrete):\nToken IDs: [1, 15, 234, 5678, 2]\nToken embeddings shape: torch.Size([5, 768])\nVocabulary size: 50,000\n\n----------------------------------------\n\nGFM Tokenization (Continuous Patches):\nPatch dimensions: 16√ó16√ó6 = 1536\nPatch embeddings shape: torch.Size([4, 768])\nNo discrete vocabulary - continuous projection\n\n\n\n\n3. Architecture Comparison\nWhile both LLMs and GFMs use transformer architectures, they differ in input processing, positional encoding, and output heads. LLMs like GPT use causal attention for autoregressive generation, while GFMs like Prithvi use bidirectional attention for representation learning.\nKey Architectural Differences:\n\nInput Processing: 1D token sequences vs.¬†2D spatial patches\nPositional Encoding: 1D learned positions vs.¬†2D spatial coordinates\nAttention Patterns: Causal masking vs.¬†full bidirectional attention\nOutput Heads: Language modeling head vs.¬†reconstruction/classification heads\n\n\nclass LLMArchitecture(nn.Module):\n    \"\"\"Simplified LLM architecture (GPT-style)\"\"\"\n    \n    def __init__(self, vocab_size=50000, embed_dim=768, num_layers=12, num_heads=12):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.positional_encoding = nn.Embedding(2048, embed_dim)  # Max sequence length\n        \n        self.layers = nn.ModuleList([\n            SimpleTransformerBlock(embed_dim, num_heads) \n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n        self.output_head = nn.Linear(embed_dim, vocab_size)\n    \n    def forward(self, input_ids):\n        seq_len = input_ids.shape[1]\n        positions = torch.arange(seq_len, device=input_ids.device)\n        \n        # Token + positional embeddings\n        x = self.embedding(input_ids) + self.positional_encoding(positions)\n        \n        # Transformer layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.ln_final(x)\n        logits = self.output_head(x)\n        \n        return logits\n\nclass GFMArchitecture(nn.Module):\n    \"\"\"Simplified GFM architecture (ViT-style)\"\"\"\n    \n    def __init__(self, patch_size=16, num_bands=6, embed_dim=768, num_layers=12, num_heads=12):\n        super().__init__()\n        self.patch_size = patch_size\n        self.num_bands = num_bands\n        \n        # Patch embedding\n        patch_dim = patch_size * patch_size * num_bands\n        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n        \n        # 2D positional embedding\n        self.pos_embed_h = nn.Embedding(100, embed_dim // 2)  # Height positions\n        self.pos_embed_w = nn.Embedding(100, embed_dim // 2)  # Width positions\n        \n        self.layers = nn.ModuleList([\n            SimpleTransformerBlock(embed_dim, num_heads) \n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n    \n    def forward(self, patches, patch_positions):\n        batch_size, num_patches, patch_dim = patches.shape\n        \n        # Patch embeddings\n        x = self.patch_embedding(patches)\n        \n        # 2D positional embeddings\n        pos_h, pos_w = patch_positions[:, :, 0], patch_positions[:, :, 1]\n        pos_emb = torch.cat([\n            self.pos_embed_h(pos_h),\n            self.pos_embed_w(pos_w)\n        ], dim=-1)\n        \n        x = x + pos_emb\n        \n        # Transformer layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.ln_final(x)\n        return x\n\n# Compare architectures\nllm_model = LLMArchitecture(vocab_size=10000, embed_dim=384, num_layers=6, num_heads=6)\ngfm_model = GFMArchitecture(patch_size=16, num_bands=6, embed_dim=384, num_layers=6, num_heads=6)\n\nllm_params = sum(p.numel() for p in llm_model.parameters())\ngfm_params = sum(p.numel() for p in gfm_model.parameters())\n\nprint(\"Architecture Comparison:\")\nprint(f\"LLM parameters: {llm_params:,}\")\nprint(f\"GFM parameters: {gfm_params:,}\")\n\n# Test forward passes\nsample_tokens = torch.randint(0, 10000, (2, 50))  # [batch_size, seq_len]\nsample_patches = torch.randn(2, 16, 16*16*6)  # [batch_size, num_patches, patch_dim]\nsample_positions = torch.randint(0, 10, (2, 16, 2))  # [batch_size, num_patches, 2]\n\nllm_output = llm_model(sample_tokens)\ngfm_output = gfm_model(sample_patches, sample_positions)\n\nprint(f\"\\nLLM output shape: {llm_output.shape}\")\nprint(f\"GFM output shape: {gfm_output.shape}\")\n\nArchitecture Comparison:\nLLM parameters: 19,123,984\nGFM parameters: 11,276,160\n\nLLM output shape: torch.Size([2, 50, 10000])\nGFM output shape: torch.Size([2, 16, 384])\n\n\n\n\n4. Pretraining Objectives\nThe pretraining objectives differ fundamentally between text and visual domains. LLMs excel at predictive modeling (predicting the next token), while GFMs focus on reconstructive modeling (rebuilding masked image patches).\nLLM Objectives:\n\nNext-Token Prediction: GPT-style autoregressive modeling for text generation\nMasked Language Modeling: BERT-style bidirectional understanding\nInstruction Following: Learning to follow human instructions (InstructGPT)\n\nGFM Objectives:\n\nMasked Patch Reconstruction: MAE-style learning of visual representations\nContrastive Learning: Learning invariances across time and space (SimCLR, CLIP)\nMulti-task Pretraining: Combining reconstruction with auxiliary tasks\n\nKey References:\n\nMasked Autoencoders Are Scalable Vision Learners - MAE methodology\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery\n\n\n# LLM next-token prediction objective\nsequence = torch.tensor([[1, 2, 3, 4, 5]])\ntargets = torch.tensor([[2, 3, 4, 5, 6]])  # Shifted by one position\n\nvocab_size = 1000\nlogits = torch.randn(1, 5, vocab_size)  # Model predictions\n\nce_loss = nn.CrossEntropyLoss()\nnext_token_loss = ce_loss(logits.view(-1, vocab_size), targets.view(-1))\n\nprint(\"LLM Pretraining Objectives:\")\nprint(f\"Next-token prediction loss: {next_token_loss.item():.4f}\")\n\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# GFM masked patch reconstruction objective\nbatch_size, num_patches, patch_dim = 2, 64, 768\noriginal_patches = torch.randn(batch_size, num_patches, patch_dim)\n\n# Random masking (75% typical for MAE)\nmask_ratio = 0.75\nnum_masked = int(num_patches * mask_ratio)\n\nmask = torch.zeros(batch_size, num_patches, dtype=torch.bool)\nfor i in range(batch_size):\n    masked_indices = torch.randperm(num_patches)[:num_masked]\n    mask[i, masked_indices] = True\n\n# Reconstruction loss on masked patches only\nreconstructed_patches = torch.randn_like(original_patches)\nreconstruction_loss = nn.MSELoss()(\n    reconstructed_patches[mask], \n    original_patches[mask]\n)\n\nprint(\"GFM Pretraining Objectives:\")\nprint(f\"Mask ratio: {mask_ratio:.1%}\")\nprint(f\"Masked patches per sample: {num_masked}\")\nprint(f\"Reconstruction loss: {reconstruction_loss.item():.4f}\")\n\nLLM Pretraining Objectives:\nNext-token prediction loss: 6.7945\n\n--------------------------------------------------\n\nGFM Pretraining Objectives:\nMask ratio: 75.0%\nMasked patches per sample: 48\nReconstruction loss: 2.0088"
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#scaling-and-evolution",
    "href": "course-materials/c00a-foundation_model_architectures.html#scaling-and-evolution",
    "title": "Foundation Model Architectures",
    "section": "Scaling and Evolution",
    "text": "Scaling and Evolution\nThe scaling trends between LLMs and GFMs reveal different optimization strategies. LLMs focus on parameter scaling (billions of parameters) while GFMs emphasize data modality scaling (spectral, spatial, and temporal dimensions).\n\nParameter Scaling Comparison\nLLM Scaling Milestones:\n\nGPT-1 (2018): 117M parameters - Demonstrated unsupervised pretraining potential\nBERT-Base (2018): 110M parameters - Bidirectional language understanding\nGPT-2 (2019): 1.5B parameters - First signs of emergent capabilities\nGPT-3 (2020): 175B parameters - Few-shot learning breakthrough\nPaLM (2022): 540B parameters - Advanced reasoning capabilities\nGPT-4 (2023): ~1T parameters - Multimodal understanding\n\nGFM Scaling Examples:\n\nSatMAE-Base: 86M parameters - Satellite imagery foundation\nPrithvi-100M: 100M parameters - IBM/NASA Earth observation model\nClay-v0.1: 139M parameters - Open-source geospatial foundation model\nScale-MAE: 600M parameters - Largest published geospatial transformer\n\nContext/Input Scaling Differences:\nLLMs:\n\nContext length: 512 ‚Üí 2K ‚Üí 8K ‚Üí 128K+ tokens\nTraining data: Web text, books, code (curated datasets)\nFocus: Language understanding and generation\n\nGFMs:\n\nInput bands: 3 (RGB) ‚Üí 6+ (multispectral) ‚Üí hyperspectral\nSpatial resolution: Various (10m to 0.3m pixel sizes)\nTemporal dimension: Single ‚Üí time series ‚Üí multi-temporal\nFocus: Earth observation and environmental monitoring\n\n\n# Visualize parameter scaling comparison\nllm_milestones = {\n    'GPT-1': 117e6,\n    'BERT-Base': 110e6,\n    'GPT-2': 1.5e9,\n    'GPT-3': 175e9,\n    'PaLM': 540e9,\n    'GPT-4': 1000e9  # Estimated\n}\n\ngfm_milestones = {\n    'SatMAE-Base': 86e6,\n    'Prithvi-100M': 100e6,\n    'Clay-v0.1': 139e6,\n    'SatLas-Base': 300e6,\n    'Scale-MAE': 600e6\n}\n\n# Create visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# LLM scaling\nmodels = list(llm_milestones.keys())\nparams = [llm_milestones[m]/1e9 for m in models]\n\nax1.bar(models, params, color='skyblue', alpha=0.7)\nax1.set_yscale('log')\nax1.set_ylabel('Parameters (Billions)')\nax1.set_title('LLM Parameter Scaling')\nax1.tick_params(axis='x', rotation=45)\n\n# GFM scaling\nmodels = list(gfm_milestones.keys())\nparams = [gfm_milestones[m]/1e6 for m in models]\n\nax2.bar(models, params, color='lightcoral', alpha=0.7)\nax2.set_ylabel('Parameters (Millions)')\nax2.set_title('GFM Parameter Scaling')\nax2.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nData Requirements and Constraints\nThe data infrastructure requirements for LLMs and GFMs differ dramatically due to the nature of text versus imagery data. Understanding these constraints is crucial for development planning.\n\n\n\nAspect\nLLMs\nGFMs\n\n\n\n\nData Volume\nTerabytes of text data (web crawls, books, code repositories)\nPetabytes of satellite imagery (constrained by storage/IO bandwidth)\n\n\nData Quality Challenges\nDeduplication algorithms, toxicity filtering, language detection\nCloud masking, atmospheric correction, sensor calibration\n\n\nPreprocessing Requirements\nTokenization, sequence packing, attention mask generation\nPatch extraction, normalization, spatial/temporal alignment\n\n\nStorage Format Optimization\nCompressed text files, pre-tokenized sequences\nCloud-optimized formats (COG, Zarr), tiled storage\n\n\nAccess Pattern Differences\nSequential text processing, random document sampling\nSpatial/temporal queries, patch-based sampling, geographic tiling\n\n\n\n\n\nImplementation Examples\n\nEmbedding Creation\nEmbeddings are the foundation of both LLMs and GFMs, but they differ in how raw inputs are converted to dense vector representations. This section demonstrates practical implementation patterns for both domains.\n\n# Text embedding creation (LLM approach)\ntext = \"The forest shows signs of deforestation.\"\ntokens = text.lower().replace('.', '').split()\n\n# Create simple vocabulary mapping\nvocab = {word: i for i, word in enumerate(set(tokens))}\nvocab['&lt;PAD&gt;'] = len(vocab)\nvocab['&lt;UNK&gt;'] = len(vocab)\n\n# Convert text to token IDs\ntoken_ids = [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\ntoken_tensor = torch.tensor(token_ids).unsqueeze(0)\n\n# Embedding layer lookup\nembed_layer = nn.Embedding(len(vocab), 256)\ntext_embeddings = embed_layer(token_tensor)\n\nprint(\"Text Embeddings (LLM):\")\nprint(f\"Original text: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Token IDs: {token_ids}\")\nprint(f\"Embeddings shape: {text_embeddings.shape}\")\n\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# Patch embedding creation (GFM approach)\npatch_size = 16\nnum_bands = 6\n\n# Create sample multi-spectral satellite patch\nsatellite_patch = torch.randn(1, num_bands, patch_size, patch_size)\n\n# Flatten patch for linear projection\npatch_flat = satellite_patch.view(1, num_bands * patch_size * patch_size)\n\n# Linear projection to embedding space\npatch_projection = nn.Linear(num_bands * patch_size * patch_size, 256)\npatch_embeddings = patch_projection(patch_flat)\n\nprint(\"Patch Embeddings (GFM):\")\nprint(f\"Original patch shape: {satellite_patch.shape}\")\nprint(f\"Flattened patch shape: {patch_flat.shape}\")\nprint(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n\nText Embeddings (LLM):\nOriginal text: The forest shows signs of deforestation.\nTokens: ['the', 'forest', 'shows', 'signs', 'of', 'deforestation']\nToken IDs: [0, 3, 1, 2, 5, 4]\nEmbeddings shape: torch.Size([1, 6, 256])\n\n--------------------------------------------------\n\nPatch Embeddings (GFM):\nOriginal patch shape: torch.Size([1, 6, 16, 16])\nFlattened patch shape: torch.Size([1, 1536])\nPatch embeddings shape: torch.Size([1, 256])\n\n\n\n\nPositional Encoding Comparison\nPositional encodings enable transformers to understand sequence order (LLMs) or spatial relationships (GFMs). The fundamental difference lies in 1D sequential positions versus 2D spatial coordinates.\nKey Differences:\n\nLLM: 1D sinusoidal encoding for sequence positions\nGFM: 2D spatial encoding combining height/width coordinates\nLearned vs Fixed: Both approaches can use learned or fixed encodings\n\n\n# 1D positional encoding for language models\ndef sinusoidal_positional_encoding(seq_len, embed_dim):\n    \"\"\"Create sinusoidal positional encodings for sequence data\"\"\"\n    pe = torch.zeros(seq_len, embed_dim)\n    position = torch.arange(0, seq_len).unsqueeze(1).float()\n    \n    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * \n                       -(np.log(10000.0) / embed_dim))\n    \n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    return pe\n\n# 2D positional encoding for geospatial models  \ndef create_2d_positional_encoding(height, width, embed_dim):\n    \"\"\"Create 2D positional encodings for spatial data\"\"\"\n    pe = torch.zeros(height, width, embed_dim)\n    \n    # Create position grids\n    y_pos = torch.arange(height).unsqueeze(1).repeat(1, width).float()\n    x_pos = torch.arange(width).unsqueeze(0).repeat(height, 1).float()\n    \n    # Encode Y positions in first half of embedding\n    for i in range(embed_dim // 2):\n        if i % 2 == 0:\n            pe[:, :, i] = torch.sin(y_pos / (10000 ** (i / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(y_pos / (10000 ** (i / embed_dim)))\n    \n    # Encode X positions in second half of embedding\n    for i in range(embed_dim // 2, embed_dim):\n        j = i - embed_dim // 2\n        if j % 2 == 0:\n            pe[:, :, i] = torch.sin(x_pos / (10000 ** (j / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(x_pos / (10000 ** (j / embed_dim)))\n    \n    return pe\n\n# Generate and visualize both encodings\nseq_len, embed_dim = 100, 256\npos_encoding_1d = sinusoidal_positional_encoding(seq_len, embed_dim)\n\nheight, width = 8, 8\npos_encoding_2d = create_2d_positional_encoding(height, width, embed_dim)\n\nprint(f\"1D positional encoding shape: {pos_encoding_1d.shape}\")\nprint(f\"2D positional encoding shape: {pos_encoding_2d.shape}\")\n\n# Visualize both encodings\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(pos_encoding_1d[:50, :50].T, cmap='RdBu', aspect='auto')\nplt.title('1D Positional Encoding (LLM)')\nplt.xlabel('Sequence Position')\nplt.ylabel('Embedding Dimension')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\npos_2d_viz = pos_encoding_2d[:, :, :64].mean(dim=-1)\nplt.imshow(pos_2d_viz, cmap='RdBu', aspect='equal')\nplt.title('2D Positional Encoding (GFM)')\nplt.xlabel('Width')\nplt.ylabel('Height')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n1D positional encoding shape: torch.Size([100, 256])\n2D positional encoding shape: torch.Size([8, 8, 256])"
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#implementation-examples",
    "href": "course-materials/c00a-foundation_model_architectures.html#implementation-examples",
    "title": "Foundation Model Architectures",
    "section": "Implementation Examples",
    "text": "Implementation Examples\n\nEmbedding Creation\nEmbeddings are the foundation of both LLMs and GFMs, but they differ in how raw inputs are converted to dense vector representations. This section demonstrates practical implementation patterns for both domains.\n\n# Text embedding creation (LLM approach)\ntext = \"The forest shows signs of deforestation.\"\ntokens = text.lower().replace('.', '').split()\n\n# Create simple vocabulary mapping\nvocab = {word: i for i, word in enumerate(set(tokens))}\nvocab['&lt;PAD&gt;'] = len(vocab)\nvocab['&lt;UNK&gt;'] = len(vocab)\n\n# Convert text to token IDs\ntoken_ids = [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\ntoken_tensor = torch.tensor(token_ids).unsqueeze(0)\n\n# Embedding layer lookup\nembed_layer = nn.Embedding(len(vocab), 256)\ntext_embeddings = embed_layer(token_tensor)\n\nprint(\"Text Embeddings (LLM):\")\nprint(f\"Original text: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Token IDs: {token_ids}\")\nprint(f\"Embeddings shape: {text_embeddings.shape}\")\n\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# Patch embedding creation (GFM approach)\npatch_size = 16\nnum_bands = 6\n\n# Create sample multi-spectral satellite patch\nsatellite_patch = torch.randn(1, num_bands, patch_size, patch_size)\n\n# Flatten patch for linear projection\npatch_flat = satellite_patch.view(1, num_bands * patch_size * patch_size)\n\n# Linear projection to embedding space\npatch_projection = nn.Linear(num_bands * patch_size * patch_size, 256)\npatch_embeddings = patch_projection(patch_flat)\n\nprint(\"Patch Embeddings (GFM):\")\nprint(f\"Original patch shape: {satellite_patch.shape}\")\nprint(f\"Flattened patch shape: {patch_flat.shape}\")\nprint(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n\nText Embeddings (LLM):\nOriginal text: The forest shows signs of deforestation.\nTokens: ['the', 'forest', 'shows', 'signs', 'of', 'deforestation']\nToken IDs: [0, 3, 1, 2, 5, 4]\nEmbeddings shape: torch.Size([1, 6, 256])\n\n--------------------------------------------------\n\nPatch Embeddings (GFM):\nOriginal patch shape: torch.Size([1, 6, 16, 16])\nFlattened patch shape: torch.Size([1, 1536])\nPatch embeddings shape: torch.Size([1, 256])\n\n\n\n\nPositional Encoding Comparison\nPositional encodings enable transformers to understand sequence order (LLMs) or spatial relationships (GFMs). The fundamental difference lies in 1D sequential positions versus 2D spatial coordinates.\nKey Differences:\n\nLLM: 1D sinusoidal encoding for sequence positions\nGFM: 2D spatial encoding combining height/width coordinates\nLearned vs Fixed: Both approaches can use learned or fixed encodings\n\n\n# 1D positional encoding for language models\ndef sinusoidal_positional_encoding(seq_len, embed_dim):\n    \"\"\"Create sinusoidal positional encodings for sequence data\"\"\"\n    pe = torch.zeros(seq_len, embed_dim)\n    position = torch.arange(0, seq_len).unsqueeze(1).float()\n    \n    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * \n                       -(np.log(10000.0) / embed_dim))\n    \n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    return pe\n\n# 2D positional encoding for geospatial models  \ndef create_2d_positional_encoding(height, width, embed_dim):\n    \"\"\"Create 2D positional encodings for spatial data\"\"\"\n    pe = torch.zeros(height, width, embed_dim)\n    \n    # Create position grids\n    y_pos = torch.arange(height).unsqueeze(1).repeat(1, width).float()\n    x_pos = torch.arange(width).unsqueeze(0).repeat(height, 1).float()\n    \n    # Encode Y positions in first half of embedding\n    for i in range(embed_dim // 2):\n        if i % 2 == 0:\n            pe[:, :, i] = torch.sin(y_pos / (10000 ** (i / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(y_pos / (10000 ** (i / embed_dim)))\n    \n    # Encode X positions in second half of embedding\n    for i in range(embed_dim // 2, embed_dim):\n        j = i - embed_dim // 2\n        if j % 2 == 0:\n            pe[:, :, i] = torch.sin(x_pos / (10000 ** (j / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(x_pos / (10000 ** (j / embed_dim)))\n    \n    return pe\n\n# Generate and visualize both encodings\nseq_len, embed_dim = 100, 256\npos_encoding_1d = sinusoidal_positional_encoding(seq_len, embed_dim)\n\nheight, width = 8, 8\npos_encoding_2d = create_2d_positional_encoding(height, width, embed_dim)\n\nprint(f\"1D positional encoding shape: {pos_encoding_1d.shape}\")\nprint(f\"2D positional encoding shape: {pos_encoding_2d.shape}\")\n\n# Visualize both encodings\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(pos_encoding_1d[:50, :50].T, cmap='RdBu', aspect='auto')\nplt.title('1D Positional Encoding (LLM)')\nplt.xlabel('Sequence Position')\nplt.ylabel('Embedding Dimension')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\npos_2d_viz = pos_encoding_2d[:, :, :64].mean(dim=-1)\nplt.imshow(pos_2d_viz, cmap='RdBu', aspect='equal')\nplt.title('2D Positional Encoding (GFM)')\nplt.xlabel('Width')\nplt.ylabel('Height')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n1D positional encoding shape: torch.Size([100, 256])\n2D positional encoding shape: torch.Size([8, 8, 256])"
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#course-mapping-and-applications",
    "href": "course-materials/c00a-foundation_model_architectures.html#course-mapping-and-applications",
    "title": "Foundation Model Architectures",
    "section": "Course Mapping and Applications",
    "text": "Course Mapping and Applications\nThis cheatsheet directly supports the GEOG 288KC curriculum by providing comparative understanding between established LLM techniques and emerging GFM approaches.\n\nWeekly Course Structure\nWeeks 1-3: Foundation Building - Focus: Data ‚Üí Attention ‚Üí Architecture - LLM Topics: Text preprocessing, tokenization, transformer blocks - GFM Topics: Satellite data, patch embedding, spatial attention\nWeeks 4-7: Model Development\n- Focus: Pretraining ‚Üí Training ‚Üí Evaluation ‚Üí Integration - LLM Topics: Language modeling, training loops, perplexity evaluation - GFM Topics: Masked reconstruction, cloud handling, linear probing\nWeeks 8-10: Deployment & Applications - Focus: Finetuning ‚Üí Deployment ‚Üí Synthesis - LLM Topics: Instruction tuning, PEFT methods, API deployment - GFM Topics: Task-specific heads, few-shot learning, geospatial inference\n\n\nKey Architectural Differences Summary\nData Nature:\n\nLLMs: Discrete text tokens with semantic consistency\nGFMs: Continuous pixel values requiring contextual interpretation\n\nTokenization Approach:\n\nLLMs: Vocabulary-based discrete mapping (BPE, WordPiece)\nGFMs: Patch-based continuous projection (linear embedding)\n\nPositional Information:\n\nLLMs: 1D sequence positions for temporal understanding\nGFMs: 2D spatial + temporal positions for spatiotemporal context\n\nTraining Objectives:\n\nLLMs: Next token prediction or masked language modeling\nGFMs: Masked patch reconstruction or contrastive learning\n\nEvaluation Metrics:\n\nLLMs: Perplexity, BLEU, downstream language task performance\nGFMs: Reconstruction quality, linear probing, spatial generalization\n\nDeployment Patterns:\n\nLLMs: Text generation with streaming and KV caching\nGFMs: Spatial inference with geographic tiling and batch processing"
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#further-reading-and-references",
    "href": "course-materials/c00a-foundation_model_architectures.html#further-reading-and-references",
    "title": "Foundation Model Architectures",
    "section": "Further Reading and References",
    "text": "Further Reading and References\n\nEssential Foundation Papers\nCore Transformer Architecture:\n\nAttention Is All You Need - Vaswani et al., 2017\nBERT: Pre-training of Deep Bidirectional Transformers - Devlin et al., 2018\nLanguage Models are Few-Shot Learners - Brown et al., 2020 (GPT-3)\n\nVision Transformers:\n\nAn Image is Worth 16x16 Words - Dosovitskiy et al., 2020 (ViT)\nMasked Autoencoders Are Scalable Vision Learners - He et al., 2021\nScaling Vision Transformers - Zhai et al., 2021\n\nGeospatial Foundation Models:\n\nPrithvi Foundation Model - IBM/NASA collaboration, 2023\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery - Cong et al., 2022\nClay Foundation Model - Made with Clay, 2024"
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#summary",
    "href": "course-materials/c00a-foundation_model_architectures.html#summary",
    "title": "Foundation Model Architectures",
    "section": "Summary",
    "text": "Summary\nKey concepts for foundation model architectures:\n\nHistorical Evolution: From symbolic AI to transformer-based foundation models\nArchitecture Comparison: LLMs use discrete tokenization, GFMs use continuous patch embeddings\nDevelopment Pipeline: 9-step process with domain-specific adaptations\nScaling Trends: LLMs scale in parameters/context, GFMs scale in spectral/spatial/temporal dimensions\nTraining Objectives: Next-token prediction vs.¬†masked patch reconstruction\nDeployment Considerations: Text streaming vs.¬†spatial tiling and batch inference\nCourse Integration: Weekly progression from data processing to deployment"
  },
  {
    "objectID": "course-materials/c00b-introduction-to-deeplearning-architecture.html",
    "href": "course-materials/c00b-introduction-to-deeplearning-architecture.html",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on defining our course goals, the architecture of the class and the code we will be working with, and getting everyone set up with a consistent computational environment.\n\n\n\nComplete local/server environment setup\nSet up development environment (Python, PyTorch, Earth Engine access)\nSubmit project application describing experience and research interests"
  },
  {
    "objectID": "course-materials/c00b-introduction-to-deeplearning-architecture.html#week-0-overview",
    "href": "course-materials/c00b-introduction-to-deeplearning-architecture.html#week-0-overview",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on defining our course goals, the architecture of the class and the code we will be working with, and getting everyone set up with a consistent computational environment.\n\n\n\nComplete local/server environment setup\nSet up development environment (Python, PyTorch, Earth Engine access)\nSubmit project application describing experience and research interests"
  },
  {
    "objectID": "course-materials/c00b-introduction-to-deeplearning-architecture.html#basics-of-foundation-models-and-llmgfm-comparisons",
    "href": "course-materials/c00b-introduction-to-deeplearning-architecture.html#basics-of-foundation-models-and-llmgfm-comparisons",
    "title": "Week 0: Getting Started",
    "section": "Basics of Foundation Models and LLM/GFM Comparisons",
    "text": "Basics of Foundation Models and LLM/GFM Comparisons"
  },
  {
    "objectID": "course-materials/c00b-introduction-to-deeplearning-architecture.html#gfm-architecture-cheatsheet",
    "href": "course-materials/c00b-introduction-to-deeplearning-architecture.html#gfm-architecture-cheatsheet",
    "title": "Week 0: Getting Started",
    "section": "GFM Architecture Cheatsheet",
    "text": "GFM Architecture Cheatsheet\nThis is a quick-reference for how our Geospatial Foundation Model (GFM) is organized, what each part does, and where to look during the course. We implement simple, readable components first, then show how to swap in optimized library counterparts as needed.\n\nRoadmap at a glance\n\n\n\n\n\n\ngraph TD\n  A[Week 1: Data Foundations] --&gt; B[Week 2: Attention & Blocks]\n  B --&gt; C[Week 3: Full Encoder]\n  C --&gt; D[Week 4: MAE Pretraining]\n  D --&gt; E[Week 5: Training Loop]\n  E --&gt; F[Week 6: Eval & Viz]\n  F --&gt; G[Week 7+: Interop & Scale]\n\n\n\n\n\n\n\n\nMinimal structure you‚Äôll use\ngeogfm/\n  core/\n    config.py                # Minimal typed configs for model, data, training\n  data/\n    loaders.py               # build_dataloader(...)\n    datasets/\n      stac_dataset.py        # Simple STAC-backed dataset\n    transforms/\n      normalization.py       # Per-channel normalization\n      patchify.py            # Extract fixed-size patches\n  modules/\n    attention/\n      multihead_attention.py     # Standard MHA (from scratch)\n    embeddings/\n      patch_embedding.py         # Conv patch embedding\n      positional_encoding.py     # Simple positional encoding\n    blocks/\n      transformer_block.py       # PreNorm block (MHA + MLP)\n    heads/\n      reconstruction_head.py     # Lightweight decoder/readout\n    losses/\n      mae_loss.py                # Masked reconstruction loss\n  models/\n    gfm_vit.py                   # GeoViT-style encoder\n    gfm_mae.py                   # MAE wrapper (masking + encoder + head)\n  training/\n    optimizer.py                 # AdamW builder\n    loop.py                      # fit/train_step/eval_step with basic checkpointing\n  evaluation/\n    visualization.py             # Visualize inputs vs reconstructions\n\n# Outside the package (repo root)\nconfigs/   # Small YAML/JSON run configs\ntests/     # Unit tests\ndata/      # Datasets, splits, stats, build scripts\n\n\nWhat each part does (one-liners)\n\ncore/config.py: Typed configs for model/data/training; keeps parameters organized.\ndata/datasets/stac_dataset.py: Reads imagery + metadata (e.g., STAC), returns tensors.\ndata/transforms/normalization.py: Normalizes channels using precomputed stats.\ndata/transforms/patchify.py: Turns large images into uniform patches for ViT.\ndata/loaders.py: Builds PyTorch DataLoaders for train/val.\nmodules/embeddings/patch_embedding.py: Projects image patches into token vectors.\nmodules/embeddings/positional_encoding.py: Adds position info to tokens.\nmodules/attention/multihead_attention.py: Lets tokens attend to each other.\nmodules/blocks/transformer_block.py: Core transformer layer (attention + MLP).\nmodules/heads/reconstruction_head.py: Reconstructs pixels from encoded tokens.\nmodules/losses/mae_loss.py: Computes masked reconstruction loss for MAE.\nmodels/gfm_vit.py: Assembles the encoder backbone from blocks.\nmodels/gfm_mae.py: Wraps encoder with masking + reconstruction for pretraining.\ntraining/optimizer.py: Creates AdamW with common defaults.\ntraining/loop.py: Runs epochs, backprop, validation, and simple checkpoints.\nevaluation/visualization.py: Plots sample inputs and reconstructions.\n\n\n\nFrom-scratch vs library-backed\n\nUse PyTorch for Dataset/DataLoader, AdamW, schedulers, AMP, and checkpointing.\nBuild core blocks from scratch first: PatchEmbedding, MHA, TransformerBlock, MAE loss/head.\nLater, swap in optimized options: torch.nn.MultiheadAttention, timm ViT blocks, FlashAttention, TorchGeo datasets, torchmetrics.\n\n\n\nQuick start (conceptual)\nfrom geogfm.core.config import ModelConfig, DataConfig, TrainConfig\nfrom geogfm.models.gfm_vit import GeoViTBackbone\nfrom geogfm.models.gfm_mae import MaskedAutoencoder\nfrom geogfm.data.loaders import build_dataloader\nfrom geogfm.training.loop import fit\n\nmodel_cfg = ModelConfig(architecture=\"gfm_vit\", embed_dim=768, depth=12, image_size=224)\ndata_cfg = DataConfig(dataset=\"stac\", patch_size=16, num_workers=8)\ntrain_cfg = TrainConfig(epochs=1, batch_size=8, optimizer={\"name\": \"adamw\", \"lr\": 2e-4})\n\nencoder = GeoViTBackbone(model_cfg)\nmodel = MaskedAutoencoder(model_cfg, encoder)\ntrain_dl, val_dl = build_dataloader(data_cfg)\nfit(model, (train_dl, val_dl), train_cfg)\nWhat to notice: - The encoder and MAE wrapper are separate, so we can reuse the encoder for other tasks later. - Data transforms (normalize/patchify) are decoupled from the model and driven by config.\n\n\nMVP vs later phases\n\nMVP (Weeks 1‚Äì6): files shown above; single-node training; basic logging and visualization.\nLater (Weeks 7‚Äì10): interop with existing models (e.g., Prithvi), task heads (classification/segmentation), inference tiling, Hub integration.\n\n\n\nComprehensive course roadmap (stages, files, libraries)\nThis table maps each week to the broader stages (see the course index), the key geogfm files you‚Äôll touch, and the primary deep learning tools you‚Äôll rely on.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n1\nStage 1: Build GFM Architecture\nData Foundations\ncore/config.py; data/datasets/stac_dataset.py; data/transforms/{normalization.py, patchify.py}; data/loaders.py\ntorch.utils.data.Dataset/DataLoader, rasterio, numpy\nConfig-driven dataloaders that yield normalized patches\n\n\n2\nStage 1\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n3\nStage 1\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking utilities, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n5\nStage 2\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n6\nStage 2\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n7\nStage 2\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n9\nStage 3\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n10\nStage 3\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan\n\n\n\n\n\n\nNext Week Preview\nWeek 1 will start building our model, beginning with fundamental data loaders and transformers needed to use geospatial data in deep learning."
  },
  {
    "objectID": "course-materials/c00a-foundation_model_architectures.html#llms-vs-gfms",
    "href": "course-materials/c00a-foundation_model_architectures.html#llms-vs-gfms",
    "title": "Foundation Model Architectures",
    "section": "LLMs vs GFMs",
    "text": "LLMs vs GFMs\nBoth LLMs and GFMs follow similar high-level development pipelines, but differ in the details because language and satellite imagery are very different kinds of data.\n\n9-Step Development Pipeline Comparison\n\nData Preparation: Gather raw data and clean it up so the model can learn useful patterns.\nTokenization (turning inputs into pieces the model can handle): Decide how to chop inputs into small parts the model can process.\nArchitecture (the model blueprint): Choose how many layers, how wide/tall the model is, and how it connects information.\nPretraining Objective (what the model practices): Pick the learning task the model does before any specific application.\nTraining Loop (how learning happens): Decide optimizers, learning rate, precision, and how to stabilize training.\nEvaluation (how we check learning): Use simple tests to see if the model is improving in the right ways.\nPretrained Weights (starting point): Load existing model parameters to avoid training from scratch.\nFinetuning (adapting the model): Add a small head or nudge the model for a specific task with labeled examples.\nDeployment (using the model in practice): Serve the model efficiently and handle real-world input sizes.\n\n\nLLM Development Pipeline\nLanguage models like GPT and BERT have established a mature development pipeline refined through years of research. The process emphasizes text quality, vocabulary optimization, and language understanding benchmarks.\nKey References:\n\nLanguage Models are Few-Shot Learners - GPT-3 methodology\nTraining language models to follow instructions - InstructGPT\nPaLM: Scaling Language Modeling - Large-scale training\n\n\n\nGFM Development Pipeline\nGeospatial foundation models adapt the transformer architecture for Earth observation data, requiring specialized handling of multi-spectral imagery, temporal sequences, and spatial relationships.\nKey References:\n\nPrithvi Foundation Model - IBM/NASA collaboration\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery\nClay Foundation Model - Open-source geospatial model\n\nSide-by-side (LLMs vs GFMs)\n\n\n\n\n\n\n\n\n\nStep\nLLMs (text)\nGFMs (satellite imagery)\n\n\n\n\n1. Data Preparation\nCollect large text sets, remove duplicates and low-quality content\nCollect scenes from sensors, correct for atmosphere/sensor, remove clouds, tile into manageable chips\n\n\n2. Tokenization\nBreak text into subword tokens; build a vocabulary\nCut images into patches; turn each patch into a vector; add 2D (and time) positions\n\n\n3. Architecture\nTransformer layers over token sequences (often decoder-only for GPT, encoder-only for BERT)\nVision Transformer-style encoders over patch sequences; may include temporal attention for time series\n\n\n4. Pretraining Objective\nPredict the next/missing word to learn language patterns\nReconstruct masked image patches or learn similarities across views/time to learn visual patterns\n\n\n5. Training Loop\nAdamW, learning-rate schedule, mixed precision; long sequences can stress memory\nSimilar tools, but memory shaped by patch size, bands, and image tiling; may mask clouds or invalid pixels\n\n\n6. Evaluation\nQuick checks like ‚Äúhow surprised is the model?‚Äù (e.g., next-word loss) and small downstream tasks\nQuick checks like reconstruction quality and small downstream tasks (e.g., linear probes on land cover)\n\n\n7. Pretrained Weights\nDownload weights and matching tokenizer from model hubs\nDownload weights from hubs (e.g., Prithvi, SatMAE); ensure band order and preprocessing match\n\n\n8. Finetuning\nAdd a small head or adapters; few labeled examples can go far\nAdd a task head (classification/segmentation); often freeze encoder and train a light head on small datasets\n\n\n9. Deployment\nServe via APIs; speed up with caching of past context\nRun sliding-window/tiling over large scenes; export results as geospatial rasters/vectors\n\n\n\n\n\n\n\nStep-by-Step Detailed Comparison\nLet‚Äôs look at more detailed comparisons beetween each development pipeline step, highlighting the fundamental differences between text and geospatial data processing.\n\nData Preparation Differences\nData preparation represents one of the most significant differences between LLMs and GFMs. LLMs work with human-generated text that requires quality filtering and deduplication, while GFMs process sensor data that requires physical corrections and calibration.\nLLM Data Challenges:\n\nScale: Training datasets like CommonCrawl contain hundreds of terabytes\nQuality: Filtering toxic content, spam, and low-quality text\nDeduplication: Removing exact and near-duplicate documents\nLanguage Detection: Identifying and filtering by language\n\nGFM Data Challenges:\n\nSensor Calibration: Converting raw digital numbers to physical units\nAtmospheric Correction: Removing atmospheric effects from satellite imagery\nCloud Masking: Identifying and handling cloudy pixels\nGeoregistration: Aligning images to geographic coordinate systems\n\n\n# LLM text preprocessing example\nsample_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning is transforming many industries.\", \n    \"Climate change requires urgent global action.\"\n]\n\n# Basic tokenization for vocabulary construction\nvocab = set()\nfor text in sample_texts:\n    vocab.update(text.lower().replace('.', '').split())\n\nprint(\"LLM Data Processing:\")\nprint(f\"Sample vocabulary size: {len(vocab)}\")\nprint(f\"Sample tokens: {list(vocab)[:10]}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# GFM satellite data preprocessing example\nnp.random.seed(42)\npatch_size = 64\nnum_bands = 6\n\n# Simulate raw satellite patch (typical 12-bit values)\nsatellite_patch = np.random.randint(0, 4096, (num_bands, patch_size, patch_size))\n\n# Simulate cloud mask (20% cloud coverage)\ncloud_mask = np.random.random((patch_size, patch_size)) &gt; 0.8\n\n# Apply atmospheric correction (normalize to [0,1])\ncorrected_patch = satellite_patch.astype(np.float32) / 4095.0\ncorrected_patch[:, cloud_mask] = np.nan  # Mask cloudy pixels\n\nprint(\"GFM Data Processing:\")\nprint(f\"Satellite patch shape: {satellite_patch.shape} (bands, height, width)\")\nprint(f\"Cloud coverage: {cloud_mask.sum() / cloud_mask.size * 100:.1f}%\")\nprint(f\"Valid pixels per band: {(~np.isnan(corrected_patch[0])).sum():,}\")\n\nLLM Data Processing:\nSample vocabulary size: 20\nSample tokens: ['fox', 'is', 'machine', 'climate', 'industries', 'lazy', 'jumps', 'over', 'change', 'many']\n\n==================================================\n\nGFM Data Processing:\nSatellite patch shape: (6, 64, 64) (bands, height, width)\nCloud coverage: 20.3%\nValid pixels per band: 3,265\n\n\n\n\nTokenization Approaches\nTokenization represents a fundamental difference between language and vision models. LLMs use discrete tokenization with learned vocabularies (like BPE), while GFMs use continuous tokenization through patch embeddings inspired by Vision Transformers.\nLLM Tokenization:\n\nByte-Pair Encoding (BPE): Learns subword units to handle out-of-vocabulary words\nVocabulary Size: Typically 30K-100K tokens balancing efficiency and coverage\nSpecial Tokens: [CLS], [SEP], [PAD], [MASK] for different tasks\n\nGFM Tokenization:\n\nPatch Embedding: Divides images into fixed-size patches (e.g., 16√ó16 pixels)\nLinear Projection: Maps high-dimensional patches to embedding space\nPositional Encoding: 2D spatial positions rather than 1D sequence positions\n\n\n# LLM discrete tokenization example\nvocab_size, embed_dim = 50000, 768\ntoken_ids = torch.tensor([1, 15, 234, 5678, 2])  # [CLS, word1, word2, word3, SEP]\n\nembedding_layer = nn.Embedding(vocab_size, embed_dim)\ntoken_embeddings = embedding_layer(token_ids)\n\nprint(\"LLM Tokenization (Discrete):\")\nprint(f\"Token IDs: {token_ids.tolist()}\")\nprint(f\"Token embeddings shape: {token_embeddings.shape}\")\nprint(f\"Vocabulary size: {vocab_size:,}\")\n\nprint(\"\\n\" + \"-\"*40 + \"\\n\")\n\n# GFM continuous patch tokenization\npatch_size = 16\nnum_bands = 6  # Multi-spectral bands\nembed_dim = 768\n\nnum_patches = 4\npatch_dim = patch_size * patch_size * num_bands\npatches = torch.randn(num_patches, patch_dim)\n\n# Linear projection for patch embedding\npatch_projection = nn.Linear(patch_dim, embed_dim)\npatch_embeddings = patch_projection(patches)\n\nprint(\"GFM Tokenization (Continuous Patches):\")\nprint(f\"Patch dimensions: {patch_size}√ó{patch_size}√ó{num_bands} = {patch_dim}\")\nprint(f\"Patch embeddings shape: {patch_embeddings.shape}\")\nprint(\"No discrete vocabulary - continuous projection\")\n\nLLM Tokenization (Discrete):\nToken IDs: [1, 15, 234, 5678, 2]\nToken embeddings shape: torch.Size([5, 768])\nVocabulary size: 50,000\n\n----------------------------------------\n\nGFM Tokenization (Continuous Patches):\nPatch dimensions: 16√ó16√ó6 = 1536\nPatch embeddings shape: torch.Size([4, 768])\nNo discrete vocabulary - continuous projection\n\n\n\n\nArchitecture Comparison\nWhile both LLMs and GFMs use transformer architectures, they differ in input processing, positional encoding, and output heads. LLMs like GPT use causal attention for autoregressive generation, while GFMs like Prithvi use bidirectional attention for representation learning.\nKey Architectural Differences:\n\nInput Processing: 1D token sequences vs.¬†2D spatial patches\nPositional Encoding: 1D learned positions vs.¬†2D spatial coordinates\nAttention Patterns: Causal masking vs.¬†full bidirectional attention\nOutput Heads: Language modeling head vs.¬†reconstruction/classification heads\n\n\nclass LLMArchitecture(nn.Module):\n    \"\"\"Simplified LLM architecture (GPT-style)\"\"\"\n    \n    def __init__(self, vocab_size=50000, embed_dim=768, num_layers=12, num_heads=12):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.positional_encoding = nn.Embedding(2048, embed_dim)  # Max sequence length\n        \n        self.layers = nn.ModuleList([\n            SimpleTransformerBlock(embed_dim, num_heads) \n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n        self.output_head = nn.Linear(embed_dim, vocab_size)\n    \n    def forward(self, input_ids):\n        seq_len = input_ids.shape[1]\n        positions = torch.arange(seq_len, device=input_ids.device)\n        \n        # Token + positional embeddings\n        x = self.embedding(input_ids) + self.positional_encoding(positions)\n        \n        # Transformer layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.ln_final(x)\n        logits = self.output_head(x)\n        \n        return logits\n\nclass GFMArchitecture(nn.Module):\n    \"\"\"Simplified GFM architecture (ViT-style)\"\"\"\n    \n    def __init__(self, patch_size=16, num_bands=6, embed_dim=768, num_layers=12, num_heads=12):\n        super().__init__()\n        self.patch_size = patch_size\n        self.num_bands = num_bands\n        \n        # Patch embedding\n        patch_dim = patch_size * patch_size * num_bands\n        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n        \n        # 2D positional embedding\n        self.pos_embed_h = nn.Embedding(100, embed_dim // 2)  # Height positions\n        self.pos_embed_w = nn.Embedding(100, embed_dim // 2)  # Width positions\n        \n        self.layers = nn.ModuleList([\n            SimpleTransformerBlock(embed_dim, num_heads) \n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(embed_dim)\n    \n    def forward(self, patches, patch_positions):\n        batch_size, num_patches, patch_dim = patches.shape\n        \n        # Patch embeddings\n        x = self.patch_embedding(patches)\n        \n        # 2D positional embeddings\n        pos_h, pos_w = patch_positions[:, :, 0], patch_positions[:, :, 1]\n        pos_emb = torch.cat([\n            self.pos_embed_h(pos_h),\n            self.pos_embed_w(pos_w)\n        ], dim=-1)\n        \n        x = x + pos_emb\n        \n        # Transformer layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.ln_final(x)\n        return x\n\n# Compare architectures\nllm_model = LLMArchitecture(vocab_size=10000, embed_dim=384, num_layers=6, num_heads=6)\ngfm_model = GFMArchitecture(patch_size=16, num_bands=6, embed_dim=384, num_layers=6, num_heads=6)\n\nllm_params = sum(p.numel() for p in llm_model.parameters())\ngfm_params = sum(p.numel() for p in gfm_model.parameters())\n\nprint(\"Architecture Comparison:\")\nprint(f\"LLM parameters: {llm_params:,}\")\nprint(f\"GFM parameters: {gfm_params:,}\")\n\n# Test forward passes\nsample_tokens = torch.randint(0, 10000, (2, 50))  # [batch_size, seq_len]\nsample_patches = torch.randn(2, 16, 16*16*6)  # [batch_size, num_patches, patch_dim]\nsample_positions = torch.randint(0, 10, (2, 16, 2))  # [batch_size, num_patches, 2]\n\nllm_output = llm_model(sample_tokens)\ngfm_output = gfm_model(sample_patches, sample_positions)\n\nprint(f\"\\nLLM output shape: {llm_output.shape}\")\nprint(f\"GFM output shape: {gfm_output.shape}\")\n\nArchitecture Comparison:\nLLM parameters: 19,123,984\nGFM parameters: 11,276,160\n\nLLM output shape: torch.Size([2, 50, 10000])\nGFM output shape: torch.Size([2, 16, 384])\n\n\n\n\nPretraining Objectives\nThe pretraining objectives differ fundamentally between text and visual domains. LLMs excel at predictive modeling (predicting the next token), while GFMs focus on reconstructive modeling (rebuilding masked image patches).\nLLM Objectives:\n\nNext-Token Prediction: GPT-style autoregressive modeling for text generation\nMasked Language Modeling: BERT-style bidirectional understanding\nInstruction Following: Learning to follow human instructions (InstructGPT)\n\nGFM Objectives:\n\nMasked Patch Reconstruction: MAE-style learning of visual representations\nContrastive Learning: Learning invariances across time and space (SimCLR, CLIP)\nMulti-task Pretraining: Combining reconstruction with auxiliary tasks\n\nKey References:\n\nMasked Autoencoders Are Scalable Vision Learners - MAE methodology\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery\n\n\n# LLM next-token prediction objective\nsequence = torch.tensor([[1, 2, 3, 4, 5]])\ntargets = torch.tensor([[2, 3, 4, 5, 6]])  # Shifted by one position\n\nvocab_size = 1000\nlogits = torch.randn(1, 5, vocab_size)  # Model predictions\n\nce_loss = nn.CrossEntropyLoss()\nnext_token_loss = ce_loss(logits.view(-1, vocab_size), targets.view(-1))\n\nprint(\"LLM Pretraining Objectives:\")\nprint(f\"Next-token prediction loss: {next_token_loss.item():.4f}\")\n\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# GFM masked patch reconstruction objective\nbatch_size, num_patches, patch_dim = 2, 64, 768\noriginal_patches = torch.randn(batch_size, num_patches, patch_dim)\n\n# Random masking (75% typical for MAE)\nmask_ratio = 0.75\nnum_masked = int(num_patches * mask_ratio)\n\nmask = torch.zeros(batch_size, num_patches, dtype=torch.bool)\nfor i in range(batch_size):\n    masked_indices = torch.randperm(num_patches)[:num_masked]\n    mask[i, masked_indices] = True\n\n# Reconstruction loss on masked patches only\nreconstructed_patches = torch.randn_like(original_patches)\nreconstruction_loss = nn.MSELoss()(\n    reconstructed_patches[mask], \n    original_patches[mask]\n)\n\nprint(\"GFM Pretraining Objectives:\")\nprint(f\"Mask ratio: {mask_ratio:.1%}\")\nprint(f\"Masked patches per sample: {num_masked}\")\nprint(f\"Reconstruction loss: {reconstruction_loss.item():.4f}\")\n\nLLM Pretraining Objectives:\nNext-token prediction loss: 7.1255\n\n--------------------------------------------------\n\nGFM Pretraining Objectives:\nMask ratio: 75.0%\nMasked patches per sample: 48\nReconstruction loss: 1.9952\n\n\n\n\nScaling and Evolution\nThe scaling trends between LLMs and GFMs reveal different optimization strategies. LLMs focus on parameter scaling (billions of parameters) while GFMs emphasize data modality scaling (spectral, spatial, and temporal dimensions).\n\n\nParameter Scaling Comparison\nLLM Scaling Milestones:\n\nGPT-1 (2018): 117M parameters - Demonstrated unsupervised pretraining potential\nBERT-Base (2018): 110M parameters - Bidirectional language understanding\nGPT-2 (2019): 1.5B parameters - First signs of emergent capabilities\nGPT-3 (2020): 175B parameters - Few-shot learning breakthrough\nPaLM (2022): 540B parameters - Advanced reasoning capabilities\nGPT-4 (2023): ~1T parameters - Multimodal understanding\n\nGFM Scaling Examples:\n\nSatMAE-Base: 86M parameters - Satellite imagery foundation\nPrithvi-100M: 100M parameters - IBM/NASA Earth observation model\nClay-v0.1: 139M parameters - Open-source geospatial foundation model\nScale-MAE: 600M parameters - Largest published geospatial transformer\n\nContext/Input Scaling Differences:\nLLMs:\n\nContext length: 512 ‚Üí 2K ‚Üí 8K ‚Üí 128K+ tokens\nTraining data: Web text, books, code (curated datasets)\nFocus: Language understanding and generation\n\nGFMs:\n\nInput bands: 3 (RGB) ‚Üí 6+ (multispectral) ‚Üí hyperspectral\nSpatial resolution: Various (10m to 0.3m pixel sizes)\nTemporal dimension: Single ‚Üí time series ‚Üí multi-temporal\nFocus: Earth observation and environmental monitoring\n\n\n# Visualize parameter scaling comparison\nllm_milestones = {\n    'GPT-1': 117e6,\n    'BERT-Base': 110e6,\n    'GPT-2': 1.5e9,\n    'GPT-3': 175e9,\n    'PaLM': 540e9,\n    'GPT-4': 1000e9  # Estimated\n}\n\ngfm_milestones = {\n    'SatMAE-Base': 86e6,\n    'Prithvi-100M': 100e6,\n    'Clay-v0.1': 139e6,\n    'SatLas-Base': 300e6,\n    'Scale-MAE': 600e6\n}\n\n# Create visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# LLM scaling\nmodels = list(llm_milestones.keys())\nparams = [llm_milestones[m]/1e9 for m in models]\n\nax1.bar(models, params, color='skyblue', alpha=0.7)\nax1.set_yscale('log')\nax1.set_ylabel('Parameters (Billions)')\nax1.set_title('LLM Parameter Scaling')\nax1.tick_params(axis='x', rotation=45)\n\n# GFM scaling\nmodels = list(gfm_milestones.keys())\nparams = [gfm_milestones[m]/1e6 for m in models]\n\nax2.bar(models, params, color='lightcoral', alpha=0.7)\nax2.set_ylabel('Parameters (Millions)')\nax2.set_title('GFM Parameter Scaling')\nax2.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nData Requirements and Constraints\nThe data infrastructure requirements for LLMs and GFMs differ dramatically due to the nature of text versus imagery data. Understanding these constraints is crucial for development planning.\n\n\n\nAspect\nLLMs\nGFMs\n\n\n\n\nData Volume\nTerabytes of text data (web crawls, books, code repositories)\nPetabytes of satellite imagery (constrained by storage/IO bandwidth)\n\n\nData Quality Challenges\nDeduplication algorithms, toxicity filtering, language detection\nCloud masking, atmospheric correction, sensor calibration\n\n\nPreprocessing Requirements\nTokenization, sequence packing, attention mask generation\nPatch extraction, normalization, spatial/temporal alignment\n\n\nStorage Format Optimization\nCompressed text files, pre-tokenized sequences\nCloud-optimized formats (COG, Zarr), tiled storage\n\n\nAccess Pattern Differences\nSequential text processing, random document sampling\nSpatial/temporal queries, patch-based sampling, geographic tiling\n\n\n\n\n\nImplementation Examples\n\nEmbedding Creation\nEmbeddings are the foundation of both LLMs and GFMs, but they differ in how raw inputs are converted to dense vector representations. This section demonstrates practical implementation patterns for both domains.\n\n# Text embedding creation (LLM approach)\ntext = \"The forest shows signs of deforestation.\"\ntokens = text.lower().replace('.', '').split()\n\n# Create simple vocabulary mapping\nvocab = {word: i for i, word in enumerate(set(tokens))}\nvocab['&lt;PAD&gt;'] = len(vocab)\nvocab['&lt;UNK&gt;'] = len(vocab)\n\n# Convert text to token IDs\ntoken_ids = [vocab.get(token, vocab['&lt;UNK&gt;']) for token in tokens]\ntoken_tensor = torch.tensor(token_ids).unsqueeze(0)\n\n# Embedding layer lookup\nembed_layer = nn.Embedding(len(vocab), 256)\ntext_embeddings = embed_layer(token_tensor)\n\nprint(\"Text Embeddings (LLM):\")\nprint(f\"Original text: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Token IDs: {token_ids}\")\nprint(f\"Embeddings shape: {text_embeddings.shape}\")\n\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# Patch embedding creation (GFM approach)\npatch_size = 16\nnum_bands = 6\n\n# Create sample multi-spectral satellite patch\nsatellite_patch = torch.randn(1, num_bands, patch_size, patch_size)\n\n# Flatten patch for linear projection\npatch_flat = satellite_patch.view(1, num_bands * patch_size * patch_size)\n\n# Linear projection to embedding space\npatch_projection = nn.Linear(num_bands * patch_size * patch_size, 256)\npatch_embeddings = patch_projection(patch_flat)\n\nprint(\"Patch Embeddings (GFM):\")\nprint(f\"Original patch shape: {satellite_patch.shape}\")\nprint(f\"Flattened patch shape: {patch_flat.shape}\")\nprint(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n\nText Embeddings (LLM):\nOriginal text: The forest shows signs of deforestation.\nTokens: ['the', 'forest', 'shows', 'signs', 'of', 'deforestation']\nToken IDs: [0, 3, 1, 2, 5, 4]\nEmbeddings shape: torch.Size([1, 6, 256])\n\n--------------------------------------------------\n\nPatch Embeddings (GFM):\nOriginal patch shape: torch.Size([1, 6, 16, 16])\nFlattened patch shape: torch.Size([1, 1536])\nPatch embeddings shape: torch.Size([1, 256])\n\n\n\n\nPositional Encoding Comparison\nPositional encodings enable transformers to understand sequence order (LLMs) or spatial relationships (GFMs). The fundamental difference lies in 1D sequential positions versus 2D spatial coordinates.\nKey Differences:\n\nLLM: 1D sinusoidal encoding for sequence positions\nGFM: 2D spatial encoding combining height/width coordinates\nLearned vs Fixed: Both approaches can use learned or fixed encodings\n\n\n# 1D positional encoding for language models\ndef sinusoidal_positional_encoding(seq_len, embed_dim):\n    \"\"\"Create sinusoidal positional encodings for sequence data\"\"\"\n    pe = torch.zeros(seq_len, embed_dim)\n    position = torch.arange(0, seq_len).unsqueeze(1).float()\n    \n    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * \n                       -(np.log(10000.0) / embed_dim))\n    \n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    return pe\n\n# 2D positional encoding for geospatial models  \ndef create_2d_positional_encoding(height, width, embed_dim):\n    \"\"\"Create 2D positional encodings for spatial data\"\"\"\n    pe = torch.zeros(height, width, embed_dim)\n    \n    # Create position grids\n    y_pos = torch.arange(height).unsqueeze(1).repeat(1, width).float()\n    x_pos = torch.arange(width).unsqueeze(0).repeat(height, 1).float()\n    \n    # Encode Y positions in first half of embedding\n    for i in range(embed_dim // 2):\n        if i % 2 == 0:\n            pe[:, :, i] = torch.sin(y_pos / (10000 ** (i / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(y_pos / (10000 ** (i / embed_dim)))\n    \n    # Encode X positions in second half of embedding\n    for i in range(embed_dim // 2, embed_dim):\n        j = i - embed_dim // 2\n        if j % 2 == 0:\n            pe[:, :, i] = torch.sin(x_pos / (10000 ** (j / embed_dim)))\n        else:\n            pe[:, :, i] = torch.cos(x_pos / (10000 ** (j / embed_dim)))\n    \n    return pe\n\n# Generate and visualize both encodings\nseq_len, embed_dim = 100, 256\npos_encoding_1d = sinusoidal_positional_encoding(seq_len, embed_dim)\n\nheight, width = 8, 8\npos_encoding_2d = create_2d_positional_encoding(height, width, embed_dim)\n\nprint(f\"1D positional encoding shape: {pos_encoding_1d.shape}\")\nprint(f\"2D positional encoding shape: {pos_encoding_2d.shape}\")\n\n# Visualize both encodings\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(pos_encoding_1d[:50, :50].T, cmap='RdBu', aspect='auto')\nplt.title('1D Positional Encoding (LLM)')\nplt.xlabel('Sequence Position')\nplt.ylabel('Embedding Dimension')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\npos_2d_viz = pos_encoding_2d[:, :, :64].mean(dim=-1)\nplt.imshow(pos_2d_viz, cmap='RdBu', aspect='equal')\nplt.title('2D Positional Encoding (GFM)')\nplt.xlabel('Width')\nplt.ylabel('Height')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n1D positional encoding shape: torch.Size([100, 256])\n2D positional encoding shape: torch.Size([8, 8, 256])"
  }
]